{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df['Product']\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode = 'binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot,\n",
    "                                                    test_size = 1500, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation = 'relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation = 'relu'))\n",
    "model.add(layers.Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 113us/step - loss: 1.9383 - acc: 0.1719 - val_loss: 1.9174 - val_acc: 0.1930\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9173 - acc: 0.2055 - val_loss: 1.9020 - val_acc: 0.2170\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.9012 - acc: 0.2220 - val_loss: 1.8884 - val_acc: 0.2350\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8858 - acc: 0.2340 - val_loss: 1.8741 - val_acc: 0.2470\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8691 - acc: 0.2421 - val_loss: 1.8578 - val_acc: 0.2570\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8503 - acc: 0.2567 - val_loss: 1.8388 - val_acc: 0.2740\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8286 - acc: 0.2775 - val_loss: 1.8158 - val_acc: 0.2910\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8028 - acc: 0.2995 - val_loss: 1.7878 - val_acc: 0.3180\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.7721 - acc: 0.3285 - val_loss: 1.7551 - val_acc: 0.3630\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.7372 - acc: 0.3635 - val_loss: 1.7185 - val_acc: 0.3950\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6988 - acc: 0.3939 - val_loss: 1.6802 - val_acc: 0.4240\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6574 - acc: 0.4248 - val_loss: 1.6356 - val_acc: 0.4460\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6128 - acc: 0.4540 - val_loss: 1.5891 - val_acc: 0.4770\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5658 - acc: 0.4873 - val_loss: 1.5420 - val_acc: 0.4970\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5171 - acc: 0.5140 - val_loss: 1.4932 - val_acc: 0.5270\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4674 - acc: 0.5375 - val_loss: 1.4449 - val_acc: 0.5400\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4174 - acc: 0.5573 - val_loss: 1.3931 - val_acc: 0.5630\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3677 - acc: 0.5815 - val_loss: 1.3448 - val_acc: 0.5750\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3187 - acc: 0.5979 - val_loss: 1.2972 - val_acc: 0.6020\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2710 - acc: 0.6193 - val_loss: 1.2494 - val_acc: 0.5970\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2250 - acc: 0.6379 - val_loss: 1.2073 - val_acc: 0.6220\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1804 - acc: 0.6503 - val_loss: 1.1633 - val_acc: 0.6230\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1382 - acc: 0.6663 - val_loss: 1.1270 - val_acc: 0.6420\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0987 - acc: 0.6783 - val_loss: 1.0898 - val_acc: 0.6520\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0608 - acc: 0.6867 - val_loss: 1.0511 - val_acc: 0.6640\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0261 - acc: 0.6956 - val_loss: 1.0297 - val_acc: 0.6740\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9937 - acc: 0.7027 - val_loss: 0.9924 - val_acc: 0.6830\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9628 - acc: 0.7164 - val_loss: 0.9628 - val_acc: 0.6830\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9351 - acc: 0.7200 - val_loss: 0.9398 - val_acc: 0.6910\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9086 - acc: 0.7237 - val_loss: 0.9159 - val_acc: 0.6860\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8845 - acc: 0.7325 - val_loss: 0.8970 - val_acc: 0.7000\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8617 - acc: 0.7373 - val_loss: 0.8760 - val_acc: 0.7060\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8409 - acc: 0.7431 - val_loss: 0.8585 - val_acc: 0.7050\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8218 - acc: 0.7497 - val_loss: 0.8414 - val_acc: 0.7110\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8033 - acc: 0.7535 - val_loss: 0.8285 - val_acc: 0.7150\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7863 - acc: 0.7573 - val_loss: 0.8146 - val_acc: 0.7020\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7702 - acc: 0.7589 - val_loss: 0.7976 - val_acc: 0.7100\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7554 - acc: 0.7639 - val_loss: 0.7893 - val_acc: 0.7260\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7413 - acc: 0.7683 - val_loss: 0.7781 - val_acc: 0.7240\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7278 - acc: 0.7727 - val_loss: 0.7652 - val_acc: 0.7240\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7148 - acc: 0.7732 - val_loss: 0.7591 - val_acc: 0.7180\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7028 - acc: 0.7781 - val_loss: 0.7455 - val_acc: 0.7270\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6916 - acc: 0.7784 - val_loss: 0.7382 - val_acc: 0.7360\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6807 - acc: 0.7840 - val_loss: 0.7295 - val_acc: 0.7360\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6701 - acc: 0.7879 - val_loss: 0.7230 - val_acc: 0.7320\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6604 - acc: 0.7876 - val_loss: 0.7084 - val_acc: 0.7440\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6505 - acc: 0.7927 - val_loss: 0.7077 - val_acc: 0.7430\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6412 - acc: 0.7933 - val_loss: 0.6992 - val_acc: 0.7490\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6325 - acc: 0.7975 - val_loss: 0.6908 - val_acc: 0.7420\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6245 - acc: 0.7987 - val_loss: 0.6828 - val_acc: 0.7520\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6161 - acc: 0.8021 - val_loss: 0.6768 - val_acc: 0.7590\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6082 - acc: 0.8044 - val_loss: 0.6778 - val_acc: 0.7510\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6009 - acc: 0.8059 - val_loss: 0.6706 - val_acc: 0.7470\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5934 - acc: 0.8092 - val_loss: 0.6668 - val_acc: 0.7550\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5866 - acc: 0.8109 - val_loss: 0.6590 - val_acc: 0.7570\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5798 - acc: 0.8113 - val_loss: 0.6588 - val_acc: 0.7550\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5728 - acc: 0.8131 - val_loss: 0.6562 - val_acc: 0.7530\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5667 - acc: 0.8160 - val_loss: 0.6463 - val_acc: 0.7620\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5603 - acc: 0.8175 - val_loss: 0.6449 - val_acc: 0.7640\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5543 - acc: 0.8201 - val_loss: 0.6403 - val_acc: 0.7620\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5480 - acc: 0.8225 - val_loss: 0.6369 - val_acc: 0.7620\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5422 - acc: 0.8253 - val_loss: 0.6299 - val_acc: 0.7740\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5368 - acc: 0.8243 - val_loss: 0.6342 - val_acc: 0.7680\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5311 - acc: 0.8277 - val_loss: 0.6318 - val_acc: 0.7690\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.5255 - acc: 0.8272 - val_loss: 0.6202 - val_acc: 0.7770\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5213 - acc: 0.8313 - val_loss: 0.6224 - val_acc: 0.7690\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5157 - acc: 0.8335 - val_loss: 0.6179 - val_acc: 0.7730\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.5106 - acc: 0.8359 - val_loss: 0.6198 - val_acc: 0.7720\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5059 - acc: 0.8357 - val_loss: 0.6191 - val_acc: 0.7690\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5009 - acc: 0.8371 - val_loss: 0.6118 - val_acc: 0.7790\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4961 - acc: 0.8389 - val_loss: 0.6175 - val_acc: 0.7680\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4920 - acc: 0.8401 - val_loss: 0.6107 - val_acc: 0.7830\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4878 - acc: 0.8421 - val_loss: 0.6099 - val_acc: 0.7780\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4827 - acc: 0.8447 - val_loss: 0.6134 - val_acc: 0.7720\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4785 - acc: 0.8448 - val_loss: 0.6023 - val_acc: 0.7790\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4744 - acc: 0.8485 - val_loss: 0.6013 - val_acc: 0.7810\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4703 - acc: 0.8488 - val_loss: 0.5990 - val_acc: 0.7800\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4661 - acc: 0.8491 - val_loss: 0.6007 - val_acc: 0.7760\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4619 - acc: 0.8519 - val_loss: 0.5983 - val_acc: 0.7780\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.4578 - acc: 0.8517 - val_loss: 0.5964 - val_acc: 0.7780\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.4535 - acc: 0.8552 - val_loss: 0.5948 - val_acc: 0.7820\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4504 - acc: 0.8553 - val_loss: 0.5952 - val_acc: 0.7820\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4466 - acc: 0.8571 - val_loss: 0.5890 - val_acc: 0.7880\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4428 - acc: 0.8569 - val_loss: 0.5928 - val_acc: 0.7800\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4392 - acc: 0.8608 - val_loss: 0.5920 - val_acc: 0.7780\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4356 - acc: 0.8604 - val_loss: 0.5916 - val_acc: 0.7780\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.4321 - acc: 0.8617 - val_loss: 0.5865 - val_acc: 0.7860\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4286 - acc: 0.8633 - val_loss: 0.5890 - val_acc: 0.7770\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.4256 - acc: 0.8641 - val_loss: 0.5841 - val_acc: 0.7850\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4217 - acc: 0.8671 - val_loss: 0.5864 - val_acc: 0.7840\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4182 - acc: 0.8669 - val_loss: 0.5835 - val_acc: 0.7870\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4150 - acc: 0.8676 - val_loss: 0.5832 - val_acc: 0.7850\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4118 - acc: 0.8697 - val_loss: 0.5836 - val_acc: 0.7880\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.4087 - acc: 0.8701 - val_loss: 0.5846 - val_acc: 0.7750\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4054 - acc: 0.8720 - val_loss: 0.5863 - val_acc: 0.7810\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4022 - acc: 0.8731 - val_loss: 0.5788 - val_acc: 0.7880\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3988 - acc: 0.8739 - val_loss: 0.5791 - val_acc: 0.7890\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3958 - acc: 0.8763 - val_loss: 0.5815 - val_acc: 0.7820\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3928 - acc: 0.8751 - val_loss: 0.5791 - val_acc: 0.7870\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.3896 - acc: 0.8769 - val_loss: 0.5768 - val_acc: 0.7900\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3871 - acc: 0.8768 - val_loss: 0.5750 - val_acc: 0.7910\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3841 - acc: 0.8797 - val_loss: 0.5791 - val_acc: 0.7800\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3809 - acc: 0.8804 - val_loss: 0.5819 - val_acc: 0.7800\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3781 - acc: 0.8817 - val_loss: 0.5773 - val_acc: 0.7820\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3746 - acc: 0.8833 - val_loss: 0.5792 - val_acc: 0.7800\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3724 - acc: 0.8824 - val_loss: 0.5769 - val_acc: 0.7820\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3692 - acc: 0.8840 - val_loss: 0.5752 - val_acc: 0.7850\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3672 - acc: 0.8849 - val_loss: 0.5806 - val_acc: 0.7770\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3637 - acc: 0.8853 - val_loss: 0.5787 - val_acc: 0.7810\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3610 - acc: 0.8879 - val_loss: 0.5815 - val_acc: 0.7810\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3583 - acc: 0.8864 - val_loss: 0.5811 - val_acc: 0.7810\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3561 - acc: 0.8881 - val_loss: 0.5764 - val_acc: 0.7840\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3527 - acc: 0.8919 - val_loss: 0.5776 - val_acc: 0.7820\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3507 - acc: 0.8929 - val_loss: 0.5731 - val_acc: 0.7880\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.3481 - acc: 0.8929 - val_loss: 0.5779 - val_acc: 0.7830\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3456 - acc: 0.8959 - val_loss: 0.5762 - val_acc: 0.7770\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.3427 - acc: 0.8939 - val_loss: 0.5757 - val_acc: 0.7830\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.3404 - acc: 0.8952 - val_loss: 0.5866 - val_acc: 0.7800\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3384 - acc: 0.8973 - val_loss: 0.5830 - val_acc: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.3359 - acc: 0.8972 - val_loss: 0.5785 - val_acc: 0.7790\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3331569595893224, 0.8950666666984558]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6228947972456614, 0.7680000004768371]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9376 - acc: 0.1753 - val_loss: 1.9240 - val_acc: 0.1800\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9164 - acc: 0.1983 - val_loss: 1.9033 - val_acc: 0.2130\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8981 - acc: 0.2104 - val_loss: 1.8842 - val_acc: 0.2340\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8794 - acc: 0.2243 - val_loss: 1.8634 - val_acc: 0.2390\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8586 - acc: 0.2404 - val_loss: 1.8403 - val_acc: 0.2470\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8349 - acc: 0.2537 - val_loss: 1.8128 - val_acc: 0.2790\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8069 - acc: 0.2833 - val_loss: 1.7806 - val_acc: 0.3150\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7742 - acc: 0.3128 - val_loss: 1.7445 - val_acc: 0.3560\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7374 - acc: 0.3493 - val_loss: 1.7047 - val_acc: 0.3840\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.6964 - acc: 0.3823 - val_loss: 1.6606 - val_acc: 0.4090\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6520 - acc: 0.4187 - val_loss: 1.6143 - val_acc: 0.4490\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6044 - acc: 0.4521 - val_loss: 1.5655 - val_acc: 0.4940\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5549 - acc: 0.4903 - val_loss: 1.5145 - val_acc: 0.5220\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5043 - acc: 0.5160 - val_loss: 1.4631 - val_acc: 0.5500\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4535 - acc: 0.5392 - val_loss: 1.4129 - val_acc: 0.5700\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4030 - acc: 0.5560 - val_loss: 1.3633 - val_acc: 0.6030\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3545 - acc: 0.5780 - val_loss: 1.3152 - val_acc: 0.6240\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3075 - acc: 0.5981 - val_loss: 1.2707 - val_acc: 0.6410\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2627 - acc: 0.6147 - val_loss: 1.2269 - val_acc: 0.6440\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2206 - acc: 0.6316 - val_loss: 1.1878 - val_acc: 0.6520\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1805 - acc: 0.6413 - val_loss: 1.1475 - val_acc: 0.6650\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1423 - acc: 0.6523 - val_loss: 1.1112 - val_acc: 0.6590\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1071 - acc: 0.6632 - val_loss: 1.0780 - val_acc: 0.6690\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0733 - acc: 0.6723 - val_loss: 1.0467 - val_acc: 0.6810\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0418 - acc: 0.6795 - val_loss: 1.0175 - val_acc: 0.6860\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0120 - acc: 0.6868 - val_loss: 0.9926 - val_acc: 0.6900\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9844 - acc: 0.6917 - val_loss: 0.9667 - val_acc: 0.6920\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9584 - acc: 0.6996 - val_loss: 0.9410 - val_acc: 0.7010\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9338 - acc: 0.7031 - val_loss: 0.9190 - val_acc: 0.7150\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9107 - acc: 0.7093 - val_loss: 0.8990 - val_acc: 0.7130\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8889 - acc: 0.7152 - val_loss: 0.8835 - val_acc: 0.7140\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8688 - acc: 0.7181 - val_loss: 0.8615 - val_acc: 0.7150\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8492 - acc: 0.7228 - val_loss: 0.8452 - val_acc: 0.7290\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8312 - acc: 0.7279 - val_loss: 0.8308 - val_acc: 0.7290\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8145 - acc: 0.7325 - val_loss: 0.8140 - val_acc: 0.7310\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7983 - acc: 0.7348 - val_loss: 0.8003 - val_acc: 0.7370\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7830 - acc: 0.7415 - val_loss: 0.7889 - val_acc: 0.7440\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7697 - acc: 0.7449 - val_loss: 0.7751 - val_acc: 0.7420\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7552 - acc: 0.7461 - val_loss: 0.7660 - val_acc: 0.7390\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7423 - acc: 0.7497 - val_loss: 0.7547 - val_acc: 0.7420\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7299 - acc: 0.7536 - val_loss: 0.7474 - val_acc: 0.7400\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7188 - acc: 0.7580 - val_loss: 0.7362 - val_acc: 0.7450\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7075 - acc: 0.7624 - val_loss: 0.7283 - val_acc: 0.7450\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.6969 - acc: 0.7633 - val_loss: 0.7193 - val_acc: 0.7490\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6868 - acc: 0.7669 - val_loss: 0.7132 - val_acc: 0.7490\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6768 - acc: 0.7683 - val_loss: 0.7044 - val_acc: 0.7580\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6677 - acc: 0.7695 - val_loss: 0.6979 - val_acc: 0.7590\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.6589 - acc: 0.7723 - val_loss: 0.6921 - val_acc: 0.7570\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6506 - acc: 0.7771 - val_loss: 0.6844 - val_acc: 0.7590\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6422 - acc: 0.7776 - val_loss: 0.6799 - val_acc: 0.7610\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6346 - acc: 0.7767 - val_loss: 0.6724 - val_acc: 0.7600\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.6267 - acc: 0.7816 - val_loss: 0.6672 - val_acc: 0.7650\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.6193 - acc: 0.7827 - val_loss: 0.6648 - val_acc: 0.7500\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6124 - acc: 0.7877 - val_loss: 0.6613 - val_acc: 0.7550\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.6057 - acc: 0.7868 - val_loss: 0.6537 - val_acc: 0.7560\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5989 - acc: 0.7915 - val_loss: 0.6501 - val_acc: 0.7560\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.5925 - acc: 0.7941 - val_loss: 0.6470 - val_acc: 0.7500\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5859 - acc: 0.7965 - val_loss: 0.6404 - val_acc: 0.7610\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5801 - acc: 0.7975 - val_loss: 0.6409 - val_acc: 0.7560\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5748 - acc: 0.7992 - val_loss: 0.6328 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 47us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5690537568569183, 0.8019999999682108]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6878751371701558, 0.7506666663487752]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 2.6002 - acc: 0.1477 - val_loss: 2.5843 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.5733 - acc: 0.1783 - val_loss: 2.5591 - val_acc: 0.2120\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5500 - acc: 0.2067 - val_loss: 2.5366 - val_acc: 0.2350\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5271 - acc: 0.2192 - val_loss: 2.5143 - val_acc: 0.2510\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.5027 - acc: 0.2332 - val_loss: 2.4901 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.4763 - acc: 0.2393 - val_loss: 2.4645 - val_acc: 0.2630\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.4473 - acc: 0.2549 - val_loss: 2.4348 - val_acc: 0.2750\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.4147 - acc: 0.2699 - val_loss: 2.4012 - val_acc: 0.2940\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.3783 - acc: 0.2937 - val_loss: 2.3627 - val_acc: 0.3170\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.3383 - acc: 0.3244 - val_loss: 2.3199 - val_acc: 0.3540\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.2946 - acc: 0.3623 - val_loss: 2.2750 - val_acc: 0.3940\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.2481 - acc: 0.4076 - val_loss: 2.2259 - val_acc: 0.4360\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 2.1987 - acc: 0.4489 - val_loss: 2.1778 - val_acc: 0.4760\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 2.1479 - acc: 0.4884 - val_loss: 2.1243 - val_acc: 0.5020\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.0955 - acc: 0.5208 - val_loss: 2.0759 - val_acc: 0.5330\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.0432 - acc: 0.5485 - val_loss: 2.0222 - val_acc: 0.5630\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9908 - acc: 0.5803 - val_loss: 1.9708 - val_acc: 0.5820\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9391 - acc: 0.6009 - val_loss: 1.9240 - val_acc: 0.6000\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8895 - acc: 0.6203 - val_loss: 1.8725 - val_acc: 0.6160\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8414 - acc: 0.6397 - val_loss: 1.8306 - val_acc: 0.6300\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7959 - acc: 0.6552 - val_loss: 1.7879 - val_acc: 0.6380\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7528 - acc: 0.6681 - val_loss: 1.7462 - val_acc: 0.6530\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7124 - acc: 0.6773 - val_loss: 1.7107 - val_acc: 0.6540\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6744 - acc: 0.6897 - val_loss: 1.6747 - val_acc: 0.6640\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6393 - acc: 0.6955 - val_loss: 1.6409 - val_acc: 0.6750\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6066 - acc: 0.7019 - val_loss: 1.6140 - val_acc: 0.6820\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5762 - acc: 0.7097 - val_loss: 1.5817 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5478 - acc: 0.7169 - val_loss: 1.5565 - val_acc: 0.6950\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5214 - acc: 0.7221 - val_loss: 1.5374 - val_acc: 0.6960\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4966 - acc: 0.7277 - val_loss: 1.5112 - val_acc: 0.7020\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4736 - acc: 0.7329 - val_loss: 1.4888 - val_acc: 0.7030\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4522 - acc: 0.7357 - val_loss: 1.4683 - val_acc: 0.7010\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4318 - acc: 0.7404 - val_loss: 1.4574 - val_acc: 0.7140\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4126 - acc: 0.7439 - val_loss: 1.4373 - val_acc: 0.7150\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3947 - acc: 0.7472 - val_loss: 1.4161 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3779 - acc: 0.7524 - val_loss: 1.4058 - val_acc: 0.7150\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3617 - acc: 0.7565 - val_loss: 1.3953 - val_acc: 0.7210\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3462 - acc: 0.7611 - val_loss: 1.3794 - val_acc: 0.7190\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3315 - acc: 0.7619 - val_loss: 1.3643 - val_acc: 0.7230\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3172 - acc: 0.7677 - val_loss: 1.3553 - val_acc: 0.7270\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3039 - acc: 0.7725 - val_loss: 1.3415 - val_acc: 0.7280\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2910 - acc: 0.7729 - val_loss: 1.3286 - val_acc: 0.7350\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2779 - acc: 0.7776 - val_loss: 1.3235 - val_acc: 0.7320\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2668 - acc: 0.7791 - val_loss: 1.3103 - val_acc: 0.7370\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2553 - acc: 0.7809 - val_loss: 1.3001 - val_acc: 0.7350\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2443 - acc: 0.7809 - val_loss: 1.2923 - val_acc: 0.7430\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2340 - acc: 0.7860 - val_loss: 1.2831 - val_acc: 0.7370\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2238 - acc: 0.7881 - val_loss: 1.2866 - val_acc: 0.7410\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2141 - acc: 0.7924 - val_loss: 1.2703 - val_acc: 0.7400\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2046 - acc: 0.7913 - val_loss: 1.2598 - val_acc: 0.7440\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1952 - acc: 0.7961 - val_loss: 1.2517 - val_acc: 0.7440\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1868 - acc: 0.7975 - val_loss: 1.2442 - val_acc: 0.7440\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1782 - acc: 0.7991 - val_loss: 1.2356 - val_acc: 0.7480\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1696 - acc: 0.8007 - val_loss: 1.2336 - val_acc: 0.7440\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1615 - acc: 0.8028 - val_loss: 1.2221 - val_acc: 0.7520\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1540 - acc: 0.8031 - val_loss: 1.2187 - val_acc: 0.7490\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1460 - acc: 0.8057 - val_loss: 1.2130 - val_acc: 0.7540\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1382 - acc: 0.8072 - val_loss: 1.2089 - val_acc: 0.7520\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1311 - acc: 0.8079 - val_loss: 1.1994 - val_acc: 0.7500\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1238 - acc: 0.8091 - val_loss: 1.1955 - val_acc: 0.7540\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1171 - acc: 0.8131 - val_loss: 1.1911 - val_acc: 0.7490\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1097 - acc: 0.8135 - val_loss: 1.1851 - val_acc: 0.7570\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1030 - acc: 0.8135 - val_loss: 1.1809 - val_acc: 0.7560\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0962 - acc: 0.8173 - val_loss: 1.1811 - val_acc: 0.7550\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0897 - acc: 0.8189 - val_loss: 1.1696 - val_acc: 0.7610\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0836 - acc: 0.8187 - val_loss: 1.1724 - val_acc: 0.7570\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0774 - acc: 0.8212 - val_loss: 1.1629 - val_acc: 0.7610\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0711 - acc: 0.8240 - val_loss: 1.1583 - val_acc: 0.7620\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0649 - acc: 0.8260 - val_loss: 1.1547 - val_acc: 0.7600\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0589 - acc: 0.8252 - val_loss: 1.1486 - val_acc: 0.7620\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0531 - acc: 0.8240 - val_loss: 1.1452 - val_acc: 0.7680\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0475 - acc: 0.8289 - val_loss: 1.1499 - val_acc: 0.7670\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0418 - acc: 0.8301 - val_loss: 1.1467 - val_acc: 0.7610\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.0363 - acc: 0.8293 - val_loss: 1.1367 - val_acc: 0.7640\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0308 - acc: 0.8304 - val_loss: 1.1342 - val_acc: 0.7680\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0256 - acc: 0.8340 - val_loss: 1.1256 - val_acc: 0.7670\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0200 - acc: 0.8364 - val_loss: 1.1202 - val_acc: 0.7680\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0145 - acc: 0.8359 - val_loss: 1.1186 - val_acc: 0.7710\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0095 - acc: 0.8391 - val_loss: 1.1139 - val_acc: 0.7660\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0043 - acc: 0.8388 - val_loss: 1.1235 - val_acc: 0.7710\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9995 - acc: 0.8400 - val_loss: 1.1052 - val_acc: 0.7740\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9945 - acc: 0.8405 - val_loss: 1.1118 - val_acc: 0.7690\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9895 - acc: 0.8415 - val_loss: 1.1013 - val_acc: 0.7710\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9842 - acc: 0.8421 - val_loss: 1.0991 - val_acc: 0.7680\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9798 - acc: 0.8449 - val_loss: 1.0942 - val_acc: 0.7750\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9750 - acc: 0.8452 - val_loss: 1.0940 - val_acc: 0.7650\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9705 - acc: 0.8461 - val_loss: 1.0880 - val_acc: 0.7700\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9655 - acc: 0.8485 - val_loss: 1.0897 - val_acc: 0.7700\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9609 - acc: 0.8487 - val_loss: 1.0851 - val_acc: 0.7760\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9564 - acc: 0.8515 - val_loss: 1.0799 - val_acc: 0.7740\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9524 - acc: 0.8511 - val_loss: 1.0745 - val_acc: 0.7790\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9477 - acc: 0.8533 - val_loss: 1.0791 - val_acc: 0.7670\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9429 - acc: 0.8556 - val_loss: 1.0709 - val_acc: 0.7750\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9386 - acc: 0.8552 - val_loss: 1.0723 - val_acc: 0.7700\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9345 - acc: 0.8564 - val_loss: 1.0697 - val_acc: 0.7720\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9303 - acc: 0.8567 - val_loss: 1.0664 - val_acc: 0.7810\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9260 - acc: 0.8583 - val_loss: 1.0643 - val_acc: 0.7740\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9220 - acc: 0.8589 - val_loss: 1.0613 - val_acc: 0.7800\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9176 - acc: 0.8623 - val_loss: 1.0535 - val_acc: 0.7810\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9136 - acc: 0.8617 - val_loss: 1.0522 - val_acc: 0.7830\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9096 - acc: 0.8635 - val_loss: 1.0522 - val_acc: 0.7750\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9055 - acc: 0.8627 - val_loss: 1.0481 - val_acc: 0.7820\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9015 - acc: 0.8649 - val_loss: 1.0505 - val_acc: 0.7750\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8975 - acc: 0.8637 - val_loss: 1.0459 - val_acc: 0.7820\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8939 - acc: 0.8659 - val_loss: 1.0407 - val_acc: 0.7830\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8901 - acc: 0.8663 - val_loss: 1.0411 - val_acc: 0.7800\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8862 - acc: 0.8668 - val_loss: 1.0387 - val_acc: 0.7810\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8823 - acc: 0.8683 - val_loss: 1.0388 - val_acc: 0.7780\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8787 - acc: 0.8684 - val_loss: 1.0313 - val_acc: 0.7860\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8748 - acc: 0.8712 - val_loss: 1.0293 - val_acc: 0.7840\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8712 - acc: 0.8680 - val_loss: 1.0312 - val_acc: 0.7830\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.8711 - val_loss: 1.0287 - val_acc: 0.7820\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8638 - acc: 0.8725 - val_loss: 1.0267 - val_acc: 0.7850\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8602 - acc: 0.8716 - val_loss: 1.0210 - val_acc: 0.7920\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8568 - acc: 0.8733 - val_loss: 1.0227 - val_acc: 0.7790\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8533 - acc: 0.8747 - val_loss: 1.0220 - val_acc: 0.7810\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8498 - acc: 0.8747 - val_loss: 1.0172 - val_acc: 0.7910\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8462 - acc: 0.8777 - val_loss: 1.0195 - val_acc: 0.7790\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8425 - acc: 0.8784 - val_loss: 1.0157 - val_acc: 0.7860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8392 - acc: 0.8781 - val_loss: 1.0203 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 16.0093 - acc: 0.1608 - val_loss: 15.5961 - val_acc: 0.2280\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 15.2472 - acc: 0.2084 - val_loss: 14.8499 - val_acc: 0.2440\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 14.5128 - acc: 0.2232 - val_loss: 14.1260 - val_acc: 0.2530\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 13.7992 - acc: 0.2321 - val_loss: 13.4217 - val_acc: 0.2610\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 13.1044 - acc: 0.2441 - val_loss: 12.7351 - val_acc: 0.2710\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 12.4280 - acc: 0.2681 - val_loss: 12.0667 - val_acc: 0.3000\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 11.7696 - acc: 0.2932 - val_loss: 11.4169 - val_acc: 0.3320\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 11.1288 - acc: 0.3245 - val_loss: 10.7840 - val_acc: 0.3650\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 10.5063 - acc: 0.3715 - val_loss: 10.1728 - val_acc: 0.3850\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.9065 - acc: 0.3993 - val_loss: 9.5847 - val_acc: 0.4090\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.3289 - acc: 0.4309 - val_loss: 9.0179 - val_acc: 0.4320\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 8.7734 - acc: 0.4535 - val_loss: 8.4738 - val_acc: 0.4610\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.2409 - acc: 0.4749 - val_loss: 7.9539 - val_acc: 0.4920\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 7.7321 - acc: 0.4983 - val_loss: 7.4567 - val_acc: 0.4940\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 7.2458 - acc: 0.5132 - val_loss: 6.9832 - val_acc: 0.5120\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 6.7822 - acc: 0.5235 - val_loss: 6.5315 - val_acc: 0.5450\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 6.3408 - acc: 0.5417 - val_loss: 6.1010 - val_acc: 0.5540\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 5.9218 - acc: 0.5512 - val_loss: 5.6944 - val_acc: 0.5680\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.5260 - acc: 0.5651 - val_loss: 5.3121 - val_acc: 0.5580\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 5.1528 - acc: 0.5755 - val_loss: 4.9496 - val_acc: 0.5950\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 4.8023 - acc: 0.5875 - val_loss: 4.6103 - val_acc: 0.6000\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 4.4739 - acc: 0.5943 - val_loss: 4.2944 - val_acc: 0.6050\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 4.1684 - acc: 0.5995 - val_loss: 4.0009 - val_acc: 0.6110\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 3.8854 - acc: 0.6071 - val_loss: 3.7297 - val_acc: 0.6230\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 3.6249 - acc: 0.6197 - val_loss: 3.4798 - val_acc: 0.6260\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.3860 - acc: 0.6261 - val_loss: 3.2529 - val_acc: 0.6260\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.1692 - acc: 0.6277 - val_loss: 3.0471 - val_acc: 0.6350\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.9735 - acc: 0.6356 - val_loss: 2.8637 - val_acc: 0.6400\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.7994 - acc: 0.6392 - val_loss: 2.6988 - val_acc: 0.6390\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.6461 - acc: 0.6428 - val_loss: 2.5550 - val_acc: 0.6500\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.5131 - acc: 0.6441 - val_loss: 2.4320 - val_acc: 0.6520\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.4000 - acc: 0.6439 - val_loss: 2.3315 - val_acc: 0.6530\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.3057 - acc: 0.6452 - val_loss: 2.2466 - val_acc: 0.6550\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.2299 - acc: 0.6463 - val_loss: 2.1768 - val_acc: 0.6570\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1706 - acc: 0.6453 - val_loss: 2.1261 - val_acc: 0.6590\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.1262 - acc: 0.6479 - val_loss: 2.0909 - val_acc: 0.6600\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0924 - acc: 0.6513 - val_loss: 2.0623 - val_acc: 0.6590\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.0652 - acc: 0.6543 - val_loss: 2.0335 - val_acc: 0.6730\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0409 - acc: 0.6571 - val_loss: 2.0145 - val_acc: 0.6680\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0194 - acc: 0.6565 - val_loss: 1.9889 - val_acc: 0.6770\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9990 - acc: 0.6587 - val_loss: 1.9714 - val_acc: 0.6700\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9801 - acc: 0.6611 - val_loss: 1.9531 - val_acc: 0.6770\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9623 - acc: 0.6608 - val_loss: 1.9331 - val_acc: 0.6800\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9454 - acc: 0.6629 - val_loss: 1.9178 - val_acc: 0.6790\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9286 - acc: 0.6633 - val_loss: 1.9027 - val_acc: 0.6830\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9128 - acc: 0.6645 - val_loss: 1.8856 - val_acc: 0.6770\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8976 - acc: 0.6679 - val_loss: 1.8731 - val_acc: 0.6710\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8832 - acc: 0.6687 - val_loss: 1.8588 - val_acc: 0.6770\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8693 - acc: 0.6680 - val_loss: 1.8476 - val_acc: 0.6840\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8556 - acc: 0.6704 - val_loss: 1.8307 - val_acc: 0.6780\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8420 - acc: 0.6723 - val_loss: 1.8163 - val_acc: 0.6810\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8291 - acc: 0.6727 - val_loss: 1.8051 - val_acc: 0.6830\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8166 - acc: 0.6747 - val_loss: 1.7926 - val_acc: 0.6840\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8046 - acc: 0.6740 - val_loss: 1.7784 - val_acc: 0.6820\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.7924 - acc: 0.6759 - val_loss: 1.7680 - val_acc: 0.6810\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7813 - acc: 0.6780 - val_loss: 1.7561 - val_acc: 0.6870\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7697 - acc: 0.6773 - val_loss: 1.7445 - val_acc: 0.6860\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7584 - acc: 0.6781 - val_loss: 1.7347 - val_acc: 0.6880\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7474 - acc: 0.6805 - val_loss: 1.7313 - val_acc: 0.6870\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7373 - acc: 0.6813 - val_loss: 1.7145 - val_acc: 0.6930\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.7270 - acc: 0.6812 - val_loss: 1.7037 - val_acc: 0.6900\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7167 - acc: 0.6820 - val_loss: 1.6936 - val_acc: 0.6920\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7071 - acc: 0.6836 - val_loss: 1.6846 - val_acc: 0.6880\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6975 - acc: 0.6847 - val_loss: 1.6791 - val_acc: 0.6950\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.6883 - acc: 0.6843 - val_loss: 1.6641 - val_acc: 0.6930\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.6785 - acc: 0.6867 - val_loss: 1.6553 - val_acc: 0.6960\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6694 - acc: 0.6876 - val_loss: 1.6505 - val_acc: 0.6960\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6602 - acc: 0.6875 - val_loss: 1.6418 - val_acc: 0.6910\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6515 - acc: 0.6908 - val_loss: 1.6304 - val_acc: 0.6940\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6424 - acc: 0.6913 - val_loss: 1.6197 - val_acc: 0.7010\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6338 - acc: 0.6913 - val_loss: 1.6132 - val_acc: 0.6950\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6258 - acc: 0.6908 - val_loss: 1.6040 - val_acc: 0.6980\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6177 - acc: 0.6921 - val_loss: 1.6032 - val_acc: 0.6920\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6100 - acc: 0.6949 - val_loss: 1.5902 - val_acc: 0.6960\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6014 - acc: 0.6943 - val_loss: 1.5806 - val_acc: 0.7010\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5937 - acc: 0.6945 - val_loss: 1.5814 - val_acc: 0.7040\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5868 - acc: 0.6949 - val_loss: 1.5650 - val_acc: 0.6990\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5781 - acc: 0.6944 - val_loss: 1.5593 - val_acc: 0.7020\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5707 - acc: 0.6981 - val_loss: 1.5505 - val_acc: 0.7060\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5636 - acc: 0.6951 - val_loss: 1.5436 - val_acc: 0.7060\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5561 - acc: 0.6993 - val_loss: 1.5362 - val_acc: 0.7110\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5488 - acc: 0.6979 - val_loss: 1.5305 - val_acc: 0.7010\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5418 - acc: 0.6996 - val_loss: 1.5245 - val_acc: 0.7130\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5351 - acc: 0.6989 - val_loss: 1.5152 - val_acc: 0.7040\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5279 - acc: 0.7000 - val_loss: 1.5055 - val_acc: 0.7030\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5208 - acc: 0.7001 - val_loss: 1.5000 - val_acc: 0.7060\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5138 - acc: 0.7027 - val_loss: 1.4959 - val_acc: 0.7100\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.5070 - acc: 0.7025 - val_loss: 1.4906 - val_acc: 0.7030\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5003 - acc: 0.7035 - val_loss: 1.4817 - val_acc: 0.7150\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4939 - acc: 0.7029 - val_loss: 1.4740 - val_acc: 0.7070\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.4873 - acc: 0.7049 - val_loss: 1.4686 - val_acc: 0.7110\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4812 - acc: 0.7048 - val_loss: 1.4652 - val_acc: 0.7120\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4747 - acc: 0.7056 - val_loss: 1.4573 - val_acc: 0.7120\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4695 - acc: 0.7049 - val_loss: 1.4518 - val_acc: 0.7130\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4623 - acc: 0.7056 - val_loss: 1.4459 - val_acc: 0.7140\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4568 - acc: 0.7060 - val_loss: 1.4404 - val_acc: 0.7150\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4503 - acc: 0.7071 - val_loss: 1.4319 - val_acc: 0.7180\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4446 - acc: 0.7069 - val_loss: 1.4266 - val_acc: 0.7140\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4385 - acc: 0.7076 - val_loss: 1.4220 - val_acc: 0.7150\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4328 - acc: 0.7088 - val_loss: 1.4133 - val_acc: 0.7170\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.4264 - acc: 0.7097 - val_loss: 1.4089 - val_acc: 0.7170\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4218 - acc: 0.7092 - val_loss: 1.4023 - val_acc: 0.7170\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4155 - acc: 0.7077 - val_loss: 1.3996 - val_acc: 0.7130\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4099 - acc: 0.7124 - val_loss: 1.3938 - val_acc: 0.7180\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4043 - acc: 0.7120 - val_loss: 1.3867 - val_acc: 0.7170\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3986 - acc: 0.7116 - val_loss: 1.3820 - val_acc: 0.7140\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3931 - acc: 0.7129 - val_loss: 1.3760 - val_acc: 0.7190\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3876 - acc: 0.7116 - val_loss: 1.3750 - val_acc: 0.7230\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3829 - acc: 0.7123 - val_loss: 1.3667 - val_acc: 0.7190\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3774 - acc: 0.7129 - val_loss: 1.3630 - val_acc: 0.7200\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3723 - acc: 0.7153 - val_loss: 1.3598 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3673 - acc: 0.7141 - val_loss: 1.3534 - val_acc: 0.7230\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3619 - acc: 0.7168 - val_loss: 1.3474 - val_acc: 0.7190\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3568 - acc: 0.7159 - val_loss: 1.3430 - val_acc: 0.7180\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3519 - acc: 0.7149 - val_loss: 1.3376 - val_acc: 0.7220\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3470 - acc: 0.7164 - val_loss: 1.3328 - val_acc: 0.7210\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3421 - acc: 0.7179 - val_loss: 1.3271 - val_acc: 0.7230\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3372 - acc: 0.7157 - val_loss: 1.3250 - val_acc: 0.7160\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3322 - acc: 0.7153 - val_loss: 1.3242 - val_acc: 0.7220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3275 - acc: 0.7184 - val_loss: 1.3137 - val_acc: 0.7210\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt8FPXV+PHPyebCJdwDIiSQIAhCBMGIRhCDIOAVRVuhWmhVqD7a6vPor9XWtlTb2mq91Ecfq1XRKhWtVkGr2JpCFY2XoIACIghIAgghcr/ksnt+f8zsOmw2ySZk2Wxy3r54uXPdMzObOfP9fme+I6qKMcYYA5AU7wCMMcY0H5YUjDHGhFhSMMYYE2JJwRhjTIglBWOMMSGWFIwxxoRYUqiHiPhEZJ+I9GnKeZs7EXlGRGa7nwtEZGU08zbie1rMPmvuRGSNiJxRx/QlIvK9oxjSUScivxaRJ49g+cdE5KdNGFJwvf8Ukcuber2N0eKSgnuCCf4LiMhBz3CDd7qq+lU1XVU3NeW8jSEip4jIRyKyV0Q+E5HxsfiecKq6WFWHNMW6wk88sd5n5huqOlBV34YmOTmOF5GNtUwbJyKLRWSPiKxr7Hc0R6p6tar+9kjWEWnfq+oEVZ17RME1kRaXFNwTTLqqpgObgAs842rsdBFJPvpRNtr/AQuAjsC5wOb4hmNqIyJJItLi/r6itB94DPhJQxdszn+PIuKLdwxHQ6v70bpZ+jkReVZE9gJXiEi+iLwnIrtEZKuIPCAiKe78ySKiIpLtDj/jTn/dvWIvEpGchs7rTj9HRD4Xkd0i8r8i8k49xfdq4Et1rFfV1fVs61oRmeQZThWRr0VkqHvSekFEvnK3e7GInFDLeg67KhSRk0VkmbtNzwJpnmndROQ1ESkTkZ0i8oqI9Han/R7IB/7kltzuj7DPOrv7rUxENorIrSIi7rSrReQ/InKfG/N6EZlQx/bf5s6zV0RWisiFYdN/4Ja49orIpyIyzB3fV0RedmPYISJ/dMcfdoUnIv1FRD3DS0TkDhEpwjkx9nFjXu1+xxcicnVYDFPcfblHRNaJyAQRmSYi74fN9xMReSHCNp4tIh97hheLyLue4fdE5Hz3c6k4VYHnAz8GLnePw1LPKnNE5F033oUi0rW2/VsbVX1PVZ8BNtQ3b3Afisj3RWQT8E93/Cj55m9ymYiM8SxznLuv94pT7fJw8LiE/1a92x3hu+v8G3B/hw+5+2E/cIYcXq36utSsmbjCnfag+717RORDETndHR9x34unBO3G9QsR+VJEtovIkyLSMWx/TXfXXyYit0R3ZKKkqi32H7ARGB827tdAJXABTlJsC5wCnAokA/2Az4Hr3fmTAQWy3eFngB1AHpACPAc804h5ewB7gcnutP8BqoDv1bE9fwS+BoZFuf23A095hicDn7qfk4DvAR2ANsCDQLFn3meA2e7n8cBG93MaUAr8yI17qht3cN7uwMXufu0I/B14wbPeJd5tjLDP/uou08E9FuuAGe60q93vuhLwAT8ESurY/m8Dx7rb+h1gH3CMO20aUAKcDAhwPJDlxvMp8Aegvbsdozy/nSc96+8PaNi2bQROcPdNMs7vrJ/7HWcBB4Gh7vynA7uAcW6MWcBA9zt3AQM86/4EmBxhG9sDh4AuQCrwFbDVHR+c1tmdtxQoiLQtnvjXAgOAdsDbwK9r2beh30Qd+38SsK6eefq7x3+O+51t3f1QDkx098sknL+jbu4yHwC/d7d3DM7f0ZO1xVXbdhPd38BOnAuZJJzffujvIuw7zscpufd2h78LdHV/Az9xp6XVs++/536ehXMOynFjmw/MCdtff3JjHgFUeH8rR/qv1ZUUXEtU9RVVDajqQVX9UFXfV9VqVV0PPAqcWcfyL6hqsapWAXOBkxox7/nAMlWd7067D+eHH5F7BTIKuAL4h4gMdcefE35V6fFX4CIRaeMOf8cdh7vtT6rqXlU9BMwGThaR9nVsC24MCvyvqlap6jwgdKWqqmWq+pK7X/cAv6XufendxhScE/ktblzrcfbLdz2zfaGqT6iqH3gKyBSRjEjrU9XnVXWru61/xTlh57mTrwZ+p6pL1fG5qpbgnAAygJ+o6n53O96JJn7XE6q62t031e7vbL37Hf8GCoFgY+9VwJ9VtdCNsURV16jqQeBvOMcaETkJJ7m9FmEb9+Ps/zOAkcBHQJG7HacDq1R1VwPif1xV16rqATeGun7bTemXqnrA3fbpwAJVfcPdLwuB5cAkEekHDMM5MVeq6lvAPxrzhVH+DbykqkXuvBWR1iMig4AngG+p6mZ33U+r6teqWg3chXOB1D/K0C4H/qCqG1R1L/BT4DtyeHXkbFU9pKofAStx9kmTaK1JocQ7ICKDROQfbjFyD84VdsQTjesrz+cDQHoj5u3ljUOdy4DSOtZzA/CAqr4GXAf8000MpwNvRlpAVT8DvgDOE5F0nET0Vwjd9XOXONUre3CuyKHu7Q7GXerGG/Rl8IOItBfnDo1N7nr/HcU6g3rglAC+9Iz7EujtGQ7fn1DL/heR74nIcrdqYBcwyBNLFs6+CZeFc6XpjzLmcOG/rfNF5H1xqu12AROiiAGchBe8MeIK4Dn34iGS/wAFOFfN/wEW4yTiM93hhmjIb7spefdbX2Ba8Li5++00nN9eL6DcTR6Rlo1alH8Dda5bRDrjtPPdqqrearsfi1M1uRuntNGe6P8OelHzbyAVpxQOgKrG7Di11qQQ3jXsIzhVBv1VtSPwC5zifixtBTKDAyIiHH7yC5eM06aAqs7HKZK+iXPCuL+O5Z7FqSq5GKdkstEdPx2nsfosoBPfXMXUt92Hxe3y3k76Y5xi70h3X54VNm9d3fJuB/w4JwXvuhvcoO5eUT4MXItT7dAZ+Ixvtq8EOC7CoiVAX4ncqLgfp4ojqGeEebxtDG2BF4A7caqtOuPUmdcXA6q6xF3HKJzj93Sk+VzhSeE/1J8UmlX3yGEXGSU41SWdPf/aq+rdOL+/bp7SLzjJNeiwYyROw3W3Wr42mr+BWveT+xuZByxU1cc948fiVAdfAnTGqdrb51lvfft+CzX/BiqBsnqWaxKtNSmE6wDsBva7DU0/OArf+SowQkQucH+4N+C5Eojgb8BsETnRLUZ+hvNDaYtTt1ibZ4FzcOop/+oZ3wGnLrIc54/oN1HGvQRIEpHrxWkk/hZOvaZ3vQeAnSLSDSfBem3DqWOvwb0SfgH4rYiki9Mo/9849bgNlY7zx1eGk3OvxikpBD0G/FhEhotjgIhk4VS9lLsxtBORtu6JGWAZcKaIZLlXiPU18KXhXOGVAX63kXGcZ/rjwNUiMtZtXMwUkYGe6U/jJLb9qvpeHd+zBBgCDAeWAitwTnB5OO0CkWwDst2LkcYSEWkT9k/cbWmD064SnCelAet9GrhYnEZ0n7v8WBHppapf4LSv/FKcGydGA+d5lv0M6CAiE93v/KUbRySN/RsI+h3ftAeGr7capzo4BadaylslVd++fxb4HxHJFpEOblzPqmqggfE1iiUFx03ADJwGq0dwGoRjSlW3AZcB9+L8KI/DqRuOWG+J07D2F5yi6tc4pYOrcX5A/wjenRDhe0qBYpzi9/OeSXNwrki24NRJvltz6Yjrq8ApdczEKRZPAV72zHIvzlVXubvO18NWcT/fVA3cG+Er/gsn2W3Aucp9yt3uBlHVFcADOI2SW3ESwvue6c/i7NPngD04jdtd3Drg83Eai0twbmu+1F1sIfASzknpA5xjUVcMu3CS2ks4x+xSnIuB4PR3cfbjAzgXJYs4/Kr3L0AudZcScOudVwAr3LYMdeNbp6rltSz2HE7C+lpEPqhr/XXog9Nw7v3Xl28a1BfgXAAcpObvoFZuafZi4Oc4CXUTzt9o8Hw1DadUVI5z0n8O9+9GVXfi3IDwFE4J82sOrxLzatTfgMc03JsF5Js7kC7Daft5E6fRfiPO72urZ7n69v2f3XneBtbjnJduaGBsjSaHl9pMvLhF0S3Apeo+YGRaN7fBczuQq6r13t7ZWonIizhVo3fEO5aWwEoKcSQik0Skk4ik4VwVVeNc4RkDzg0F71hCOJyIjBSRHLea6lyckt38eMfVUjTbpwdbidE4t6mm4hRfL6rttjfTuohIKc4zGZPjHUsz1At4Eec5gFJgpltdaJqAVR8ZY4wJseojY4wxIQlXfZSRkaHZ2dnxDsMYYxLK0qVLd6hqXbe9AwmYFLKzsykuLo53GMYYk1BE5Mv657LqI2OMMR6WFIwxxoRYUjDGGBNiScEYY0yIJQVjjDEhlhSMMcaEWFIwxpgoFZUUcefbd1JUUtQk8x3pMrGQcM8pGGNMPBSVFDHuL+Oo9FeS6kulcHoh+Vn5FJUUsXjjYgqyC0LDkeYLX9fijYvp1q4b5QfK6dauGzcuvLHOdQOHfU+sWFIwxiSs8BNybdODJ9/aTq6R5guftmn3Jir9lfjVT6W/kr8s/wt/Wf4X5iybQ3WgOnQyX7xxcY35IiWAiuoKAgRIkiSSJImABghogEp/JYs3LgYIJRdfkg9BDvueWCUGSwrGmKOivhN4tMtHurr2Jfm48qQrmT5sOkBovvCTb3JSco2TKzgnX+98ab407p90Px9v/Th00vcl+UhOSoYA+JJ8zFk2h0p/Jeq+XTN4Mi/ILiDVlxqK6/GPH6c6UI2ihycAnBepBTRAsGNSQfAl+eiY1pHZi2dT4a8goAH8/m9eGR78nlglhYTrJTUvL0+tmwtjYi/SSbyxJ/Zoql6AGp+jvboG54Sa4ksJnfRF5LDpwXkA5wRNEuP7jadfl378+aM/49dvTrxJJOFL8oVO5gA+8TFzxEz6dOrDpt2bDltGEJKTkunbqS+pyan07dSXPRV7WLp1KYeqD9XYH4KE1huUnpJOhb+CqkBVxH0oCCJCmi+tUSUFEVmqqnn1zWclBWNMDZFO4kC9deXe5b3JI7xKpa7qkeDnKn9VzQTgubpGnSt2VSX4X5XfOaEqSpI6J3aUGiWF4Lrf3PAmyV8m1zhBAzVOzoqyde9W2qW044udXxy2jKJUBapC41eVrcInPkb1GcV7pe9R5a9CUQQhSZLI7JhJqi+V/l3707VtVy494VImD5pMhb+Ca1+9lqeWPxVaf5+Ofbhz/J3kdM6xNgVjzNERXjUTXn8ePImHj4vUgBqpnt1bpZLqS6Vbu26HVY8E/M7JXtHDPsPhCcB7gq+tisebXMZlj2Pzvs3sObSHQ/5D9ErvRec2nfly95es37k+VIef6kvl2PRjqQ5Us79yP5X+Sg5UHwCcK/RBGYNok9yGtza9xb82/Iue6T0ZnDGYAAGO73o8Y3PGsnnPZu4puge/+vGJj9vG3Mbsgtl1tleEa5Pchlknz+K5lc+F4pp36bzQ/LFMBkExrT4SkUnAHwEf8Jiq/i5s+n3AWHewHdBDVTvXtU6rPjKm4eqq9gmWCqKtew+erO6fdH+oasdbxXOo+lDEKpdaq4JIItnnXJ/6A36SJAlF8Qf8oWqeFF8K03KnUXagjCRJomx/GYpSdqCMQ9WHCAQCHKw+yMHqg1T6K0Pb5hMffTv3JadzDu1T21N+oJxt+7ex7ut1h80zf9p8zhtwXtT7rLZ9HG1J6kiOV2NFW30Us6Tgvoj+c+BsnFfmfQhMU9VVtcz/Q2C4ql5Z13otKZjWqjEnitqu3L31+eH14xD5ZB5e1x9NHX/4lfuE4yaw6+AuikqLIlbZRKtjWkf6d+3PgK4DSE9ND1X19GjXg57pPenbuS9Dug+hf9f+pPhSaiy/t2Ivz6x4hhXbVzB96PQmO/HG4mTeVJpDUsgHZqvqRHf4VgBVvbOW+d8Ffqmq/6prvZYUTGsU7T3ykZaJdOUO1Khy8dbhp/nSeOmyl/Crn0ufvzT0vY+e/yh9Ovfh5c9e5oH3HzgskQChxFCbjmkdqfZXh6pmfOLj5tNvZkj3IVQFqmib3JasTlkc0/4Y9lTsYdv+bVRUV3BM+jEc0/4YurbtSoe0Ds5dQKZBmkNDc2+gxDNcCpwaaUYR6QvkAP+uZfosYBZAnz59mjZKY46S2k7gDb1HvqK6gtmLZ3PJ4EvqfOApuEwwIQSv3MNvpSQA/br049gOxzLpuElUB6op2VPClOencKDqQCjOg9UH+e7L361zGwMaoFeHXkwZNIVZJ89ixbYVXLXgqlApZeHlC8nPyuetL99iyaYljM0e2+yuqFu7WJYUvgVMVNWr3eHvAiNV9YcR5v0JkBlpWjgrKZjmJppbN8Ov9L318eHVMHU1oEa8I0cDodsrC7IL+MXiX4Tq5kUk9HnoMUOp9FeysmxljW1ol9IudOtkRrsMtu/fzqWDL6Vn+548XPxwqPH0p2f8lDP6nMG2/dso3lJM2f4yjut6HAENUJBdwKisUbRNaVvv/jFHX0JVH4nIx8B1qvpufeu1pGCak0gne+/J3PuU688X/Ry/+kP3wAc0EPFe+kj3yCdJEgV9C9i+fzsry1YedntjQANR1c8nJyXTNrkt+yr3heZPSUphxrAZ3DPxHnYd2sUjxY/w8Vcfc1P+TYzrNy60jXZST3zNISkk4zQ0jwM24zQ0f0dVV4bNNxB4A8jRKIKxpGCOlkgPVoU3us5ePJs3N7wZulqv7YEn+KYO35sIgsv4A/7DSwCBb+7Jb6jkpGRUlVRfKi98+wVOzzqddintSElKQURC2zWm7xhOzTzV6udbibi3KahqtYhcj3PC9wFPqOpKEbkdKFbVBe6s04B50SQE07pFe8XamP5wamuk9T5Y5a26CX8IKngy96s/Yh1+sAroqhFXcfKxJ4faApIkif5d+/PVvq/YfWh3xCdwx/cbz6WDL2VM3zGk+dL4aOtHvFvyLidknECvjr3YvGczNyy8oUbVVG3bn5+Vb1f8plbWzYVplhrT82Rwubru0qmrDt97z733Nk1v1whBkbpL8Db8+pJ8XDzoYrbv387ijYsPWzbVl0obXxv2VO4BYHD3wQw9Zig5nXPo1rYb1YFqNu7aSKovlam5U6O+R96qeExd4l59FCuWFFqGuhpnI3V0BoRO0sGT8OyC2TXu4vFW59RXdRMUqQrH+/BWpJKCT5w2Ab86jbi90nvRIa0DFf4KyvaXsbdyb41t9omPGcNm0L19d3Yf2k1WpywuOeESBmYMjOWuNgawpGCasfr61Qk/cXs7Ogu/l95bEqjtqVzv7Ze11uGHJwpJ4oRuJ5DsS+aEjBPomNaRT7d/yoHqA6z7eh37KveRkpRCl7ZdyO6cTb8u/agOVFMdqKZrm65kdswkp0sOI3uPZOfBnbz15Vt2FW/iKu5tCsYERds5WnBcsCMzb0dn/oCfmSNmsn7n+lBJwNv/TnCdwW4Txud80/ult44/LdmpKirdU8r2/dv5YucX9GjfgxdWvXBY9wgBDfDFzi84tsOxzF8zn4PVB0lPTSerYxbn9D+HS064hPOOP4/01PSo9sGoPqOafL8aEwuWFExMRSoVROoc7eOtH4f6qq/t1s5gX/lvb3r7sGXvfPtOurXrdtg6ZxfMBuCp5U9R4a9AEDI7ZtK1bVce+vAhPt3+acQnb3t16EV2p2y+NeRbXJt3LWnJaagqFf4K2iS3OZq7zpi4sKRgYspbKgg+iTu7YHbo3v3w9oOZI2Yyfdg3fdFMHza9RttD+LIV/gqSk5K5bMhlBDTAmL5jWL9zPS+sfgG/+kO3fqanptM+tT2d0joxeeBkJh43kRHHjmDnoZ1s27eN3h1706N9jxrbICKWEEyrYW0KJqYi1fV72wLufPvO0ENdPvFxx9g7uPWMW+td7+Y9m5n24jTe3vR2rfP07tCbKSdM4Zz+5zC6z2g6pHVoyk0zJqFYm4KJi/D2g/ysfAqnFx52V5C3LSC8Kin4UBiAqvLxVx9TvKWY3Yd2s7tiN+t3rmdV2SpWlq3EH3ASiaqS4kth7pS59OvSj6/2fUWXtl0Y2XskSZIUv51hTAKykoJptGjfmVvfcwbvbHqHp1c8za5Du9h1aBdtU9riEx/vlrzL1n1bQ98nSKhL5GHHDOPqEVfz1b6v7P58Y6Jgt6SamKjrIbBI/em3SW4TSgBvrn+TZ1Y8Q16vPMb0HcPGXRv5x+f/4B9r/8HmvZtJ9aVyYo8TqQpUcaj6EMOOGcZ5A87jzOwzyWiXQfuU9ohInPeAMYnJkoI5YuF9/3hf1hJtR24+8XH1iKup9Ffy/Mrn2V+1/7Dv6JDagQnHTeDiQRdzwcAL6JjW8ahtnzGtibUpmEap7aniGg+BRXgpeniXz1X+KgIa4JGlj5Cems7U3KlMy51GkiRRfrCcbm27MarPKFJ9qXHeamNMkCUFE+Kt9/eWBMJfpO59CMzbX1BBdgHHdzuezXucqqAKfwWDMgZxU/5NTM2dGvWDXsaY+LGkYEK8zxQESwKR3rN73oDzyO2Ry1f7vqJTWid6pvekU1on7n73bl5b+xoV/gouGnQRt4y6hVMzI75szxjTTFlSaMXCbx8N3h5a4a8gSZIYlDGIPRV76NG+B0mSxOfln7Pz0E5e+uwlXvrspRrr65nekx+c/ANmnjyT3B65cdgiY8yRsqTQSoXfInr32XezcN1CUn2pHKw+CAp+9dOvSz8OVh+k0l/J2cedzbBjhjGk+xAGZgwkp3MOeyr2sGHXBqr8VZyWeZrTzmCMSViWFFqhYBfTFf4KAhrgUPUhrn/9esDpHfS+ifdxTd41UXXt0D25O93bd491yMaYo8SSQisT3u0E1Hx5zMGqg9bXjzGtlPUB0Mp4u5gGaJ/Snpvyb6JtsvMUcXhXE8aY1sVKCi1cpK4oUnwp+Kud10w+e8mzXDDwAi454RLrLsIYY0mhJaqtKwog1B1Femo6T1z4BBcMvACwl7kbYxyWFFqYiA+g8U1XFAENMLLXSJ66+CmO73Z8HCM1xjRHlhRamPAH0LwdyCWRRFpyGvdOvNcSgjEmIksKLYz3/QQpSSlU+Cvo3aE3N552I1X+KmszMMbUyZJCCxN8qc2ijYt4afVLfLHzC5Zfs5xu7brFOzRjTAKwW1JbkKKSIu58+04AhnQfQvHWYm4fe7slBGNM1Kyk0EKEd1vRtW1XBncfzDV518Q7NGNMAolpUhCRScAfAR/wmKr+LsI83wZmAwosV9XvxDKmliZ4++mm3ZtCDcyHqg+xee9mnpj8BMlJlveNMdGL2RlDRHzAQ8DZQCnwoYgsUNVVnnkGALcCo1R1p4j0iFU8LUltL8JJTkpG/UqAAGf3O5sJx02Id6jGmAQTy8vIkcA6VV0PICLzgMnAKs88M4GHVHUngKpuj2E8LUJtL8IhAGf2PZN/b/w3p2eezqvfeTXeoRpjElAsG5p7AyWe4VJ3nNfxwPEi8o6IvOdWN9UgIrNEpFhEisvKymIUbvPn7d3Ur34CgQA+8eETHyLCoo2LOCvnLApnFNorLo0xjRLLpCARxmnYcDIwACgApgGPiUjnGgupPqqqeaqa17176+ymOVhCeHP9mwTUfSdychq/KvgVmR0zqQ5UMzV3KvOnzrceTo0xjRbL6qNSIMsznAlsiTDPe6paBWwQkTU4SeLDGMaVkLy9myaRxPic8Vx+4uX8+M0fc6DqAE9f/DRXDL0i3mEaYxJcLEsKHwIDRCRHRFKBqcCCsHleBsYCiEgGTnXS+hjGlLCCTyr7xEdachqXDL6EG964gRRfCh/M/MASgjGmScSspKCq1SJyPfAGzi2pT6jqShG5HShW1QXutAkisgrwA/9PVctjFVMiCz6pvHjjYo7tcCzXvXYdvTv05l/f/Rd9O/eNd3jGmBZCVMOr+Zu3vLw8LS4ujncYceMP+Dnt8dMo3VPKsh8s45j0Y+IdkjEmAYjIUlXNq28+e7Ipwfz5oz9TvKWYuVPmWkIwxjQ56/sogZTtL+OnhT9lbPZYpuVOi3c4xpgWyJJCAvnxmz9mb+VeHjz3wcPek2CMMU3FkkKCeGXNKzy57Eluzr+Zwd0HxzscY0wLZW0KzVSwf6OC7AL6denHVQuuYtgxw5hdMDveoRljWjBLCs1QeDfYI44dwZ6KPSyasYi05LR4h2eMacEsKTRD3vcsV1RX8E7JO9w38T6G9BgS79CMMS2ctSk0Q96nlwME6N6uO6f0OiXeYRljWgFLCs1Q8Onl0zJPA6D8QDlnP302RSVFcY7MGNPSWVJoRoLvWC4qKWLoMUNZ/tVyAAIEqPRXsnjj4vgGaIxp8axNoZkIb1yeMWwG+6r2keZLozpQTaovlYLsgniHaYxp4SwpNBPexuVKfyVzls3h/OPP56ejfxq6NTU/Kz/eYRpjWjhLCs1AUUkRm3ZvIjkpGQIQ0AApvhT+OOmP9OvSz5KBMeaosaQQZ95qI1+Sj+O7Hc/qHav565S/0q9Lv3iHZ4xpZSwpxJm32kj9yuodq7ll1C1cMPCCeIdmjGmF7O6jOPM+k6Ao/br0446z7oh3WMaYVsqSQpwFn0m4Ju8aFOWm/JuctgVjjIkDSwrNQH5WPlX+Ktomt+XyEy+PdzjGmFbMkkIzsLdiL3/99K9clnsZndp0inc4xphWzJJCMzDv03nsq9zHrBGz4h2KMaaVs6TQDDz60aPk9sgN9XVkjDHxYkkhzp5f+TzFW4q55uRr7BWbxpi4s6QQRyW7S/jBqz9gZO+RzDrZqo6MMfFnSSFOlmxawplPnsmhqkPMnTKXFF9KvEMyxhhLCvFQVFLEWU+dxYZdG/Crn7L9ZfEOyRhjAEsKcfHGF29QFagCnM7v7D0JxpjmIqZJQUQmicgaEVknIrdEmP49ESkTkWXuv6tjGU9zESwZ+MRn70kwxjQrMetPQUR8wEPA2UAp8KGILFDVVWGzPqeq18cqjubmQNUBXlj9AiN7j+SigRfZexKMMc1KLDvZGQmsU9X1ACIyD5gMhCeFVuXRpY+yff92Xvz2i4zuMzre4RhjzGFiWX3UGyjxDJe648JdIiIrROQFEcmKYTxxd6j6EHe9cxdn9j3TEoIxplmKZUkh0pNYGjb8CvCsqlaIyDXAU8BZNVYkMguYBdCnT5+mjjPmikqKWLxxMYKwdd9WJh43kaKSIqs2MsY0O6LDzjKGAAAeiklEQVQafp5uohWL5AOzVXWiO3wrgKreWcv8PuBrVa2zR7i8vDwtLi5u6nBjxvtmNVUlQCDUwFw4vdASgzHmqBCRpaqaV998saw++hAYICI5IpIKTAUWeGcQkWM9gxcCq2MYT1x436wWIACAX/1U+ivtVlRjTLMTs+ojVa0WkeuBNwAf8ISqrhSR24FiVV0A/EhELgSqga+B78UqnngJvlmtwl9BQAOkJKUQ0IDdimqMaZZiVn0UK4lWfQROFdJN/7yJpVuX8vp3Xuf9ze/brajGmKMq2uoje+/jUXBa5mls2buFicdN5Kx+Z3FWvxpt6cYY0yxYNxdHwfJty/ly95dcNOiieIdijDF1sqRwFLz82cskSRIXHH9BvEMxxpg6WVI4Cv6++u+cnnU63dt3j3coxhhTJ0sKMba6bDWfbP+Ebw/+drxDMcaYellSiLHnVj6HIFwy+JJ4h2KMMfWyu49ipKikiEUbF/HksicZ03cMvTr0indIxhhTr6iSgogcB5S6fRQVAEOBv6jqrlgGl6iCXVsEH1j71uBvxTskY4yJSrTVRy8CfhHpDzwO5AB/jVlUCS7YtUVAnW4tUpNT4xyRMcZEJ9qkEFDVauBi4H5V/W/g2HqWabWCXVsAJEkS5w84P84RGWNMdKJNClUiMg2YAbzqjkuJTUiJLz8rn/877/8A+H+n/z/rzsIYkzCiTQrfB/KB36jqBhHJAZ6JXViJ7/Pyz/GJj5tPvzneoRhjTNSiamh236v8IwAR6QJ0UNXfxTKwRKaqvLj6RQqyC8holxHvcIwxJmpRlRREZLGIdBSRrsByYI6I3Bvb0BLXqrJVfF7+OVNOmBLvUIwxpkGirT7qpKp7gCnAHFU9GRgfu7AS24urX0QQLh50cbxDMcaYBok2KSS7b0n7Nt80NJtaBPs6OraD3aBljEks0SaF23HeoPaFqn4oIv2AtbELK3F98fUXLN+23KqOjDEJKdqG5r8Bf/MMrwesM58I7i1ymlqyO2fHNxBjjGmEaBuaM0XkJRHZLiLbRORFEcmMdXCJpqikiD8t/RMAV/z9CopKiuIckTHGNEy01UdzgAVAL6A38Io7zni88cUboa4tKv2VLN64OL4BGWNMA0WbFLqr6hxVrXb/PQnYG2PCdG3bFXC6tkj1pVKQXRDfgIwxpoGiTQo7ROQKEfG5/64AymMZWCLaeXAnALedcRuF0wutewtjTMKJ9n0KVwIPAvcBCryL0/WF8Xin5B1O7HEivxr7q3iHYowxjRJVSUFVN6nqharaXVV7qOpFOA+yGZc/4Oe90vcYlTUq3qEYY0yjHcnrOP+nyaJoAT7Z/gl7K/cyqo8lBWNM4jqS13FKk0WR4IpKirhzyZ0AVlIwxiS0I0kK2mRRJLDgqzcPVh8EYOvereR0yYlzVMYY0zh1Vh+JyF4R2RPh316cZxbqJCKTRGSNiKwTkVvqmO9SEVERyWvENsRV8NWbQf/58j9xjMYYY45MnSUFVe3Q2BWLiA94CDgbKAU+FJEF7rsZvPN1wHlXw/uN/a54KsguIMWXgr/aT0pSij2bYIxJaEfS0FyfkcA6VV2vqpXAPGByhPnuAO4CDsUwlpjJz8rnZ2f8DICHz3vYnk0wxiS0WCaF3kCJZ7jUHRciIsOBLFWtsztuEZklIsUiUlxWVtb0kR6hLXu3kJ6azvRh0+MdijHGHJFYJoVIdyeFGqdFJAnnYbib6luRqj6qqnmqmte9e/PrXaNwQyFj+o4hxZcS71CMMeaIxDIplAJZnuFMYItnuAOQCywWkY3AacCCRGtsLt1TyuflnzMuZ1y8QzHGmCN2JLek1udDYICI5ACbganAd4ITVXU3EHqrvYgsBm5W1eIYxtSkikqKQu9PsKRgjGkJYpYUVLVaRK7HeWObD3hCVVeKyO1AsaouiNV3Hw3hzyfsq9wX54iMMebIxbKkgKq+BrwWNu4XtcxbEMtYmpr3+QRBeOvLt6yLC2NMwotlm0KLFnw+ASA5KdmeTzDGtAiWFBopPyuf6065DoC5U+ba8wnGmBbBksIR2LBrA3069eHSwZfGOxRjjGkSlhQaSVVZvHEx43LGIWIdxhpjWoaYNjS3REUlRSzeuJjB3Qfz9cGvOaXXKfEOyRhjmowlhQYI3oZa6a8kOcnZdYMyBsU5KmOMaTqWFBogeBuqX/2o3+mxw5KCMaYlsTaFBijILiDVl4pPfCRJEu2S29EzvWe8wzLGmCZjJYUGyM/Kp3B6IYs3Lubvn/0dQayR2RjTolhJoYHys/K59Yxb2bZvm1UdGWNaHEsKjbCvch8le0oY2G1gvEMxxpgmZUmhET4v/xywRmZjTMtjSaER1uxYA1hSMMa0PJYUGuGzHZ+RJEn079o/3qEYY0yTsqTQCJ+Vf0ZO5xzSktPiHYoxxjQpSwqNsGbHGqs6Msa0SPacQpSCfR6N6TuGNeVr7PWbxpgWyZJCFLx9HqX4UjhUfchKCsaYFsmSQhQi9Xk0MMOeUTDGtDzWphAFb59HPvEBdjuqMaZlspJCFLx9Hr1b8i5FpUV0b9c93mEZY0yTs6QQpfysfPKz8hn04CDys/KtIzxjTItk1UcNULa/jDXlaxidNTreoRhjTExYUmiAd0veBWBUn1FxjsQYY2LDkkIDvFPyDqm+VPJ65cU7FGOMiQlLCg2wZNMS8nrl0Sa5TbxDMcaYmIhpUhCRSSKyRkTWicgtEaZfIyKfiMgyEVkiIoNjGc+ROFR9iKVbl1p7gjGmRYvZ3Uci4gMeAs4GSoEPRWSBqq7yzPZXVf2TO/+FwL3ApFjF1FDBri0Ksgvwq59Kf6W1JxhjWrRY3pI6ElinqusBRGQeMBkIJQVV3eOZvz2gMYynQbxdW6T6Uplx0gwATs86Pc6RGWNM7MQyKfQGSjzDpcCp4TOJyHXA/wCpwFmRViQis4BZAH369GnyQCPxdm1R6a9k8cbFDMoYREa7jKPy/cYYEw+xbFOI9HRXjZKAqj6kqscBPwFui7QiVX1UVfNUNa9796PzJLG3a4tUXyqb92y29gRjTIsXy5JCKZDlGc4EttQx/zzg4RjG0yDeri16d+jNjPkzGN3HkoIxpmWLZUnhQ2CAiOSISCowFVjgnUFEBngGzwPWxjCeBsvPyufWM25l897NAEzsPzHOERljTGzFrKSgqtUicj3wBuADnlDVlSJyO1CsqguA60VkPFAF7ARmxCqeI/Hautc4+diT6ZneM96hGGNMTMW0QzxVfQ14LWzcLzyfb4jl9zeFrw9+zbsl7/KzM34W71CMMSbm7Inmevzzi38S0ADnDjg33qEYY0zMWVKox2trXyOjXQan9Dol3qEYY0zM2fsUIgg+yXxG3zN4fd3rTOo/CV+SL95hGWNMzFlSCON9kjk5KZkKfwXn9reqI2NM62BJIYz3SeaAP4AgTDhuQrzDMsaYo8LaFMJ4n2QGyO2RS7d23eIclTHGHB2WFFxFJUXc+fadABROL+Tm029GUS4/8fI4R2aMMUePVR9Rs0fUwumF9O3UF4ALB14Y5+iMMeboadVJIXiX0abdm2r0iPr2prfp37U/gzIGxTtMY4w5alptUvCWDnxJPpKTkiEAqb5URvYeyez/zOa6U65DJFJnr8YY0zK12qTgvcuIAMwcMZM+nfpQkF3A1n1bqfRXWtWRMabVabVJIXiXUbAdYfqw6eRn5QPw/fnfp3ObzozKsldvGmNal1aZFIJtCfdPup/yA+UUZBeEEoI/4OfVz1/l3AHnkuJLiXOkxhhzdLW6pBDpTqNgQgCnr6MdB3Zw4fFWdWSMaX1a3XMKkd69HHSw6iA3vnEjgzIGcfEJF8cvSGOMiZNWV1IIb0soyC4ITfv9O79n/c71FE4vJNWXGr8gjYmRqqoqSktLOXToULxDMTHSpk0bMjMzSUlpXPV3q0sK3ncve9sSvvj6C3635HdMzZ3KWTlnxTlKY2KjtLSUDh06kJ2dbbdbt0CqSnl5OaWlpeTk5DRqHa0mKQQbl4OJwNuO4A/4+cGrPyDVl8o9E+6JY5TGxNahQ4csIbRgIkK3bt0oKytr9DpaRVKor3H5ljdvoXBDIY9f+Di9OvSKY6TGxJ4lhJbtSI9vq2horqtx+enlT/OHoj9w/SnXc+XwK+MXpDHGNAOtIil4u8P2Ni5/su0TZr4yk7HZY7l34r3xDdKYVqC8vJyTTjqJk046iZ49e9K7d+/QcGVlZVTr+P73v8+aNWvqnOehhx5i7ty5TRFyk7vtttu4//77a4yfMWMG3bt356STTopDVN9oFdVHtTUu/3zRz2mb0pbnv/W8PahmzFHQrVs3li1bBsDs2bNJT0/n5ptvPmweVUVVSUqKfM06Z86cer/nuuuuO/Jgj7Irr7yS6667jlmzZsU1jlaRFIAajcvLvlrG/DXz+VXBr8holxHHyIyJjxsX3siyr5Y16TpP6nkS90+qeRVcn3Xr1nHRRRcxevRo3n//fV599VV+9atf8dFHH3Hw4EEuu+wyfvGLXwAwevRoHnzwQXJzc8nIyOCaa67h9ddfp127dsyfP58ePXpw2223kZGRwY033sjo0aMZPXo0//73v9m9ezdz5szh9NNPZ//+/UyfPp1169YxePBg1q5dy2OPPVbjSv2Xv/wlr732GgcPHmT06NE8/PDDiAiff/4511xzDeXl5fh8Pv7+97+TnZ3Nb3/7W5599lmSkpI4//zz+c1vfhPVPjjzzDNZt25dg/ddU2sV1UeR/PqtX9MxrSM/OvVH8Q7FGAOsWrWKq666io8//pjevXvzu9/9juLiYpYvX86//vUvVq1aVWOZ3bt3c+aZZ7J8+XLy8/N54oknIq5bVfnggw+4++67uf322wH43//9X3r27Mny5cu55ZZb+PjjjyMue8MNN/Dhhx/yySefsHv3bhYuXAjAtGnT+O///m+WL1/Ou+++S48ePXjllVd4/fXX+eCDD1i+fDk33XRTE+2do6fVlBS8Pt3+KS+ufpGfj/k5ndt0jnc4xsRFY67oY+m4447jlFNOCQ0/++yzPP7441RXV7NlyxZWrVrF4MGDD1umbdu2nHPOOQCcfPLJvP322xHXPWXKlNA8GzduBGDJkiX85Cc/AWDYsGEMGTIk4rKFhYXcfffdHDp0iB07dnDyySdz2mmnsWPHDi644ALAeWAM4M033+TKK6+kbdu2AHTt2rUxuyKuWmVS+M3bvyE9NZ0bT7sx3qEYY1zt27cPfV67di1//OMf+eCDD+jcuTNXXHFFxKewU1O/6XnA5/NRXV0dcd1paWk15lHVemM6cOAA119/PR999BG9e/fmtttuC8UR6dZPVU34W35jWn0kIpNEZI2IrBORWyJM/x8RWSUiK0SkUET6xjIegIAGWLBmAVeceAVd2yZeFjemNdizZw8dOnSgY8eObN26lTfeeKPJv2P06NE8//zzAHzyyScRq6cOHjxIUlISGRkZ7N27lxdffBGALl26kJGRwSuvvAI4DwUeOHCACRMm8Pjjj3Pw4EEAvv766yaPO9ZilhRExAc8BJwDDAamicjgsNk+BvJUdSjwAnBXrOIJ2rxnMweqDjCs57BYf5UxppFGjBjB4MGDyc3NZebMmYwa1fTvNvnhD3/I5s2bGTp0KPfccw+5ubl06tTpsHm6devGjBkzyM3N5eKLL+bUU08NTZs7dy733HMPQ4cOZfTo0ZSVlXH++eczadIk8vLyOOmkk7jvvvsifvfs2bPJzMwkMzOT7OxsAL71rW9xxhlnsGrVKjIzM3nyySebfJujIdEUoRq1YpF8YLaqTnSHbwVQ1TtrmX848KCq1nn08/LytLi4uNFxvbn+Tc5++mz+Pf3fjM0Z2+j1GJOIVq9ezQknnBDvMJqF6upqqquradOmDWvXrmXChAmsXbuW5OTEr1WPdJxFZKmq5tW3bCy3vjdQ4hkuBU6tZV6Aq4DXI00QkVnALIA+ffocUVBrdjgPvQzMGHhE6zHGJLZ9+/Yxbtw4qqurUVUeeeSRFpEQjlQs90Ck1paIxRIRuQLIA86MNF1VHwUeBaekcCRBrSlfQ3pqOsemH3skqzHGJLjOnTuzdOnSeIfR7MQyKZQCWZ7hTGBL+EwiMh74GXCmqlbEMB4APtvxGQO7DUz4OwSMMSYWYnn30YfAABHJEZFUYCqwwDuD247wCHChqm6PYSwha8rXWNWRMcbUImZJQVWrgeuBN4DVwPOqulJEbheR4AuQ7wbSgb+JyDIRWVDL6prEgaoDbNq9iYHdLCkYY0wkMW1VUdXXgNfCxv3C83l8LL8/3NrytQAMyhh0NL/WGGMSRqvq+2hNuXvnkZUUjImLgoKCGg+i3X///fzXf/1Xnculp6cDsGXLFi699NJa113f7er3338/Bw4cCA2fe+657Nq1K5rQj6rFixdz/vnn1xj/4IMP0r9/f0SEHTt2xOS7W1dScG9HHdBtQJwjMSZxFJUUcefbd1JUUnTE65o2bRrz5s07bNy8efOYNm1aVMv36tWLF154odHfH54UXnvtNTp3Tpz+z0aNGsWbb75J376x6/yhdSWF8jX06dSHdint4h2KMQkh+Crbny/6OeP+Mu6IE8Oll17Kq6++SkWFc6Phxo0b2bJlC6NHjw49NzBixAhOPPFE5s+fX2P5jRs3kpubCzhdUEydOpWhQ4dy2WWXhbqWALj22mvJy8tjyJAh/PKXvwTggQceYMuWLYwdO5axY50HV7Ozs0NX3Pfeey+5ubnk5uaGXoKzceNGTjjhBGbOnMmQIUOYMGHCYd8T9Morr3DqqacyfPhwxo8fz7Zt2wDnWYjvf//7nHjiiQwdOjTUTcbChQsZMWIEw4YNY9y4cVHvv+HDh4eegI6VVvWkRvB2VGNMdCK9ytb7XpKG6tatGyNHjmThwoVMnjyZefPmcdlllyEitGnThpdeeomOHTuyY8cOTjvtNC688MJabx9/+OGHadeuHStWrGDFihWMGDEiNO03v/kNXbt2xe/3M27cOFasWMGPfvQj7r33XhYtWkRGxuHvUFm6dClz5szh/fffR1U59dRTOfPMM+nSpQtr167l2Wef5c9//jPf/va3efHFF7niiisOW3706NG89957iAiPPfYYd911F/fccw933HEHnTp14pNPPgFg586dlJWVMXPmTN566y1ycnKaXf9IraakoKrO7aiWFIyJWm2vsj0S3iokb9WRqvLTn/6UoUOHMn78eDZv3hy64o7krbfeCp2chw4dytChQ0PTnn/+eUaMGMHw4cNZuXJlxM7uvJYsWcLFF19M+/btSU9PZ8qUKaFuuHNyckIv3vF2ve1VWlrKxIkTOfHEE7n77rtZuXIl4HSl7X0LXJcuXXjvvfcYM2YMOTk5QPPrXrvVJIWt+7ayr3KfPaNgTAMEX2V7x9g7KJxeeESlhKCLLrqIwsLC0FvVglf4c+fOpaysjKVLl7Js2TKOOeaYiN1le0UqRWzYsIE//OEPFBYWsmLFCs4777x611NXH3DBbreh9u65f/jDH3L99dfzySef8Mgjj4S+L1JX2s29e+1WkxRCfR5ZScGYBsnPyufWM25tkoQAzp1EBQUFXHnllYc1MO/evZsePXqQkpLCokWL+PLLL+tcz5gxY5g7dy4An376KStWrACcbrfbt29Pp06d2LZtG6+//k2Xah06dGDv3r0R1/Xyyy9z4MAB9u/fz0svvcQZZ5wR9Tbt3r2b3r17A/DUU0+Fxk+YMIEHH3wwNLxz507y8/P5z3/+w4YNG4Dm171260kK5dYRnjHNxbRp01i+fDlTp04Njbv88sspLi4mLy+PuXPnMmhQ3c8TXXvttezbt4+hQ4dy1113MXLkSMB5i9rw4cMZMmQIV1555WHdbs+aNYtzzjkn1NAcNGLECL73ve8xcuRITj31VK6++mqGDx8e9fbMnj071PW1t73itttuY+fOneTm5jJs2DAWLVpE9+7defTRR5kyZQrDhg3jsssui7jOwsLCUPfamZmZFBUV8cADD5CZmUlpaSlDhw7l6quvjjrGaMWs6+xYaWzX2fM/m8+Ty5/kxW+/SJK0mlxozGGs6+zWobl2nd2sTB40mcmDJsc7DGOMadbsktkYY0yIJQVjWplEqzI2DXOkx9eSgjGtSJs2bSgvL7fE0EKpKuXl5bRp06bR62g1bQrGGEJ3rpSVlcU7FBMjbdq0ITMzs9HLW1IwphVJSUkJPUlrTCRWfWSMMSbEkoIxxpgQSwrGGGNCEu6JZhEpA+ruFKWmDCA2ryk6+mxbmifbluarJW3PkWxLX1XtXt9MCZcUGkNEiqN5vDsR2LY0T7YtzVdL2p6jsS1WfWSMMSbEkoIxxpiQ1pIUHo13AE3ItqV5sm1pvlrS9sR8W1pFm4IxxpjotJaSgjHGmChYUjDGGBPSopOCiEwSkTUisk5Ebol3PA0hIlkiskhEVovIShG5wR3fVUT+JSJr3f93iXes0RIRn4h8LCKvusM5IvK+uy3PiUhqvGOMloh0FpEXROQz9xjlJ+qxEZH/dn9jn4rIsyLSJlGOjYg8ISLbReRTz7iIx0EcD7jngxUiMiJ+kddUy7bc7f7GVojISyLS2TPtVndb1ojIxKaKo8UmBRHxAQ8B5wCDgWkiMji+UTVINXCTqp4AnAZc58Z/C1CoqgOAQnc4UdwArPYM/x64z92WncBVcYmqcf4ILFTVQcAwnO1KuGMjIr2BHwF5qpoL+ICpJM6xeRKYFDautuNwDjDA/TcLePgoxRitJ6m5Lf8CclV1KPA5cCuAey6YCgxxl/k/95x3xFpsUgBGAutUdb2qVgLzgIR5H6eqblXVj9zPe3FOOr1xtuEpd7angIviE2HDiEgmcB7wmDsswFnAC+4sibQtHYExwOMAqlqpqrtI0GOD01tyWxFJBtoBW0mQY6OqbwFfh42u7ThMBv6ijveAziJy7NGJtH6RtkVV/6mq1e7ge0CwT+zJwDxVrVDVDcA6nHPeEWvJSaE3UOIZLnXHJRwRyQaGA+8Dx6jqVnASB9AjfpE1yP3Aj4GAO9wN2OX5wSfS8ekHlAFz3Oqwx0SkPQl4bFR1M/AHYBNOMtgNLCVxjw3UfhwS/ZxwJfC6+zlm29KSk4JEGJdw99+KSDrwInCjqu6JdzyNISLnA9tVdal3dIRZE+X4JAMjgIdVdTiwnwSoKorErW+fDOQAvYD2ONUs4RLl2NQlYX9zIvIznCrlucFREWZrkm1pyUmhFMjyDGcCW+IUS6OISApOQpirqn93R28LFnnd/2+PV3wNMAq4UEQ24lTjnYVTcujsVllAYh2fUqBUVd93h1/ASRKJeGzGAxtUtUxVq4C/A6eTuMcGaj8OCXlOEJEZwPnA5frNg2Ux25aWnBQ+BAa4d1Gk4jTKLIhzTFFz69wfB1ar6r2eSQuAGe7nGcD8ox1bQ6nqraqaqarZOMfh36p6ObAIuNSdLSG2BUBVvwJKRGSgO2ocsIoEPDY41UaniUg79zcX3JaEPDau2o7DAmC6exfSacDuYDVTcyUik4CfABeq6gHPpAXAVBFJE5EcnMbzD5rkS1W1xf4DzsVpsf8C+Fm842lg7KNxioMrgGXuv3Nx6uILgbXu/7vGO9YGblcB8Kr7uZ/7Q14H/A1Ii3d8DdiOk4Bi9/i8DHRJ1GMD/Ar4DPgUeBpIS5RjAzyL0xZShXP1fFVtxwGnyuUh93zwCc4dV3Hfhnq2ZR1O20HwHPAnz/w/c7dlDXBOU8Vh3VwYY4wJacnVR8YYYxrIkoIxxpgQSwrGGGNCLCkYY4wJsaRgjDEmxJKCMS4R8YvIMs+/JntKWUSyvb1fGtNcJdc/izGtxkFVPSneQRgTT1ZSMKYeIrJRRH4vIh+4//q74/uKSKHb132hiPRxxx/j9n2/3P13ursqn4j82X13wT9FpK07/49EZJW7nnlx2kxjAEsKxni1Das+uswzbY+qjgQexOm3CffzX9Tp634u8IA7/gHgP6o6DKdPpJXu+AHAQ6o6BNgFXOKOvwUY7q7nmlhtnDHRsCeajXGJyD5VTY8wfiNwlqqudzsp/EpVu4nIDuBYVa1yx29V1QwRKQMyVbXCs45s4F/qvPgFEfkJkKKqvxaRhcA+nO4yXlbVfTHeVGNqZSUFY6KjtXyubZ5IKjyf/XzTpnceTp88JwNLPb2TGnPUWVIwJjqXef5f5H5+F6fXV4DLgSXu50LgWgi9l7pjbSsVkSQgS1UX4byEqDNQo7RizNFiVyTGfKOtiCzzDC9U1eBtqWki8j7OhdQ0d9yPgCdE5P/hvInt++74G4BHReQqnBLBtTi9X0biA54RkU44vXjep86rPY2JC2tTMKYebptCnqruiHcsxsSaVR8ZY4wJsZKCMcaYECspGGOMCbGkYIwxJsSSgjHGmBBLCsYYY0IsKRhjjAn5/83cK0K1cH9WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 130us/step - loss: 16.0184 - acc: 0.1743 - val_loss: 15.6090 - val_acc: 0.1940\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 15.2486 - acc: 0.2043 - val_loss: 14.8550 - val_acc: 0.2210\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 14.5078 - acc: 0.2167 - val_loss: 14.1243 - val_acc: 0.2450\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 13.7873 - acc: 0.2308 - val_loss: 13.4116 - val_acc: 0.2620\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 13.0844 - acc: 0.2424 - val_loss: 12.7166 - val_acc: 0.2790\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 12.3988 - acc: 0.2604 - val_loss: 12.0395 - val_acc: 0.2950\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 11.7312 - acc: 0.2903 - val_loss: 11.3792 - val_acc: 0.3250\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 11.0817 - acc: 0.3316 - val_loss: 10.7386 - val_acc: 0.3600\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 10.4513 - acc: 0.3827 - val_loss: 10.1170 - val_acc: 0.4100\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 9.8403 - acc: 0.4397 - val_loss: 9.5151 - val_acc: 0.4590\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 9.2490 - acc: 0.4861 - val_loss: 8.9356 - val_acc: 0.5280\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 8.6811 - acc: 0.5233 - val_loss: 8.3809 - val_acc: 0.5880\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 8.1388 - acc: 0.5672 - val_loss: 7.8506 - val_acc: 0.6010\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 7.6217 - acc: 0.5920 - val_loss: 7.3462 - val_acc: 0.6130\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 7.1285 - acc: 0.6123 - val_loss: 6.8652 - val_acc: 0.6300\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 6.6594 - acc: 0.6295 - val_loss: 6.4070 - val_acc: 0.6630\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 6.2138 - acc: 0.6492 - val_loss: 5.9750 - val_acc: 0.6750\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 5.7920 - acc: 0.6581 - val_loss: 5.5661 - val_acc: 0.6830\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 5.3945 - acc: 0.6663 - val_loss: 5.1808 - val_acc: 0.6900\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.0204 - acc: 0.6761 - val_loss: 4.8179 - val_acc: 0.6940\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 4.6699 - acc: 0.6811 - val_loss: 4.4812 - val_acc: 0.7060\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 4.3428 - acc: 0.6884 - val_loss: 4.1653 - val_acc: 0.7080\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.0385 - acc: 0.6929 - val_loss: 3.8712 - val_acc: 0.7000\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.7573 - acc: 0.6959 - val_loss: 3.6021 - val_acc: 0.7010\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.4988 - acc: 0.6980 - val_loss: 3.3544 - val_acc: 0.7030\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 3.2629 - acc: 0.6973 - val_loss: 3.1321 - val_acc: 0.7040\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 3.0495 - acc: 0.6993 - val_loss: 2.9324 - val_acc: 0.7120\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.8577 - acc: 0.7031 - val_loss: 2.7495 - val_acc: 0.7090\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.6873 - acc: 0.7031 - val_loss: 2.5898 - val_acc: 0.7100\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.5370 - acc: 0.7031 - val_loss: 2.4476 - val_acc: 0.7030\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.4074 - acc: 0.7004 - val_loss: 2.3286 - val_acc: 0.7080\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.2970 - acc: 0.7013 - val_loss: 2.2278 - val_acc: 0.7060\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.2053 - acc: 0.6999 - val_loss: 2.1435 - val_acc: 0.7090\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1312 - acc: 0.7004 - val_loss: 2.0810 - val_acc: 0.7030\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0741 - acc: 0.7008 - val_loss: 2.0309 - val_acc: 0.7020\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 2.0311 - acc: 0.7015 - val_loss: 1.9925 - val_acc: 0.7050\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9988 - acc: 0.7023 - val_loss: 1.9631 - val_acc: 0.7040\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9730 - acc: 0.7019 - val_loss: 1.9415 - val_acc: 0.7060\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9510 - acc: 0.7015 - val_loss: 1.9210 - val_acc: 0.7070\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9312 - acc: 0.7027 - val_loss: 1.8995 - val_acc: 0.7050\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9134 - acc: 0.7004 - val_loss: 1.8871 - val_acc: 0.7020\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8965 - acc: 0.7013 - val_loss: 1.8649 - val_acc: 0.7050\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8805 - acc: 0.7011 - val_loss: 1.8517 - val_acc: 0.7070\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8656 - acc: 0.7020 - val_loss: 1.8354 - val_acc: 0.7010\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8509 - acc: 0.6999 - val_loss: 1.8226 - val_acc: 0.7010\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8375 - acc: 0.7005 - val_loss: 1.8131 - val_acc: 0.6980\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8244 - acc: 0.7015 - val_loss: 1.7961 - val_acc: 0.7080\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8117 - acc: 0.7031 - val_loss: 1.7846 - val_acc: 0.7010\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7994 - acc: 0.7029 - val_loss: 1.7712 - val_acc: 0.7030\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7873 - acc: 0.7041 - val_loss: 1.7587 - val_acc: 0.7030\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7756 - acc: 0.7032 - val_loss: 1.7503 - val_acc: 0.7050\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7648 - acc: 0.7039 - val_loss: 1.7356 - val_acc: 0.7080\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7537 - acc: 0.7021 - val_loss: 1.7272 - val_acc: 0.7070\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7431 - acc: 0.7039 - val_loss: 1.7230 - val_acc: 0.7020\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7326 - acc: 0.7036 - val_loss: 1.7049 - val_acc: 0.7040\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.7221 - acc: 0.7037 - val_loss: 1.6983 - val_acc: 0.7090\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7128 - acc: 0.7045 - val_loss: 1.6856 - val_acc: 0.7000\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7027 - acc: 0.7049 - val_loss: 1.6778 - val_acc: 0.7070\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.6934 - acc: 0.7064 - val_loss: 1.6679 - val_acc: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6835 - acc: 0.7072 - val_loss: 1.6566 - val_acc: 0.7060\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6744 - acc: 0.7045 - val_loss: 1.6485 - val_acc: 0.7160\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6649 - acc: 0.7087 - val_loss: 1.6403 - val_acc: 0.7110\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6560 - acc: 0.7064 - val_loss: 1.6426 - val_acc: 0.7090\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6475 - acc: 0.7091 - val_loss: 1.6214 - val_acc: 0.7130\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6384 - acc: 0.7083 - val_loss: 1.6104 - val_acc: 0.7120\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6296 - acc: 0.7088 - val_loss: 1.6094 - val_acc: 0.7180\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6216 - acc: 0.7084 - val_loss: 1.5966 - val_acc: 0.7170\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6133 - acc: 0.7113 - val_loss: 1.5877 - val_acc: 0.7150\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.6049 - acc: 0.7091 - val_loss: 1.5831 - val_acc: 0.7230\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5971 - acc: 0.7115 - val_loss: 1.5728 - val_acc: 0.7220\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5894 - acc: 0.7124 - val_loss: 1.5698 - val_acc: 0.7210\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5817 - acc: 0.7095 - val_loss: 1.5658 - val_acc: 0.7210\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5739 - acc: 0.7104 - val_loss: 1.5484 - val_acc: 0.7200\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5664 - acc: 0.7111 - val_loss: 1.5414 - val_acc: 0.7250\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5586 - acc: 0.7132 - val_loss: 1.5336 - val_acc: 0.7250\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5504 - acc: 0.7124 - val_loss: 1.5272 - val_acc: 0.7180\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5432 - acc: 0.7116 - val_loss: 1.5222 - val_acc: 0.7270\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5364 - acc: 0.7137 - val_loss: 1.5133 - val_acc: 0.7250\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5289 - acc: 0.7167 - val_loss: 1.5132 - val_acc: 0.7280\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5223 - acc: 0.7132 - val_loss: 1.5056 - val_acc: 0.7240\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5153 - acc: 0.7152 - val_loss: 1.4926 - val_acc: 0.7260\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5082 - acc: 0.7163 - val_loss: 1.4859 - val_acc: 0.7280\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.5012 - acc: 0.7157 - val_loss: 1.4793 - val_acc: 0.7300\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4945 - acc: 0.7159 - val_loss: 1.4734 - val_acc: 0.7270\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.4880 - acc: 0.7173 - val_loss: 1.4678 - val_acc: 0.7280\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4813 - acc: 0.7164 - val_loss: 1.4656 - val_acc: 0.7310\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4753 - acc: 0.7181 - val_loss: 1.4552 - val_acc: 0.7290\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4686 - acc: 0.7175 - val_loss: 1.4471 - val_acc: 0.7290\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4625 - acc: 0.7185 - val_loss: 1.4384 - val_acc: 0.7310\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4560 - acc: 0.7184 - val_loss: 1.4328 - val_acc: 0.7320\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4498 - acc: 0.7176 - val_loss: 1.4291 - val_acc: 0.7280\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4436 - acc: 0.7185 - val_loss: 1.4237 - val_acc: 0.7360\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4376 - acc: 0.7199 - val_loss: 1.4181 - val_acc: 0.7370\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4322 - acc: 0.7209 - val_loss: 1.4158 - val_acc: 0.7310\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4256 - acc: 0.7197 - val_loss: 1.4054 - val_acc: 0.7320\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4201 - acc: 0.7208 - val_loss: 1.3987 - val_acc: 0.7340\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4141 - acc: 0.7207 - val_loss: 1.3952 - val_acc: 0.7390\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4086 - acc: 0.7201 - val_loss: 1.3889 - val_acc: 0.7310\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4029 - acc: 0.7204 - val_loss: 1.3876 - val_acc: 0.7370\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3976 - acc: 0.7212 - val_loss: 1.3830 - val_acc: 0.7330\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.3925 - acc: 0.7225 - val_loss: 1.3746 - val_acc: 0.7340\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.3862 - acc: 0.7225 - val_loss: 1.3711 - val_acc: 0.7360\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.3814 - acc: 0.7219 - val_loss: 1.3613 - val_acc: 0.7340\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3753 - acc: 0.7233 - val_loss: 1.3549 - val_acc: 0.7370\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3705 - acc: 0.7248 - val_loss: 1.3537 - val_acc: 0.7370\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3653 - acc: 0.7257 - val_loss: 1.3485 - val_acc: 0.7360\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.3600 - acc: 0.7239 - val_loss: 1.3409 - val_acc: 0.7340\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3551 - acc: 0.7253 - val_loss: 1.3380 - val_acc: 0.7400\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3499 - acc: 0.7257 - val_loss: 1.3386 - val_acc: 0.7370\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3451 - acc: 0.7253 - val_loss: 1.3340 - val_acc: 0.7340\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3402 - acc: 0.7252 - val_loss: 1.3228 - val_acc: 0.7360\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3351 - acc: 0.7271 - val_loss: 1.3180 - val_acc: 0.7370\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3307 - acc: 0.7261 - val_loss: 1.3115 - val_acc: 0.7380\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3257 - acc: 0.7263 - val_loss: 1.3094 - val_acc: 0.7370\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3206 - acc: 0.7275 - val_loss: 1.3040 - val_acc: 0.7370\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3161 - acc: 0.7265 - val_loss: 1.2999 - val_acc: 0.7390\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3119 - acc: 0.7280 - val_loss: 1.3001 - val_acc: 0.7360\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3072 - acc: 0.7277 - val_loss: 1.2905 - val_acc: 0.7400\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3026 - acc: 0.7273 - val_loss: 1.2906 - val_acc: 0.7350\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2987 - acc: 0.7269 - val_loss: 1.2832 - val_acc: 0.7370\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2933 - acc: 0.7289 - val_loss: 1.2842 - val_acc: 0.7390\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2898 - acc: 0.7288 - val_loss: 1.2756 - val_acc: 0.7380\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2854 - acc: 0.7279 - val_loss: 1.2713 - val_acc: 0.7380\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2808 - acc: 0.7285 - val_loss: 1.2641 - val_acc: 0.7420\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2762 - acc: 0.7312 - val_loss: 1.2634 - val_acc: 0.7420\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2722 - acc: 0.7295 - val_loss: 1.2574 - val_acc: 0.7400\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2684 - acc: 0.7295 - val_loss: 1.2524 - val_acc: 0.7430\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2646 - acc: 0.7292 - val_loss: 1.2523 - val_acc: 0.7420\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2603 - acc: 0.7315 - val_loss: 1.2461 - val_acc: 0.7430\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2565 - acc: 0.7301 - val_loss: 1.2432 - val_acc: 0.7400\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2519 - acc: 0.7320 - val_loss: 1.2379 - val_acc: 0.7420\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2481 - acc: 0.7301 - val_loss: 1.2330 - val_acc: 0.7460\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2441 - acc: 0.7333 - val_loss: 1.2324 - val_acc: 0.7400\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2403 - acc: 0.7327 - val_loss: 1.2273 - val_acc: 0.7480\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2360 - acc: 0.7317 - val_loss: 1.2277 - val_acc: 0.7380\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2328 - acc: 0.7316 - val_loss: 1.2207 - val_acc: 0.7390\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2289 - acc: 0.7327 - val_loss: 1.2171 - val_acc: 0.7410\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2250 - acc: 0.7341 - val_loss: 1.2128 - val_acc: 0.7430\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2213 - acc: 0.7340 - val_loss: 1.2144 - val_acc: 0.7390\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2177 - acc: 0.7355 - val_loss: 1.2073 - val_acc: 0.7460\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.2143 - acc: 0.7336 - val_loss: 1.2002 - val_acc: 0.7440\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2103 - acc: 0.7351 - val_loss: 1.2002 - val_acc: 0.7410\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2068 - acc: 0.7343 - val_loss: 1.1919 - val_acc: 0.7450\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2032 - acc: 0.7343 - val_loss: 1.1895 - val_acc: 0.7470\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1999 - acc: 0.7360 - val_loss: 1.1868 - val_acc: 0.7470\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1966 - acc: 0.7361 - val_loss: 1.1841 - val_acc: 0.7470\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1935 - acc: 0.7353 - val_loss: 1.1852 - val_acc: 0.7400\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.1899 - acc: 0.7352 - val_loss: 1.1820 - val_acc: 0.7400\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1873 - acc: 0.7349 - val_loss: 1.1765 - val_acc: 0.7430\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1841 - acc: 0.7367 - val_loss: 1.1704 - val_acc: 0.7410\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1806 - acc: 0.7353 - val_loss: 1.1698 - val_acc: 0.7470\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1774 - acc: 0.7364 - val_loss: 1.1709 - val_acc: 0.7420\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1745 - acc: 0.7367 - val_loss: 1.1625 - val_acc: 0.7450\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1715 - acc: 0.7360 - val_loss: 1.1602 - val_acc: 0.7440\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1683 - acc: 0.7373 - val_loss: 1.1584 - val_acc: 0.7470\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1656 - acc: 0.7371 - val_loss: 1.1523 - val_acc: 0.7430\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1626 - acc: 0.7381 - val_loss: 1.1576 - val_acc: 0.7430\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1601 - acc: 0.7388 - val_loss: 1.1492 - val_acc: 0.7460\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1570 - acc: 0.7379 - val_loss: 1.1498 - val_acc: 0.7420\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1544 - acc: 0.7384 - val_loss: 1.1434 - val_acc: 0.7470\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1519 - acc: 0.7389 - val_loss: 1.1417 - val_acc: 0.7470\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1489 - acc: 0.7376 - val_loss: 1.1431 - val_acc: 0.7410\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1465 - acc: 0.7388 - val_loss: 1.1412 - val_acc: 0.7390\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1433 - acc: 0.7393 - val_loss: 1.1327 - val_acc: 0.7480\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1410 - acc: 0.7391 - val_loss: 1.1421 - val_acc: 0.7430\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1390 - acc: 0.7404 - val_loss: 1.1300 - val_acc: 0.7450\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1359 - acc: 0.7391 - val_loss: 1.1277 - val_acc: 0.7500\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1336 - acc: 0.7409 - val_loss: 1.1241 - val_acc: 0.7460\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1310 - acc: 0.7408 - val_loss: 1.1336 - val_acc: 0.7360\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1286 - acc: 0.7405 - val_loss: 1.1200 - val_acc: 0.7470\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1264 - acc: 0.7408 - val_loss: 1.1178 - val_acc: 0.7450\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1233 - acc: 0.7425 - val_loss: 1.1259 - val_acc: 0.7400\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1219 - acc: 0.7427 - val_loss: 1.1240 - val_acc: 0.7340\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1198 - acc: 0.7420 - val_loss: 1.1132 - val_acc: 0.7470\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1167 - acc: 0.7417 - val_loss: 1.1115 - val_acc: 0.7410\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1146 - acc: 0.7423 - val_loss: 1.1120 - val_acc: 0.7500\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1129 - acc: 0.7427 - val_loss: 1.1068 - val_acc: 0.7470\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1105 - acc: 0.7429 - val_loss: 1.1074 - val_acc: 0.7380\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1085 - acc: 0.7428 - val_loss: 1.1076 - val_acc: 0.7390\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1061 - acc: 0.7428 - val_loss: 1.1019 - val_acc: 0.7460\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1043 - acc: 0.7423 - val_loss: 1.0959 - val_acc: 0.7440\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1012 - acc: 0.7436 - val_loss: 1.0972 - val_acc: 0.7440\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0999 - acc: 0.7447 - val_loss: 1.0972 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.0974 - acc: 0.7441 - val_loss: 1.0931 - val_acc: 0.7460\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0957 - acc: 0.7444 - val_loss: 1.0927 - val_acc: 0.7480\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0934 - acc: 0.7447 - val_loss: 1.0877 - val_acc: 0.7460\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0919 - acc: 0.7461 - val_loss: 1.0893 - val_acc: 0.7430\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0899 - acc: 0.7439 - val_loss: 1.0940 - val_acc: 0.7400\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0879 - acc: 0.7451 - val_loss: 1.0883 - val_acc: 0.7430\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0863 - acc: 0.7447 - val_loss: 1.0830 - val_acc: 0.7440\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0843 - acc: 0.7449 - val_loss: 1.0770 - val_acc: 0.7450\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0824 - acc: 0.7451 - val_loss: 1.0837 - val_acc: 0.7470\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0810 - acc: 0.7447 - val_loss: 1.0809 - val_acc: 0.7440\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0792 - acc: 0.7440 - val_loss: 1.0824 - val_acc: 0.7390\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0775 - acc: 0.7459 - val_loss: 1.0720 - val_acc: 0.7450\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0754 - acc: 0.7451 - val_loss: 1.0722 - val_acc: 0.7460\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0740 - acc: 0.7472 - val_loss: 1.0754 - val_acc: 0.7430\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0726 - acc: 0.7483 - val_loss: 1.0727 - val_acc: 0.7470\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0710 - acc: 0.7461 - val_loss: 1.0723 - val_acc: 0.7380\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0694 - acc: 0.7464 - val_loss: 1.0738 - val_acc: 0.7390\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0672 - acc: 0.7463 - val_loss: 1.0731 - val_acc: 0.7470\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0660 - acc: 0.7480 - val_loss: 1.0607 - val_acc: 0.7520\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0646 - acc: 0.7471 - val_loss: 1.0605 - val_acc: 0.7480\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0624 - acc: 0.7477 - val_loss: 1.0624 - val_acc: 0.7420\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0612 - acc: 0.7475 - val_loss: 1.0625 - val_acc: 0.7400\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0604 - acc: 0.7480 - val_loss: 1.0620 - val_acc: 0.7440\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0584 - acc: 0.7475 - val_loss: 1.0631 - val_acc: 0.7410\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0565 - acc: 0.7464 - val_loss: 1.0580 - val_acc: 0.7450\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0550 - acc: 0.7485 - val_loss: 1.0582 - val_acc: 0.7460\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0538 - acc: 0.7483 - val_loss: 1.0506 - val_acc: 0.7450\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0524 - acc: 0.7484 - val_loss: 1.0585 - val_acc: 0.7430\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0507 - acc: 0.7493 - val_loss: 1.0505 - val_acc: 0.7480\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0495 - acc: 0.7484 - val_loss: 1.0454 - val_acc: 0.7470\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0478 - acc: 0.7489 - val_loss: 1.0469 - val_acc: 0.7460\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0468 - acc: 0.7501 - val_loss: 1.0456 - val_acc: 0.7430\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0451 - acc: 0.7492 - val_loss: 1.0493 - val_acc: 0.7440\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0441 - acc: 0.7472 - val_loss: 1.0432 - val_acc: 0.7470\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0425 - acc: 0.7504 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0409 - acc: 0.7499 - val_loss: 1.0445 - val_acc: 0.7430\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0396 - acc: 0.7484 - val_loss: 1.0411 - val_acc: 0.7470\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0389 - acc: 0.7515 - val_loss: 1.0403 - val_acc: 0.7470\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0369 - acc: 0.7516 - val_loss: 1.0400 - val_acc: 0.7420\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0359 - acc: 0.7511 - val_loss: 1.0403 - val_acc: 0.7450\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0350 - acc: 0.7505 - val_loss: 1.0436 - val_acc: 0.7440\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0339 - acc: 0.7513 - val_loss: 1.0356 - val_acc: 0.7440\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0325 - acc: 0.7516 - val_loss: 1.0365 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0309 - acc: 0.7524 - val_loss: 1.0326 - val_acc: 0.7470\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0301 - acc: 0.7528 - val_loss: 1.0310 - val_acc: 0.7450\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0286 - acc: 0.7507 - val_loss: 1.0299 - val_acc: 0.7420\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0268 - acc: 0.7520 - val_loss: 1.0344 - val_acc: 0.7410\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0259 - acc: 0.7499 - val_loss: 1.0275 - val_acc: 0.7460\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0244 - acc: 0.7528 - val_loss: 1.0274 - val_acc: 0.7520\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0236 - acc: 0.7517 - val_loss: 1.0274 - val_acc: 0.7430\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0219 - acc: 0.7545 - val_loss: 1.0236 - val_acc: 0.7460\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0207 - acc: 0.7528 - val_loss: 1.0200 - val_acc: 0.7490\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0198 - acc: 0.7532 - val_loss: 1.0225 - val_acc: 0.7530\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0181 - acc: 0.7543 - val_loss: 1.0274 - val_acc: 0.7430\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0177 - acc: 0.7536 - val_loss: 1.0184 - val_acc: 0.7470\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0164 - acc: 0.7535 - val_loss: 1.0182 - val_acc: 0.7530\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0150 - acc: 0.7519 - val_loss: 1.0171 - val_acc: 0.7480\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0138 - acc: 0.7541 - val_loss: 1.0214 - val_acc: 0.7440\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0131 - acc: 0.7536 - val_loss: 1.0200 - val_acc: 0.7430\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0114 - acc: 0.7528 - val_loss: 1.0167 - val_acc: 0.7480\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0107 - acc: 0.7524 - val_loss: 1.0178 - val_acc: 0.7540\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0101 - acc: 0.7548 - val_loss: 1.0120 - val_acc: 0.7580\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0086 - acc: 0.7541 - val_loss: 1.0116 - val_acc: 0.7510\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0069 - acc: 0.7564 - val_loss: 1.0094 - val_acc: 0.7500\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0061 - acc: 0.7556 - val_loss: 1.0154 - val_acc: 0.7500\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0054 - acc: 0.7529 - val_loss: 1.0085 - val_acc: 0.7470\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0037 - acc: 0.7545 - val_loss: 1.0110 - val_acc: 0.7460\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0030 - acc: 0.7545 - val_loss: 1.0115 - val_acc: 0.7540\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0014 - acc: 0.7553 - val_loss: 1.0102 - val_acc: 0.7460\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0004 - acc: 0.7561 - val_loss: 1.0051 - val_acc: 0.7450\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9998 - acc: 0.7549 - val_loss: 1.0019 - val_acc: 0.7490\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9985 - acc: 0.7568 - val_loss: 1.0062 - val_acc: 0.7450\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9967 - acc: 0.7564 - val_loss: 0.9993 - val_acc: 0.7530\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9958 - acc: 0.7564 - val_loss: 0.9999 - val_acc: 0.7510\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9952 - acc: 0.7557 - val_loss: 1.0043 - val_acc: 0.7470\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9945 - acc: 0.7572 - val_loss: 1.0070 - val_acc: 0.7400\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9928 - acc: 0.7576 - val_loss: 0.9983 - val_acc: 0.7550\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9934 - acc: 0.7564 - val_loss: 0.9959 - val_acc: 0.7560\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9914 - acc: 0.7572 - val_loss: 1.0097 - val_acc: 0.7470\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9911 - acc: 0.7567 - val_loss: 0.9976 - val_acc: 0.7520\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9891 - acc: 0.7575 - val_loss: 0.9978 - val_acc: 0.7480\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9884 - acc: 0.7567 - val_loss: 1.0020 - val_acc: 0.7530\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9880 - acc: 0.7589 - val_loss: 0.9915 - val_acc: 0.7540\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9860 - acc: 0.7575 - val_loss: 0.9926 - val_acc: 0.7470\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9853 - acc: 0.7580 - val_loss: 0.9900 - val_acc: 0.7540\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9849 - acc: 0.7595 - val_loss: 0.9908 - val_acc: 0.7510\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9839 - acc: 0.7596 - val_loss: 0.9874 - val_acc: 0.7500\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9827 - acc: 0.7599 - val_loss: 0.9957 - val_acc: 0.7510\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9825 - acc: 0.7580 - val_loss: 0.9887 - val_acc: 0.7530\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9808 - acc: 0.7588 - val_loss: 0.9868 - val_acc: 0.7490\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9795 - acc: 0.7596 - val_loss: 0.9892 - val_acc: 0.7510\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9798 - acc: 0.7595 - val_loss: 0.9863 - val_acc: 0.7490\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9788 - acc: 0.7579 - val_loss: 0.9814 - val_acc: 0.7550\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9774 - acc: 0.7601 - val_loss: 0.9847 - val_acc: 0.7540\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9770 - acc: 0.7603 - val_loss: 0.9837 - val_acc: 0.7550\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9763 - acc: 0.7595 - val_loss: 0.9888 - val_acc: 0.7470\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9753 - acc: 0.7603 - val_loss: 0.9826 - val_acc: 0.7540\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9739 - acc: 0.7593 - val_loss: 0.9841 - val_acc: 0.7500\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9738 - acc: 0.7613 - val_loss: 0.9806 - val_acc: 0.7550\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9729 - acc: 0.7605 - val_loss: 0.9819 - val_acc: 0.7550\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9720 - acc: 0.7595 - val_loss: 0.9826 - val_acc: 0.7510\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9713 - acc: 0.7591 - val_loss: 0.9815 - val_acc: 0.7540\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9701 - acc: 0.7605 - val_loss: 0.9832 - val_acc: 0.7510\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9707 - acc: 0.7603 - val_loss: 0.9799 - val_acc: 0.7490\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9691 - acc: 0.7605 - val_loss: 0.9729 - val_acc: 0.7570\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9679 - acc: 0.7604 - val_loss: 0.9730 - val_acc: 0.7520\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9667 - acc: 0.7615 - val_loss: 0.9772 - val_acc: 0.7540\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9669 - acc: 0.7607 - val_loss: 0.9826 - val_acc: 0.7550\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9658 - acc: 0.7617 - val_loss: 0.9912 - val_acc: 0.7420\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9653 - acc: 0.7613 - val_loss: 0.9733 - val_acc: 0.7590\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9637 - acc: 0.7616 - val_loss: 0.9766 - val_acc: 0.7460\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9639 - acc: 0.7616 - val_loss: 0.9707 - val_acc: 0.7510\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9627 - acc: 0.7600 - val_loss: 0.9754 - val_acc: 0.7480\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9625 - acc: 0.7628 - val_loss: 0.9725 - val_acc: 0.7530\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9616 - acc: 0.7611 - val_loss: 0.9653 - val_acc: 0.7540\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9604 - acc: 0.7628 - val_loss: 0.9669 - val_acc: 0.7570\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9600 - acc: 0.7619 - val_loss: 0.9690 - val_acc: 0.7570\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9597 - acc: 0.7605 - val_loss: 0.9681 - val_acc: 0.7580\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9586 - acc: 0.7617 - val_loss: 0.9717 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9582 - acc: 0.7613 - val_loss: 0.9677 - val_acc: 0.7550\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9573 - acc: 0.7603 - val_loss: 0.9635 - val_acc: 0.7550\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9562 - acc: 0.7620 - val_loss: 0.9657 - val_acc: 0.7520\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9559 - acc: 0.7627 - val_loss: 0.9676 - val_acc: 0.7530\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9549 - acc: 0.7624 - val_loss: 0.9767 - val_acc: 0.7460\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9548 - acc: 0.7616 - val_loss: 0.9674 - val_acc: 0.7510\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9537 - acc: 0.7633 - val_loss: 0.9676 - val_acc: 0.7500\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9527 - acc: 0.7629 - val_loss: 0.9629 - val_acc: 0.7520\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9522 - acc: 0.7619 - val_loss: 0.9629 - val_acc: 0.7560\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9518 - acc: 0.7621 - val_loss: 0.9673 - val_acc: 0.7510\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9515 - acc: 0.7635 - val_loss: 0.9592 - val_acc: 0.7580\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9506 - acc: 0.7625 - val_loss: 0.9629 - val_acc: 0.7550\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9499 - acc: 0.7616 - val_loss: 0.9737 - val_acc: 0.7470\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9500 - acc: 0.7623 - val_loss: 0.9562 - val_acc: 0.7530\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9480 - acc: 0.7637 - val_loss: 0.9677 - val_acc: 0.7500\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9478 - acc: 0.7651 - val_loss: 0.9709 - val_acc: 0.7490\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9477 - acc: 0.7635 - val_loss: 0.9721 - val_acc: 0.7430\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9467 - acc: 0.7632 - val_loss: 0.9559 - val_acc: 0.7510\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9460 - acc: 0.7632 - val_loss: 0.9595 - val_acc: 0.7540\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9453 - acc: 0.7639 - val_loss: 0.9630 - val_acc: 0.7510\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9447 - acc: 0.7629 - val_loss: 0.9566 - val_acc: 0.7560\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9441 - acc: 0.7635 - val_loss: 0.9598 - val_acc: 0.7500\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9442 - acc: 0.7649 - val_loss: 0.9596 - val_acc: 0.7630\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9440 - acc: 0.7629 - val_loss: 0.9566 - val_acc: 0.7550\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9425 - acc: 0.7636 - val_loss: 0.9574 - val_acc: 0.7540\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9422 - acc: 0.7641 - val_loss: 0.9597 - val_acc: 0.7510\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9418 - acc: 0.7632 - val_loss: 0.9566 - val_acc: 0.7530\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9406 - acc: 0.7637 - val_loss: 0.9507 - val_acc: 0.7610\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9415 - acc: 0.7653 - val_loss: 0.9594 - val_acc: 0.7500\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9403 - acc: 0.7627 - val_loss: 0.9595 - val_acc: 0.7530\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9391 - acc: 0.7665 - val_loss: 0.9592 - val_acc: 0.7520\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9391 - acc: 0.7655 - val_loss: 0.9522 - val_acc: 0.7540\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9385 - acc: 0.7664 - val_loss: 0.9492 - val_acc: 0.7620\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9381 - acc: 0.7647 - val_loss: 0.9556 - val_acc: 0.7550\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9371 - acc: 0.7641 - val_loss: 0.9549 - val_acc: 0.7570\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9368 - acc: 0.7641 - val_loss: 0.9516 - val_acc: 0.7530\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9360 - acc: 0.7647 - val_loss: 0.9527 - val_acc: 0.7520\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9355 - acc: 0.7652 - val_loss: 0.9597 - val_acc: 0.7480\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9354 - acc: 0.7660 - val_loss: 0.9476 - val_acc: 0.7610\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9350 - acc: 0.7664 - val_loss: 0.9481 - val_acc: 0.7580\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9350 - acc: 0.7659 - val_loss: 0.9465 - val_acc: 0.7520\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9337 - acc: 0.7660 - val_loss: 0.9537 - val_acc: 0.7530\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9329 - acc: 0.7665 - val_loss: 0.9522 - val_acc: 0.7590\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9333 - acc: 0.7655 - val_loss: 0.9502 - val_acc: 0.7530\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9324 - acc: 0.7660 - val_loss: 0.9474 - val_acc: 0.7570\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9317 - acc: 0.7656 - val_loss: 0.9503 - val_acc: 0.7580\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9311 - acc: 0.7675 - val_loss: 0.9430 - val_acc: 0.7590\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9306 - acc: 0.7669 - val_loss: 0.9461 - val_acc: 0.7540\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9304 - acc: 0.7657 - val_loss: 0.9415 - val_acc: 0.7590\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9297 - acc: 0.7663 - val_loss: 0.9481 - val_acc: 0.7580\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9299 - acc: 0.7665 - val_loss: 0.9448 - val_acc: 0.7560\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9291 - acc: 0.7671 - val_loss: 0.9500 - val_acc: 0.7580\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9284 - acc: 0.7664 - val_loss: 0.9452 - val_acc: 0.7650\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9287 - acc: 0.7673 - val_loss: 0.9505 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9278 - acc: 0.7665 - val_loss: 0.9451 - val_acc: 0.7550\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9275 - acc: 0.7677 - val_loss: 0.9418 - val_acc: 0.7610\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9266 - acc: 0.7673 - val_loss: 0.9445 - val_acc: 0.7590\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9267 - acc: 0.7667 - val_loss: 0.9452 - val_acc: 0.7550\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9273 - acc: 0.7649 - val_loss: 0.9441 - val_acc: 0.7550\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9257 - acc: 0.7671 - val_loss: 0.9399 - val_acc: 0.7590\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9250 - acc: 0.7663 - val_loss: 0.9437 - val_acc: 0.7600\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9246 - acc: 0.7672 - val_loss: 0.9430 - val_acc: 0.7570\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9246 - acc: 0.7679 - val_loss: 0.9382 - val_acc: 0.7610\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9237 - acc: 0.7685 - val_loss: 0.9460 - val_acc: 0.7560\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9240 - acc: 0.7684 - val_loss: 0.9403 - val_acc: 0.7600\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9234 - acc: 0.7680 - val_loss: 0.9386 - val_acc: 0.7560\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9227 - acc: 0.7688 - val_loss: 0.9412 - val_acc: 0.7560\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9226 - acc: 0.7681 - val_loss: 0.9429 - val_acc: 0.7570\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9230 - acc: 0.7655 - val_loss: 0.9412 - val_acc: 0.7540\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9220 - acc: 0.7673 - val_loss: 0.9359 - val_acc: 0.7620\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9212 - acc: 0.7685 - val_loss: 0.9425 - val_acc: 0.7580\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9210 - acc: 0.7671 - val_loss: 0.9392 - val_acc: 0.7640\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9206 - acc: 0.7671 - val_loss: 0.9361 - val_acc: 0.7620\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9200 - acc: 0.7700 - val_loss: 0.9380 - val_acc: 0.7590\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9197 - acc: 0.7700 - val_loss: 0.9356 - val_acc: 0.7650\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9198 - acc: 0.7676 - val_loss: 0.9347 - val_acc: 0.7570\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9191 - acc: 0.7689 - val_loss: 0.9472 - val_acc: 0.7490\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9190 - acc: 0.7688 - val_loss: 0.9378 - val_acc: 0.7580\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9183 - acc: 0.7691 - val_loss: 0.9445 - val_acc: 0.7540\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9174 - acc: 0.7684 - val_loss: 0.9405 - val_acc: 0.7550\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9175 - acc: 0.7668 - val_loss: 0.9337 - val_acc: 0.7610\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9172 - acc: 0.7693 - val_loss: 0.9319 - val_acc: 0.7560\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9172 - acc: 0.7685 - val_loss: 0.9341 - val_acc: 0.7610\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9163 - acc: 0.7669 - val_loss: 0.9418 - val_acc: 0.7520\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9167 - acc: 0.7695 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9158 - acc: 0.7687 - val_loss: 0.9325 - val_acc: 0.7650\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9151 - acc: 0.7683 - val_loss: 0.9302 - val_acc: 0.7650\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9152 - acc: 0.7673 - val_loss: 0.9309 - val_acc: 0.7650\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9146 - acc: 0.7693 - val_loss: 0.9326 - val_acc: 0.7580\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9138 - acc: 0.7681 - val_loss: 0.9336 - val_acc: 0.7620\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9135 - acc: 0.7660 - val_loss: 0.9354 - val_acc: 0.7600\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9139 - acc: 0.7679 - val_loss: 0.9308 - val_acc: 0.7630\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9129 - acc: 0.7704 - val_loss: 0.9303 - val_acc: 0.7640\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9131 - acc: 0.7692 - val_loss: 0.9334 - val_acc: 0.7610\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9129 - acc: 0.7693 - val_loss: 0.9263 - val_acc: 0.7600\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9125 - acc: 0.7687 - val_loss: 0.9281 - val_acc: 0.7640\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9113 - acc: 0.7711 - val_loss: 0.9327 - val_acc: 0.7650\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9121 - acc: 0.7691 - val_loss: 0.9301 - val_acc: 0.7570\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9105 - acc: 0.7708 - val_loss: 0.9372 - val_acc: 0.7540\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9107 - acc: 0.7703 - val_loss: 0.9282 - val_acc: 0.7630\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9101 - acc: 0.7701 - val_loss: 0.9358 - val_acc: 0.7610\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9095 - acc: 0.7719 - val_loss: 0.9311 - val_acc: 0.7620\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9096 - acc: 0.7711 - val_loss: 0.9287 - val_acc: 0.7570\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9085 - acc: 0.7696 - val_loss: 0.9249 - val_acc: 0.7650\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9093 - acc: 0.7719 - val_loss: 0.9313 - val_acc: 0.7550\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9087 - acc: 0.7709 - val_loss: 0.9296 - val_acc: 0.7630\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9088 - acc: 0.7695 - val_loss: 0.9286 - val_acc: 0.7650\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9073 - acc: 0.7743 - val_loss: 0.9260 - val_acc: 0.7620\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9085 - acc: 0.7700 - val_loss: 0.9292 - val_acc: 0.7620\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9076 - acc: 0.7701 - val_loss: 0.9246 - val_acc: 0.7570\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9066 - acc: 0.7719 - val_loss: 0.9289 - val_acc: 0.7610\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9071 - acc: 0.7699 - val_loss: 0.9248 - val_acc: 0.7650\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9060 - acc: 0.7692 - val_loss: 0.9221 - val_acc: 0.7620\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9056 - acc: 0.7729 - val_loss: 0.9281 - val_acc: 0.7580\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9047 - acc: 0.7709 - val_loss: 0.9246 - val_acc: 0.7600\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9054 - acc: 0.7725 - val_loss: 0.9330 - val_acc: 0.7550\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9048 - acc: 0.7709 - val_loss: 0.9255 - val_acc: 0.7580\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9043 - acc: 0.7724 - val_loss: 0.9266 - val_acc: 0.7650\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9042 - acc: 0.7732 - val_loss: 0.9279 - val_acc: 0.7580\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9040 - acc: 0.7721 - val_loss: 0.9301 - val_acc: 0.7570\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9035 - acc: 0.7724 - val_loss: 0.9295 - val_acc: 0.7570\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9029 - acc: 0.7717 - val_loss: 0.9236 - val_acc: 0.7730\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9029 - acc: 0.7729 - val_loss: 0.9269 - val_acc: 0.7580\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9024 - acc: 0.7744 - val_loss: 0.9190 - val_acc: 0.7620\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9019 - acc: 0.7731 - val_loss: 0.9239 - val_acc: 0.7610\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9015 - acc: 0.7704 - val_loss: 0.9239 - val_acc: 0.7640\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9011 - acc: 0.7747 - val_loss: 0.9276 - val_acc: 0.7580\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9012 - acc: 0.7720 - val_loss: 0.9243 - val_acc: 0.7580\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9003 - acc: 0.7709 - val_loss: 0.9249 - val_acc: 0.7560\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9002 - acc: 0.7732 - val_loss: 0.9243 - val_acc: 0.7630\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8997 - acc: 0.7728 - val_loss: 0.9214 - val_acc: 0.7640\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9002 - acc: 0.7728 - val_loss: 0.9214 - val_acc: 0.7620\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8993 - acc: 0.7731 - val_loss: 0.9213 - val_acc: 0.7640\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8988 - acc: 0.7732 - val_loss: 0.9302 - val_acc: 0.7620\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8989 - acc: 0.7749 - val_loss: 0.9250 - val_acc: 0.7580\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8986 - acc: 0.7729 - val_loss: 0.9228 - val_acc: 0.7620\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8978 - acc: 0.7739 - val_loss: 0.9185 - val_acc: 0.7600\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8977 - acc: 0.7737 - val_loss: 0.9251 - val_acc: 0.7550\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8972 - acc: 0.7749 - val_loss: 0.9181 - val_acc: 0.7620\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8970 - acc: 0.7731 - val_loss: 0.9179 - val_acc: 0.7630\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8961 - acc: 0.7741 - val_loss: 0.9312 - val_acc: 0.7540\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8967 - acc: 0.7751 - val_loss: 0.9239 - val_acc: 0.7620\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8956 - acc: 0.7731 - val_loss: 0.9259 - val_acc: 0.7600\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8967 - acc: 0.7728 - val_loss: 0.9227 - val_acc: 0.7620\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8956 - acc: 0.7733 - val_loss: 0.9238 - val_acc: 0.7570\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8951 - acc: 0.7732 - val_loss: 0.9191 - val_acc: 0.7570\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8951 - acc: 0.7727 - val_loss: 0.9190 - val_acc: 0.7570\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8953 - acc: 0.7720 - val_loss: 0.9255 - val_acc: 0.7530\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8942 - acc: 0.7743 - val_loss: 0.9172 - val_acc: 0.7620\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8939 - acc: 0.7747 - val_loss: 0.9337 - val_acc: 0.7470\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8936 - acc: 0.7713 - val_loss: 0.9239 - val_acc: 0.7570\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8933 - acc: 0.7752 - val_loss: 0.9159 - val_acc: 0.7650\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8933 - acc: 0.7736 - val_loss: 0.9158 - val_acc: 0.7680\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8927 - acc: 0.7749 - val_loss: 0.9204 - val_acc: 0.7570\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8923 - acc: 0.7748 - val_loss: 0.9195 - val_acc: 0.7610\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8917 - acc: 0.7759 - val_loss: 0.9335 - val_acc: 0.7510\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8922 - acc: 0.7728 - val_loss: 0.9152 - val_acc: 0.7610\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8911 - acc: 0.7740 - val_loss: 0.9153 - val_acc: 0.7650\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8909 - acc: 0.7751 - val_loss: 0.9210 - val_acc: 0.7550\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8905 - acc: 0.7741 - val_loss: 0.9182 - val_acc: 0.7550\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8905 - acc: 0.7748 - val_loss: 0.9160 - val_acc: 0.7600\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8901 - acc: 0.7751 - val_loss: 0.9193 - val_acc: 0.7560\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8903 - acc: 0.7736 - val_loss: 0.9207 - val_acc: 0.7540\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8893 - acc: 0.7732 - val_loss: 0.9164 - val_acc: 0.7570\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8895 - acc: 0.7749 - val_loss: 0.9122 - val_acc: 0.7710\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8889 - acc: 0.7752 - val_loss: 0.9117 - val_acc: 0.7660\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8884 - acc: 0.7752 - val_loss: 0.9113 - val_acc: 0.7650\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8877 - acc: 0.7765 - val_loss: 0.9237 - val_acc: 0.7550\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8879 - acc: 0.7767 - val_loss: 0.9230 - val_acc: 0.7510\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8876 - acc: 0.7753 - val_loss: 0.9130 - val_acc: 0.7580\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8871 - acc: 0.7755 - val_loss: 0.9102 - val_acc: 0.7590\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8871 - acc: 0.7759 - val_loss: 0.9137 - val_acc: 0.7630\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8860 - acc: 0.7779 - val_loss: 0.9147 - val_acc: 0.7640\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8865 - acc: 0.7775 - val_loss: 0.9190 - val_acc: 0.7590\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8853 - acc: 0.7776 - val_loss: 0.9455 - val_acc: 0.7420\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8868 - acc: 0.7753 - val_loss: 0.9102 - val_acc: 0.7660\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8853 - acc: 0.7772 - val_loss: 0.9126 - val_acc: 0.7590\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8852 - acc: 0.7747 - val_loss: 0.9196 - val_acc: 0.7590\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8852 - acc: 0.7768 - val_loss: 0.9201 - val_acc: 0.7600\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8844 - acc: 0.7743 - val_loss: 0.9109 - val_acc: 0.7620\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8838 - acc: 0.7767 - val_loss: 0.9194 - val_acc: 0.7540\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8845 - acc: 0.7768 - val_loss: 0.9122 - val_acc: 0.7590\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8838 - acc: 0.7767 - val_loss: 0.9272 - val_acc: 0.7550\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8842 - acc: 0.7775 - val_loss: 0.9091 - val_acc: 0.7680\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8833 - acc: 0.7757 - val_loss: 0.9088 - val_acc: 0.7630\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8828 - acc: 0.7760 - val_loss: 0.9229 - val_acc: 0.7530\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8828 - acc: 0.7768 - val_loss: 0.9047 - val_acc: 0.7660\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8821 - acc: 0.7776 - val_loss: 0.9228 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8819 - acc: 0.7776 - val_loss: 0.9063 - val_acc: 0.7690\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8820 - acc: 0.7776 - val_loss: 0.9216 - val_acc: 0.7500\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8819 - acc: 0.7751 - val_loss: 0.9176 - val_acc: 0.7530\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8813 - acc: 0.7780 - val_loss: 0.9082 - val_acc: 0.7630\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8818 - acc: 0.7781 - val_loss: 0.9140 - val_acc: 0.7570\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8816 - acc: 0.7775 - val_loss: 0.9050 - val_acc: 0.7610\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8808 - acc: 0.7771 - val_loss: 0.9038 - val_acc: 0.7660\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8800 - acc: 0.7776 - val_loss: 0.9150 - val_acc: 0.7590\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8800 - acc: 0.7764 - val_loss: 0.9257 - val_acc: 0.7570\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8797 - acc: 0.7769 - val_loss: 0.9107 - val_acc: 0.7620\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8788 - acc: 0.7781 - val_loss: 0.9051 - val_acc: 0.7710\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8788 - acc: 0.7773 - val_loss: 0.9122 - val_acc: 0.7640\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8785 - acc: 0.7784 - val_loss: 0.9134 - val_acc: 0.7580\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8786 - acc: 0.7791 - val_loss: 0.9051 - val_acc: 0.7630\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8778 - acc: 0.7795 - val_loss: 0.9052 - val_acc: 0.7670\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8784 - acc: 0.7777 - val_loss: 0.9107 - val_acc: 0.7560\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8780 - acc: 0.7795 - val_loss: 0.9061 - val_acc: 0.7650\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8770 - acc: 0.7784 - val_loss: 0.9074 - val_acc: 0.7660\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8773 - acc: 0.7777 - val_loss: 0.9089 - val_acc: 0.7590\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8767 - acc: 0.7765 - val_loss: 0.9003 - val_acc: 0.7670\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8764 - acc: 0.7796 - val_loss: 0.9078 - val_acc: 0.7650\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8761 - acc: 0.7789 - val_loss: 0.8999 - val_acc: 0.7710\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8755 - acc: 0.7792 - val_loss: 0.9299 - val_acc: 0.7480\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8766 - acc: 0.7761 - val_loss: 0.9042 - val_acc: 0.7640\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8749 - acc: 0.7775 - val_loss: 0.9096 - val_acc: 0.7630\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8750 - acc: 0.7801 - val_loss: 0.9019 - val_acc: 0.7650\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8748 - acc: 0.7785 - val_loss: 0.9086 - val_acc: 0.7620\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8753 - acc: 0.7785 - val_loss: 0.8997 - val_acc: 0.7630\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8746 - acc: 0.7784 - val_loss: 0.9063 - val_acc: 0.7640\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8747 - acc: 0.7781 - val_loss: 0.8990 - val_acc: 0.7650\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8745 - acc: 0.7795 - val_loss: 0.9044 - val_acc: 0.7630\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8729 - acc: 0.7813 - val_loss: 0.8999 - val_acc: 0.7660\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8733 - acc: 0.7800 - val_loss: 0.9051 - val_acc: 0.7560\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8740 - acc: 0.7784 - val_loss: 0.9133 - val_acc: 0.7600\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8733 - acc: 0.7784 - val_loss: 0.9117 - val_acc: 0.7600\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8734 - acc: 0.7791 - val_loss: 0.9014 - val_acc: 0.7630\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8723 - acc: 0.7808 - val_loss: 0.9342 - val_acc: 0.7410\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8726 - acc: 0.7787 - val_loss: 0.9128 - val_acc: 0.7580\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8726 - acc: 0.7791 - val_loss: 0.9060 - val_acc: 0.7590\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8729 - acc: 0.7797 - val_loss: 0.9225 - val_acc: 0.7510\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8722 - acc: 0.7783 - val_loss: 0.9022 - val_acc: 0.7670\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8713 - acc: 0.7771 - val_loss: 0.9058 - val_acc: 0.7600\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8707 - acc: 0.7797 - val_loss: 0.9063 - val_acc: 0.7600\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8713 - acc: 0.7805 - val_loss: 0.9085 - val_acc: 0.7640\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8709 - acc: 0.7784 - val_loss: 0.9042 - val_acc: 0.7660\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8705 - acc: 0.7780 - val_loss: 0.9231 - val_acc: 0.7470\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8702 - acc: 0.7795 - val_loss: 0.9095 - val_acc: 0.7540\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8695 - acc: 0.7797 - val_loss: 0.9011 - val_acc: 0.7700\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8690 - acc: 0.7805 - val_loss: 0.9036 - val_acc: 0.7630\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8697 - acc: 0.7789 - val_loss: 0.9000 - val_acc: 0.7680\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8689 - acc: 0.7800 - val_loss: 0.9020 - val_acc: 0.7650\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8686 - acc: 0.7812 - val_loss: 0.9115 - val_acc: 0.7680\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8697 - acc: 0.7800 - val_loss: 0.8967 - val_acc: 0.7670\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8686 - acc: 0.7808 - val_loss: 0.8989 - val_acc: 0.7670\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8680 - acc: 0.7815 - val_loss: 0.8977 - val_acc: 0.7680\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8692 - acc: 0.7799 - val_loss: 0.9155 - val_acc: 0.7520\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8679 - acc: 0.7807 - val_loss: 0.8983 - val_acc: 0.7690\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8671 - acc: 0.7797 - val_loss: 0.9008 - val_acc: 0.7600\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8674 - acc: 0.7799 - val_loss: 0.9023 - val_acc: 0.7610\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8669 - acc: 0.7801 - val_loss: 0.8983 - val_acc: 0.7680\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8666 - acc: 0.7812 - val_loss: 0.8995 - val_acc: 0.7590\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8667 - acc: 0.7805 - val_loss: 0.9045 - val_acc: 0.7550\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8663 - acc: 0.7804 - val_loss: 0.9048 - val_acc: 0.7630\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8663 - acc: 0.7780 - val_loss: 0.9107 - val_acc: 0.7590\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8655 - acc: 0.7820 - val_loss: 0.9024 - val_acc: 0.7580\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8649 - acc: 0.7828 - val_loss: 0.8995 - val_acc: 0.7690\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8663 - acc: 0.7813 - val_loss: 0.9016 - val_acc: 0.7630\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8648 - acc: 0.7832 - val_loss: 0.9158 - val_acc: 0.7560\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8651 - acc: 0.7807 - val_loss: 0.9001 - val_acc: 0.7640\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8649 - acc: 0.7816 - val_loss: 0.9059 - val_acc: 0.7560\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8634 - acc: 0.7816 - val_loss: 0.9094 - val_acc: 0.7520\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8649 - acc: 0.7807 - val_loss: 0.9006 - val_acc: 0.7590\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8640 - acc: 0.7819 - val_loss: 0.9102 - val_acc: 0.7600\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8642 - acc: 0.7827 - val_loss: 0.8947 - val_acc: 0.7600\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8635 - acc: 0.7812 - val_loss: 0.8925 - val_acc: 0.7700\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8637 - acc: 0.7829 - val_loss: 0.8950 - val_acc: 0.7710\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8618 - acc: 0.7825 - val_loss: 0.9018 - val_acc: 0.7690\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8621 - acc: 0.7839 - val_loss: 0.9054 - val_acc: 0.7600\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8627 - acc: 0.7808 - val_loss: 0.8984 - val_acc: 0.7640\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8618 - acc: 0.7837 - val_loss: 0.9132 - val_acc: 0.7570\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8620 - acc: 0.7824 - val_loss: 0.9023 - val_acc: 0.7700\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8622 - acc: 0.7848 - val_loss: 0.8958 - val_acc: 0.7670\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8611 - acc: 0.7828 - val_loss: 0.8940 - val_acc: 0.7660\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8612 - acc: 0.7825 - val_loss: 0.8982 - val_acc: 0.7670\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8613 - acc: 0.7841 - val_loss: 0.8972 - val_acc: 0.7700\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8613 - acc: 0.7797 - val_loss: 0.9065 - val_acc: 0.7610\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8612 - acc: 0.7807 - val_loss: 0.9083 - val_acc: 0.7540\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8606 - acc: 0.7836 - val_loss: 0.8986 - val_acc: 0.7620\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8602 - acc: 0.7832 - val_loss: 0.8967 - val_acc: 0.7650\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8604 - acc: 0.7843 - val_loss: 0.8969 - val_acc: 0.7630\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8600 - acc: 0.7820 - val_loss: 0.9033 - val_acc: 0.7520\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8596 - acc: 0.7821 - val_loss: 0.8969 - val_acc: 0.7610\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8601 - acc: 0.7840 - val_loss: 0.8977 - val_acc: 0.7700\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8594 - acc: 0.7841 - val_loss: 0.9022 - val_acc: 0.7620\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8598 - acc: 0.7828 - val_loss: 0.8920 - val_acc: 0.7720\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8593 - acc: 0.7851 - val_loss: 0.8919 - val_acc: 0.7660\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8587 - acc: 0.7832 - val_loss: 0.9041 - val_acc: 0.7600\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8587 - acc: 0.7835 - val_loss: 0.8993 - val_acc: 0.7640\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8589 - acc: 0.7856 - val_loss: 0.8985 - val_acc: 0.7610\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8581 - acc: 0.7855 - val_loss: 0.8935 - val_acc: 0.7620\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8572 - acc: 0.7853 - val_loss: 0.8914 - val_acc: 0.7690\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8576 - acc: 0.7829 - val_loss: 0.9009 - val_acc: 0.7610\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8566 - acc: 0.7857 - val_loss: 0.9037 - val_acc: 0.7610\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8583 - acc: 0.7836 - val_loss: 0.9049 - val_acc: 0.7550\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8569 - acc: 0.7825 - val_loss: 0.8958 - val_acc: 0.7680\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8571 - acc: 0.7819 - val_loss: 0.8889 - val_acc: 0.7740\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8574 - acc: 0.7851 - val_loss: 0.8995 - val_acc: 0.7570\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8566 - acc: 0.7849 - val_loss: 0.8916 - val_acc: 0.7650\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8565 - acc: 0.7855 - val_loss: 0.8980 - val_acc: 0.7610\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8554 - acc: 0.7861 - val_loss: 0.8968 - val_acc: 0.7690\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8555 - acc: 0.7851 - val_loss: 0.8950 - val_acc: 0.7710\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8553 - acc: 0.7837 - val_loss: 0.8947 - val_acc: 0.7660\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8547 - acc: 0.7871 - val_loss: 0.8905 - val_acc: 0.7710\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8547 - acc: 0.7857 - val_loss: 0.9013 - val_acc: 0.7620\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8562 - acc: 0.7857 - val_loss: 0.8969 - val_acc: 0.7560\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8546 - acc: 0.7865 - val_loss: 0.8992 - val_acc: 0.7630\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8550 - acc: 0.7864 - val_loss: 0.8941 - val_acc: 0.7660\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8536 - acc: 0.7856 - val_loss: 0.8934 - val_acc: 0.7690\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8540 - acc: 0.7861 - val_loss: 0.8895 - val_acc: 0.7710\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8537 - acc: 0.7849 - val_loss: 0.8973 - val_acc: 0.7590\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8539 - acc: 0.7859 - val_loss: 0.8930 - val_acc: 0.7610\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8538 - acc: 0.7863 - val_loss: 0.8975 - val_acc: 0.7650\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8535 - acc: 0.7873 - val_loss: 0.9140 - val_acc: 0.7500\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8541 - acc: 0.7839 - val_loss: 0.8980 - val_acc: 0.7640\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8530 - acc: 0.7851 - val_loss: 0.8875 - val_acc: 0.7690\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8521 - acc: 0.7867 - val_loss: 0.9010 - val_acc: 0.7590\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8535 - acc: 0.7868 - val_loss: 0.8929 - val_acc: 0.7680\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8518 - acc: 0.7881 - val_loss: 0.8867 - val_acc: 0.7710\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8532 - acc: 0.7867 - val_loss: 0.8964 - val_acc: 0.7590\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8519 - acc: 0.7875 - val_loss: 0.8880 - val_acc: 0.7690\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8515 - acc: 0.7864 - val_loss: 0.8857 - val_acc: 0.7760\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8516 - acc: 0.7863 - val_loss: 0.8947 - val_acc: 0.7620\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8507 - acc: 0.7853 - val_loss: 0.8885 - val_acc: 0.7650\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8516 - acc: 0.7868 - val_loss: 0.9005 - val_acc: 0.7630\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8511 - acc: 0.7871 - val_loss: 0.9090 - val_acc: 0.7550\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8518 - acc: 0.7896 - val_loss: 0.8987 - val_acc: 0.7580\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8506 - acc: 0.7871 - val_loss: 0.8883 - val_acc: 0.7690\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8500 - acc: 0.7848 - val_loss: 0.8916 - val_acc: 0.7750\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8504 - acc: 0.7847 - val_loss: 0.8964 - val_acc: 0.7610\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8493 - acc: 0.7871 - val_loss: 0.8897 - val_acc: 0.7680\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8495 - acc: 0.7869 - val_loss: 0.8958 - val_acc: 0.7670\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8497 - acc: 0.7875 - val_loss: 0.8992 - val_acc: 0.7600\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8488 - acc: 0.7880 - val_loss: 0.8921 - val_acc: 0.7670\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8497 - acc: 0.7877 - val_loss: 0.8904 - val_acc: 0.7660\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8497 - acc: 0.7869 - val_loss: 0.9029 - val_acc: 0.7580\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8489 - acc: 0.7885 - val_loss: 0.8908 - val_acc: 0.7620\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8489 - acc: 0.7875 - val_loss: 0.8872 - val_acc: 0.7660\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8484 - acc: 0.7879 - val_loss: 0.8902 - val_acc: 0.7700\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8482 - acc: 0.7876 - val_loss: 0.8893 - val_acc: 0.7690\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8474 - acc: 0.7888 - val_loss: 0.8935 - val_acc: 0.7630\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8477 - acc: 0.7859 - val_loss: 0.8890 - val_acc: 0.7680\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8480 - acc: 0.7884 - val_loss: 0.8885 - val_acc: 0.7630\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8467 - acc: 0.7875 - val_loss: 0.9006 - val_acc: 0.7590\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8469 - acc: 0.7891 - val_loss: 0.8882 - val_acc: 0.7670\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8482 - acc: 0.7871 - val_loss: 0.8908 - val_acc: 0.7740\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8474 - acc: 0.7883 - val_loss: 0.9026 - val_acc: 0.7560\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8470 - acc: 0.7881 - val_loss: 0.8920 - val_acc: 0.7610\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8473 - acc: 0.7889 - val_loss: 0.8878 - val_acc: 0.7700\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8469 - acc: 0.7900 - val_loss: 0.8972 - val_acc: 0.7600\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8460 - acc: 0.7884 - val_loss: 0.9104 - val_acc: 0.7580\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8465 - acc: 0.7899 - val_loss: 0.8838 - val_acc: 0.7660\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8460 - acc: 0.7889 - val_loss: 0.8888 - val_acc: 0.7670\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8462 - acc: 0.7876 - val_loss: 0.8875 - val_acc: 0.7700\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8457 - acc: 0.7885 - val_loss: 0.8918 - val_acc: 0.7690\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8452 - acc: 0.7888 - val_loss: 0.8881 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8456 - acc: 0.7872 - val_loss: 0.8915 - val_acc: 0.7670\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8457 - acc: 0.7883 - val_loss: 0.8981 - val_acc: 0.7630\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8462 - acc: 0.7887 - val_loss: 0.8865 - val_acc: 0.7690\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8453 - acc: 0.7909 - val_loss: 0.8936 - val_acc: 0.7700\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8441 - acc: 0.7884 - val_loss: 0.8968 - val_acc: 0.7600\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8446 - acc: 0.7903 - val_loss: 0.8857 - val_acc: 0.7720\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8442 - acc: 0.7908 - val_loss: 0.8942 - val_acc: 0.7610\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8436 - acc: 0.7905 - val_loss: 0.8952 - val_acc: 0.7610\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8440 - acc: 0.7888 - val_loss: 0.8872 - val_acc: 0.7700\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8434 - acc: 0.7892 - val_loss: 0.8845 - val_acc: 0.7640\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8427 - acc: 0.7895 - val_loss: 0.8900 - val_acc: 0.7620\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8427 - acc: 0.7884 - val_loss: 0.8980 - val_acc: 0.7610\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8430 - acc: 0.7901 - val_loss: 0.8876 - val_acc: 0.7690\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8426 - acc: 0.7892 - val_loss: 0.8864 - val_acc: 0.7730\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8430 - acc: 0.7900 - val_loss: 0.9003 - val_acc: 0.7600\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8433 - acc: 0.7887 - val_loss: 0.8822 - val_acc: 0.7670\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8420 - acc: 0.7877 - val_loss: 0.8876 - val_acc: 0.7710\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8422 - acc: 0.7912 - val_loss: 0.8907 - val_acc: 0.7650\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8419 - acc: 0.7897 - val_loss: 0.8929 - val_acc: 0.7620\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8422 - acc: 0.7908 - val_loss: 0.9104 - val_acc: 0.7520\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8422 - acc: 0.7911 - val_loss: 0.8979 - val_acc: 0.7580\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8415 - acc: 0.7896 - val_loss: 0.8879 - val_acc: 0.7670\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8402 - acc: 0.7915 - val_loss: 0.8875 - val_acc: 0.7650\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8416 - acc: 0.7932 - val_loss: 0.8860 - val_acc: 0.7650\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8412 - acc: 0.7897 - val_loss: 0.8919 - val_acc: 0.7700\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8413 - acc: 0.7897 - val_loss: 0.8832 - val_acc: 0.7700\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8405 - acc: 0.7929 - val_loss: 0.8904 - val_acc: 0.7600\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8397 - acc: 0.7913 - val_loss: 0.9027 - val_acc: 0.7560\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8403 - acc: 0.7895 - val_loss: 0.8831 - val_acc: 0.7750\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8394 - acc: 0.7909 - val_loss: 0.8990 - val_acc: 0.7540\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8396 - acc: 0.7915 - val_loss: 0.9000 - val_acc: 0.7640\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8399 - acc: 0.7903 - val_loss: 0.8899 - val_acc: 0.7630\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8383 - acc: 0.7911 - val_loss: 0.8902 - val_acc: 0.7660\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8389 - acc: 0.7903 - val_loss: 0.8974 - val_acc: 0.7560\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8395 - acc: 0.7900 - val_loss: 0.8859 - val_acc: 0.7690\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8391 - acc: 0.7915 - val_loss: 0.9054 - val_acc: 0.7610\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8389 - acc: 0.7923 - val_loss: 0.8869 - val_acc: 0.7650\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8371 - acc: 0.7915 - val_loss: 0.9005 - val_acc: 0.7620\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8405 - acc: 0.7907 - val_loss: 0.8880 - val_acc: 0.7590\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8380 - acc: 0.7927 - val_loss: 0.8895 - val_acc: 0.7690\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8392 - acc: 0.7917 - val_loss: 0.8809 - val_acc: 0.7710\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8385 - acc: 0.7917 - val_loss: 0.8942 - val_acc: 0.7560\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8375 - acc: 0.7915 - val_loss: 0.8897 - val_acc: 0.7700\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8372 - acc: 0.7921 - val_loss: 0.8897 - val_acc: 0.7690\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8370 - acc: 0.7904 - val_loss: 0.8834 - val_acc: 0.7690\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8370 - acc: 0.7897 - val_loss: 0.8913 - val_acc: 0.7740\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8375 - acc: 0.7917 - val_loss: 0.9010 - val_acc: 0.7520\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8367 - acc: 0.7920 - val_loss: 0.8954 - val_acc: 0.7610\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8370 - acc: 0.7937 - val_loss: 0.8829 - val_acc: 0.7650\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8362 - acc: 0.7921 - val_loss: 0.9081 - val_acc: 0.7510\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8365 - acc: 0.7929 - val_loss: 0.8855 - val_acc: 0.7730\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8352 - acc: 0.7908 - val_loss: 0.8887 - val_acc: 0.7760\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8360 - acc: 0.7912 - val_loss: 0.9136 - val_acc: 0.7540\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8359 - acc: 0.7920 - val_loss: 0.8874 - val_acc: 0.7710\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8361 - acc: 0.7919 - val_loss: 0.8962 - val_acc: 0.7760\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8366 - acc: 0.7915 - val_loss: 0.8876 - val_acc: 0.7640\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8356 - acc: 0.7919 - val_loss: 0.8925 - val_acc: 0.7640\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8357 - acc: 0.7931 - val_loss: 0.8953 - val_acc: 0.7670\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8354 - acc: 0.7905 - val_loss: 0.8932 - val_acc: 0.7630\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8362 - acc: 0.7936 - val_loss: 0.8840 - val_acc: 0.7690\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8342 - acc: 0.7949 - val_loss: 0.8837 - val_acc: 0.7800\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8347 - acc: 0.7908 - val_loss: 0.8914 - val_acc: 0.7580\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8346 - acc: 0.7917 - val_loss: 0.8885 - val_acc: 0.7640\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8341 - acc: 0.7939 - val_loss: 0.8798 - val_acc: 0.7760\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8341 - acc: 0.7935 - val_loss: 0.8938 - val_acc: 0.7590\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8332 - acc: 0.7936 - val_loss: 0.8941 - val_acc: 0.7560\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8341 - acc: 0.7904 - val_loss: 0.8851 - val_acc: 0.7720\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8332 - acc: 0.7937 - val_loss: 0.8877 - val_acc: 0.7590\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8324 - acc: 0.7916 - val_loss: 0.8996 - val_acc: 0.7530\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8331 - acc: 0.7937 - val_loss: 0.8892 - val_acc: 0.7640\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8322 - acc: 0.7960 - val_loss: 0.8841 - val_acc: 0.7680\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8319 - acc: 0.7933 - val_loss: 0.8845 - val_acc: 0.7730\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8339 - acc: 0.7935 - val_loss: 0.8970 - val_acc: 0.7640\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8323 - acc: 0.7949 - val_loss: 0.8924 - val_acc: 0.7590\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8315 - acc: 0.7944 - val_loss: 0.8845 - val_acc: 0.7750\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8326 - acc: 0.7931 - val_loss: 0.8979 - val_acc: 0.7600\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8312 - acc: 0.7952 - val_loss: 0.8770 - val_acc: 0.7770\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8316 - acc: 0.7948 - val_loss: 0.8789 - val_acc: 0.7770\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8308 - acc: 0.7929 - val_loss: 0.8860 - val_acc: 0.7590\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8305 - acc: 0.7948 - val_loss: 0.8772 - val_acc: 0.7760\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8310 - acc: 0.7932 - val_loss: 0.9096 - val_acc: 0.7540\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8314 - acc: 0.7947 - val_loss: 0.8955 - val_acc: 0.7580\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8312 - acc: 0.7939 - val_loss: 0.8791 - val_acc: 0.7660\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8306 - acc: 0.7932 - val_loss: 0.8791 - val_acc: 0.7690\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8305 - acc: 0.7936 - val_loss: 0.9098 - val_acc: 0.7530\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8305 - acc: 0.7944 - val_loss: 0.8863 - val_acc: 0.7570\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8289 - acc: 0.7955 - val_loss: 0.8802 - val_acc: 0.7720\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8290 - acc: 0.7940 - val_loss: 0.8811 - val_acc: 0.7620\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8286 - acc: 0.7965 - val_loss: 0.8858 - val_acc: 0.7690\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8290 - acc: 0.7935 - val_loss: 0.8833 - val_acc: 0.7610\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8289 - acc: 0.7945 - val_loss: 0.8845 - val_acc: 0.7620\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8283 - acc: 0.7956 - val_loss: 0.8862 - val_acc: 0.7720\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8283 - acc: 0.7972 - val_loss: 0.8881 - val_acc: 0.7570\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8281 - acc: 0.7963 - val_loss: 0.8841 - val_acc: 0.7770\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8286 - acc: 0.7947 - val_loss: 0.8907 - val_acc: 0.7590\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7960 - val_loss: 0.8812 - val_acc: 0.7710\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8290 - acc: 0.7956 - val_loss: 0.8795 - val_acc: 0.7630\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8289 - acc: 0.7971 - val_loss: 0.8880 - val_acc: 0.7620\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8277 - acc: 0.7957 - val_loss: 0.8822 - val_acc: 0.7720\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8276 - acc: 0.7969 - val_loss: 0.8968 - val_acc: 0.7670\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8287 - acc: 0.7957 - val_loss: 0.8780 - val_acc: 0.7670\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8260 - acc: 0.7975 - val_loss: 0.8982 - val_acc: 0.7600\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8271 - acc: 0.7945 - val_loss: 0.8809 - val_acc: 0.7780\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8267 - acc: 0.7941 - val_loss: 0.8915 - val_acc: 0.7690\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8278 - acc: 0.7960 - val_loss: 0.8781 - val_acc: 0.7810\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8257 - acc: 0.7968 - val_loss: 0.8766 - val_acc: 0.7690\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8277 - acc: 0.7957 - val_loss: 0.9072 - val_acc: 0.7610\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8269 - acc: 0.7972 - val_loss: 0.8885 - val_acc: 0.7630\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8263 - acc: 0.7948 - val_loss: 0.8750 - val_acc: 0.7770\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8275 - acc: 0.7953 - val_loss: 0.8857 - val_acc: 0.7590\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8266 - acc: 0.7956 - val_loss: 0.8753 - val_acc: 0.7700\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8258 - acc: 0.7975 - val_loss: 0.8785 - val_acc: 0.7680\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8245 - acc: 0.7965 - val_loss: 0.8918 - val_acc: 0.7570\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8261 - acc: 0.7972 - val_loss: 0.8854 - val_acc: 0.7620\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8256 - acc: 0.7975 - val_loss: 0.8922 - val_acc: 0.7560\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8248 - acc: 0.7972 - val_loss: 0.8753 - val_acc: 0.7690\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8247 - acc: 0.7979 - val_loss: 0.8918 - val_acc: 0.7570\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8249 - acc: 0.7963 - val_loss: 0.9009 - val_acc: 0.7570\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8256 - acc: 0.7947 - val_loss: 0.8881 - val_acc: 0.7700\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8238 - acc: 0.7993 - val_loss: 0.8802 - val_acc: 0.7600\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8249 - acc: 0.7971 - val_loss: 0.8877 - val_acc: 0.7580\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8246 - acc: 0.7956 - val_loss: 0.9042 - val_acc: 0.7560\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8244 - acc: 0.7975 - val_loss: 0.8915 - val_acc: 0.7590\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8238 - acc: 0.7977 - val_loss: 0.8905 - val_acc: 0.7620\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8235 - acc: 0.7961 - val_loss: 0.9025 - val_acc: 0.7540\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8242 - acc: 0.7995 - val_loss: 0.8807 - val_acc: 0.7720\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8225 - acc: 0.7983 - val_loss: 0.8934 - val_acc: 0.7590\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8229 - acc: 0.8001 - val_loss: 0.8713 - val_acc: 0.7810\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8227 - acc: 0.7979 - val_loss: 0.8944 - val_acc: 0.7580\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8226 - acc: 0.7981 - val_loss: 0.8847 - val_acc: 0.7640\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8223 - acc: 0.7985 - val_loss: 0.8832 - val_acc: 0.7570\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8238 - acc: 0.7971 - val_loss: 0.8750 - val_acc: 0.7750\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8210 - acc: 0.7973 - val_loss: 0.8898 - val_acc: 0.7560\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8235 - acc: 0.7959 - val_loss: 0.8783 - val_acc: 0.7630\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8225 - acc: 0.7981 - val_loss: 0.8821 - val_acc: 0.7840\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8216 - acc: 0.7984 - val_loss: 0.8870 - val_acc: 0.7640\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8215 - acc: 0.7989 - val_loss: 0.8904 - val_acc: 0.7630\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8212 - acc: 0.7979 - val_loss: 0.8905 - val_acc: 0.7640\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8216 - acc: 0.7979 - val_loss: 0.8941 - val_acc: 0.7530\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8217 - acc: 0.7979 - val_loss: 0.8876 - val_acc: 0.7590\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8204 - acc: 0.8013 - val_loss: 0.8779 - val_acc: 0.7750\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8215 - acc: 0.7985 - val_loss: 0.8752 - val_acc: 0.7690\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8213 - acc: 0.7988 - val_loss: 0.8750 - val_acc: 0.7610\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8218 - acc: 0.7988 - val_loss: 0.8766 - val_acc: 0.7670\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8210 - acc: 0.7977 - val_loss: 0.8854 - val_acc: 0.7660\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8188 - acc: 0.7996 - val_loss: 0.8825 - val_acc: 0.7690\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8200 - acc: 0.7987 - val_loss: 0.8885 - val_acc: 0.7560\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8200 - acc: 0.7983 - val_loss: 0.8765 - val_acc: 0.7750\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8194 - acc: 0.7984 - val_loss: 0.8770 - val_acc: 0.7800\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8194 - acc: 0.7993 - val_loss: 0.8873 - val_acc: 0.7600\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8200 - acc: 0.7999 - val_loss: 0.8796 - val_acc: 0.7650\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8199 - acc: 0.8001 - val_loss: 0.8885 - val_acc: 0.7660\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8188 - acc: 0.8003 - val_loss: 0.8819 - val_acc: 0.7640\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8186 - acc: 0.7991 - val_loss: 0.8744 - val_acc: 0.7650\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8188 - acc: 0.7981 - val_loss: 0.8782 - val_acc: 0.7750\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8184 - acc: 0.7991 - val_loss: 0.8758 - val_acc: 0.7690\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8195 - acc: 0.7996 - val_loss: 0.8733 - val_acc: 0.7800\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8175 - acc: 0.8007 - val_loss: 0.8832 - val_acc: 0.7740\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8189 - acc: 0.8003 - val_loss: 0.8771 - val_acc: 0.7810\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8197 - acc: 0.8012 - val_loss: 0.8853 - val_acc: 0.7550\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8205 - acc: 0.7997 - val_loss: 0.8730 - val_acc: 0.7770\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8190 - acc: 0.7995 - val_loss: 0.8864 - val_acc: 0.7530\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8163 - acc: 0.7992 - val_loss: 0.8757 - val_acc: 0.7770\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8176 - acc: 0.7999 - val_loss: 0.8944 - val_acc: 0.7550\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8183 - acc: 0.8000 - val_loss: 0.8949 - val_acc: 0.7620\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8194 - acc: 0.7968 - val_loss: 0.8735 - val_acc: 0.7860\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8162 - acc: 0.7995 - val_loss: 0.8794 - val_acc: 0.7780\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8168 - acc: 0.7997 - val_loss: 0.8695 - val_acc: 0.7780\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8173 - acc: 0.8011 - val_loss: 0.8823 - val_acc: 0.7680\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8163 - acc: 0.8015 - val_loss: 0.8988 - val_acc: 0.7610\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8187 - acc: 0.7989 - val_loss: 0.8799 - val_acc: 0.7690\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8160 - acc: 0.8011 - val_loss: 0.8828 - val_acc: 0.7510\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8172 - acc: 0.8004 - val_loss: 0.8723 - val_acc: 0.7730\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8159 - acc: 0.7985 - val_loss: 0.8810 - val_acc: 0.7740\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8163 - acc: 0.8009 - val_loss: 0.8832 - val_acc: 0.7740\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8171 - acc: 0.8011 - val_loss: 0.8859 - val_acc: 0.7620\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8150 - acc: 0.7996 - val_loss: 0.8868 - val_acc: 0.7610\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8151 - acc: 0.8023 - val_loss: 0.8797 - val_acc: 0.7810\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8150 - acc: 0.8013 - val_loss: 0.9055 - val_acc: 0.7500\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8157 - acc: 0.8004 - val_loss: 0.8790 - val_acc: 0.7760\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8163 - acc: 0.8007 - val_loss: 0.8817 - val_acc: 0.7660\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8160 - acc: 0.8007 - val_loss: 0.8729 - val_acc: 0.7700\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8157 - acc: 0.8009 - val_loss: 0.8763 - val_acc: 0.7770\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8146 - acc: 0.7989 - val_loss: 0.8823 - val_acc: 0.7740\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8155 - acc: 0.7983 - val_loss: 0.8751 - val_acc: 0.7720\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8140 - acc: 0.8000 - val_loss: 0.8827 - val_acc: 0.7620\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8146 - acc: 0.8001 - val_loss: 0.8772 - val_acc: 0.7730\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8143 - acc: 0.8019 - val_loss: 0.8750 - val_acc: 0.7620\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8135 - acc: 0.8021 - val_loss: 0.8768 - val_acc: 0.7750\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8151 - acc: 0.8015 - val_loss: 0.8875 - val_acc: 0.7620\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8144 - acc: 0.7991 - val_loss: 0.8729 - val_acc: 0.7730\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8140 - acc: 0.8004 - val_loss: 0.8726 - val_acc: 0.7670\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8134 - acc: 0.7991 - val_loss: 0.8713 - val_acc: 0.7690\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8142 - acc: 0.7999 - val_loss: 0.8920 - val_acc: 0.7660\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8140 - acc: 0.8025 - val_loss: 0.8767 - val_acc: 0.7630\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8129 - acc: 0.8011 - val_loss: 0.8695 - val_acc: 0.7770\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8131 - acc: 0.8043 - val_loss: 0.8797 - val_acc: 0.7630\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8125 - acc: 0.8045 - val_loss: 0.8700 - val_acc: 0.7820\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8128 - acc: 0.8015 - val_loss: 0.8790 - val_acc: 0.7590\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8127 - acc: 0.8023 - val_loss: 0.8793 - val_acc: 0.7590\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8118 - acc: 0.8017 - val_loss: 0.8739 - val_acc: 0.7700\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8098 - acc: 0.8008 - val_loss: 0.8884 - val_acc: 0.7590\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8136 - acc: 0.8016 - val_loss: 0.8679 - val_acc: 0.7780\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8114 - acc: 0.8015 - val_loss: 0.8880 - val_acc: 0.7620\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8127 - acc: 0.8009 - val_loss: 0.8918 - val_acc: 0.7610\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8111 - acc: 0.8004 - val_loss: 0.8746 - val_acc: 0.7700\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8103 - acc: 0.8029 - val_loss: 0.8788 - val_acc: 0.7540\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8108 - acc: 0.8012 - val_loss: 0.8760 - val_acc: 0.7720\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8124 - acc: 0.8011 - val_loss: 0.8840 - val_acc: 0.7590\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8101 - acc: 0.8021 - val_loss: 0.8768 - val_acc: 0.7510\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8107 - acc: 0.8023 - val_loss: 0.8703 - val_acc: 0.7740\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8100 - acc: 0.8017 - val_loss: 0.8725 - val_acc: 0.7670\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8110 - acc: 0.8043 - val_loss: 0.8729 - val_acc: 0.7690\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8105 - acc: 0.7997 - val_loss: 0.8715 - val_acc: 0.7650\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8121 - acc: 0.8023 - val_loss: 0.8763 - val_acc: 0.7770\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8103 - acc: 0.8016 - val_loss: 0.8697 - val_acc: 0.7690\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8094 - acc: 0.8036 - val_loss: 0.9052 - val_acc: 0.7500\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8110 - acc: 0.8011 - val_loss: 0.8757 - val_acc: 0.7690\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8089 - acc: 0.8027 - val_loss: 0.8864 - val_acc: 0.7590\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8115 - acc: 0.8003 - val_loss: 0.8849 - val_acc: 0.7610\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8099 - acc: 0.8024 - val_loss: 0.8794 - val_acc: 0.7600\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8091 - acc: 0.8033 - val_loss: 0.8740 - val_acc: 0.7750\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8091 - acc: 0.8040 - val_loss: 0.9069 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8113 - acc: 0.8021 - val_loss: 0.8787 - val_acc: 0.7620\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8084 - acc: 0.8044 - val_loss: 0.8709 - val_acc: 0.7790\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8082 - acc: 0.8033 - val_loss: 0.8784 - val_acc: 0.7700\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8091 - acc: 0.8020 - val_loss: 0.8802 - val_acc: 0.7640\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8074 - acc: 0.8021 - val_loss: 0.8885 - val_acc: 0.7590\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8071 - acc: 0.8024 - val_loss: 0.8674 - val_acc: 0.7750\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8086 - acc: 0.8011 - val_loss: 0.8811 - val_acc: 0.7740\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8070 - acc: 0.8027 - val_loss: 0.8829 - val_acc: 0.7790\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8089 - acc: 0.8025 - val_loss: 0.9765 - val_acc: 0.7400\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8100 - acc: 0.8015 - val_loss: 0.9009 - val_acc: 0.7640\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8075 - acc: 0.8037 - val_loss: 0.8802 - val_acc: 0.7580\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8071 - acc: 0.8044 - val_loss: 0.8759 - val_acc: 0.7780\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8084 - acc: 0.8029 - val_loss: 0.8877 - val_acc: 0.7620\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8086 - acc: 0.8040 - val_loss: 0.8682 - val_acc: 0.7680\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8076 - acc: 0.8048 - val_loss: 0.8822 - val_acc: 0.7640\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8052 - acc: 0.8017 - val_loss: 0.8787 - val_acc: 0.7680\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8064 - acc: 0.8048 - val_loss: 0.8785 - val_acc: 0.7740\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8062 - acc: 0.8023 - val_loss: 0.8811 - val_acc: 0.7640\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8060 - acc: 0.8055 - val_loss: 0.8896 - val_acc: 0.7620\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8061 - acc: 0.8027 - val_loss: 0.8746 - val_acc: 0.7670\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8054 - acc: 0.8045 - val_loss: 0.8793 - val_acc: 0.7760\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8057 - acc: 0.8031 - val_loss: 0.8828 - val_acc: 0.7660\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8059 - acc: 0.8033 - val_loss: 0.8667 - val_acc: 0.7730\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8063 - acc: 0.8040 - val_loss: 0.8736 - val_acc: 0.7730\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8051 - acc: 0.8052 - val_loss: 0.8891 - val_acc: 0.7560\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8047 - acc: 0.8029 - val_loss: 0.9021 - val_acc: 0.7660\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8060 - acc: 0.8037 - val_loss: 0.8856 - val_acc: 0.7630\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8059 - acc: 0.8045 - val_loss: 0.8812 - val_acc: 0.7650\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8045 - acc: 0.8049 - val_loss: 0.8785 - val_acc: 0.7710\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8043 - acc: 0.8053 - val_loss: 0.8662 - val_acc: 0.7730\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8041 - acc: 0.8045 - val_loss: 0.8709 - val_acc: 0.7680\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8039 - acc: 0.8055 - val_loss: 0.8857 - val_acc: 0.7640\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8054 - acc: 0.8040 - val_loss: 0.8781 - val_acc: 0.7730\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8051 - acc: 0.8049 - val_loss: 0.8976 - val_acc: 0.7490\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8053 - acc: 0.8044 - val_loss: 0.8993 - val_acc: 0.7570\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8061 - acc: 0.8061 - val_loss: 0.8799 - val_acc: 0.7630\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8049 - acc: 0.8067 - val_loss: 0.8945 - val_acc: 0.7560\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8043 - acc: 0.8053 - val_loss: 0.8642 - val_acc: 0.7790\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8039 - acc: 0.8057 - val_loss: 0.8884 - val_acc: 0.7580\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8036 - acc: 0.8052 - val_loss: 0.8669 - val_acc: 0.7730\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8025 - acc: 0.8048 - val_loss: 0.8661 - val_acc: 0.7730\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8032 - acc: 0.8029 - val_loss: 0.8696 - val_acc: 0.7730\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8035 - acc: 0.8068 - val_loss: 0.8700 - val_acc: 0.7670\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8025 - acc: 0.8056 - val_loss: 0.8791 - val_acc: 0.7690\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8038 - acc: 0.8049 - val_loss: 0.8784 - val_acc: 0.7540\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8035 - acc: 0.8043 - val_loss: 0.8734 - val_acc: 0.7640\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8024 - acc: 0.8036 - val_loss: 0.8797 - val_acc: 0.7620\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8014 - acc: 0.8075 - val_loss: 0.8644 - val_acc: 0.7650\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8016 - acc: 0.8044 - val_loss: 0.8780 - val_acc: 0.7610\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8016 - acc: 0.8055 - val_loss: 0.8679 - val_acc: 0.7660\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8022 - acc: 0.8080 - val_loss: 0.8712 - val_acc: 0.7780\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8015 - acc: 0.8052 - val_loss: 0.8807 - val_acc: 0.7630\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8030 - acc: 0.8064 - val_loss: 0.8755 - val_acc: 0.7780\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8035 - acc: 0.8049 - val_loss: 0.8676 - val_acc: 0.7700\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8011 - acc: 0.8053 - val_loss: 0.9398 - val_acc: 0.7320\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8023 - acc: 0.8068 - val_loss: 0.8632 - val_acc: 0.7720\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8011 - acc: 0.8051 - val_loss: 0.8715 - val_acc: 0.7610\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8000 - acc: 0.8061 - val_loss: 0.8857 - val_acc: 0.7580\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8017 - acc: 0.8052 - val_loss: 0.8646 - val_acc: 0.7750\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8014 - acc: 0.8055 - val_loss: 0.8588 - val_acc: 0.7770\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7987 - acc: 0.8073 - val_loss: 0.8792 - val_acc: 0.7610\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8017 - acc: 0.8060 - val_loss: 0.8688 - val_acc: 0.7760\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8002 - acc: 0.8067 - val_loss: 0.8637 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8006 - acc: 0.8067 - val_loss: 0.8688 - val_acc: 0.7610\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7982 - acc: 0.8057 - val_loss: 0.8954 - val_acc: 0.7600\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8009 - acc: 0.8052 - val_loss: 0.8791 - val_acc: 0.7650\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7992 - acc: 0.8064 - val_loss: 0.8724 - val_acc: 0.7750\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7986 - acc: 0.8071 - val_loss: 0.8697 - val_acc: 0.7710\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7992 - acc: 0.8065 - val_loss: 0.8971 - val_acc: 0.7520\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7991 - acc: 0.8061 - val_loss: 0.8621 - val_acc: 0.7750\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7992 - acc: 0.8079 - val_loss: 0.8945 - val_acc: 0.7570\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7991 - acc: 0.8052 - val_loss: 0.8855 - val_acc: 0.7700\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7987 - acc: 0.8063 - val_loss: 0.8673 - val_acc: 0.7780\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7989 - acc: 0.8063 - val_loss: 0.8688 - val_acc: 0.7670\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7979 - acc: 0.8056 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7983 - acc: 0.8071 - val_loss: 0.8737 - val_acc: 0.7750\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7987 - acc: 0.8075 - val_loss: 0.8781 - val_acc: 0.7660\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7986 - acc: 0.8091 - val_loss: 0.8752 - val_acc: 0.7660\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7990 - acc: 0.8047 - val_loss: 0.8783 - val_acc: 0.7640\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7983 - acc: 0.8077 - val_loss: 0.8725 - val_acc: 0.7680\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7983 - acc: 0.8083 - val_loss: 0.8768 - val_acc: 0.7730\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7994 - acc: 0.8073 - val_loss: 0.8725 - val_acc: 0.7810\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7992 - acc: 0.8067 - val_loss: 0.8729 - val_acc: 0.7630\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7978 - acc: 0.8036 - val_loss: 0.8750 - val_acc: 0.7580\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8009 - acc: 0.8072 - val_loss: 0.8669 - val_acc: 0.7730\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7982 - acc: 0.8064 - val_loss: 0.8888 - val_acc: 0.7520\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7991 - acc: 0.8095 - val_loss: 0.8718 - val_acc: 0.7730\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7980 - acc: 0.8095 - val_loss: 0.9133 - val_acc: 0.7530\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7979 - acc: 0.8065 - val_loss: 0.8637 - val_acc: 0.7810\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7961 - acc: 0.8096 - val_loss: 0.8658 - val_acc: 0.7730\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7971 - acc: 0.8067 - val_loss: 0.8928 - val_acc: 0.7590\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7983 - acc: 0.8064 - val_loss: 0.8874 - val_acc: 0.7540\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7985 - acc: 0.8064 - val_loss: 0.8647 - val_acc: 0.7720\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7963 - acc: 0.8061 - val_loss: 0.8986 - val_acc: 0.7620\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7987 - acc: 0.8033 - val_loss: 0.8614 - val_acc: 0.7720\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7971 - acc: 0.8051 - val_loss: 0.8715 - val_acc: 0.7620\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7961 - acc: 0.8067 - val_loss: 0.8755 - val_acc: 0.7680\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7944 - acc: 0.8071 - val_loss: 0.8603 - val_acc: 0.7790\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7962 - acc: 0.8077 - val_loss: 0.8808 - val_acc: 0.7580\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7959 - acc: 0.8071 - val_loss: 0.8655 - val_acc: 0.7810\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7957 - acc: 0.8073 - val_loss: 0.8720 - val_acc: 0.7650\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7966 - acc: 0.8080 - val_loss: 0.9360 - val_acc: 0.7500\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7958 - acc: 0.8081 - val_loss: 0.8615 - val_acc: 0.7750\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7966 - acc: 0.8060 - val_loss: 0.8716 - val_acc: 0.7660\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7946 - acc: 0.8087 - val_loss: 0.8766 - val_acc: 0.7700\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7947 - acc: 0.8076 - val_loss: 0.8701 - val_acc: 0.7800\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7977 - acc: 0.8065 - val_loss: 0.9195 - val_acc: 0.7550\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7974 - acc: 0.8076 - val_loss: 0.8625 - val_acc: 0.7800\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7939 - acc: 0.8063 - val_loss: 0.8833 - val_acc: 0.7630\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.7944 - acc: 0.8083 - val_loss: 0.8753 - val_acc: 0.7580\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7932 - acc: 0.8057 - val_loss: 0.8664 - val_acc: 0.7750\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7925 - acc: 0.8105 - val_loss: 0.8729 - val_acc: 0.7720\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7932 - acc: 0.8095 - val_loss: 0.8726 - val_acc: 0.7840\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7946 - acc: 0.8089 - val_loss: 0.8832 - val_acc: 0.7570\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7961 - acc: 0.8080 - val_loss: 0.9009 - val_acc: 0.7510\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7946 - acc: 0.8081 - val_loss: 0.8728 - val_acc: 0.7690\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7941 - acc: 0.8093 - val_loss: 0.9018 - val_acc: 0.7550\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7926 - acc: 0.8085 - val_loss: 0.8624 - val_acc: 0.7810\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7927 - acc: 0.8104 - val_loss: 0.8726 - val_acc: 0.7650\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.7932 - acc: 0.8112 - val_loss: 0.8770 - val_acc: 0.7620\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.7929 - acc: 0.8111 - val_loss: 0.9027 - val_acc: 0.7560\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7938 - acc: 0.8113 - val_loss: 0.8734 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 194us/step\n",
      "1500/1500 [==============================] - 0s 196us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8005266609827677, 0.8089333333333333]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9102121858596802, 0.7673333333333333]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 12:05:44.540457 140736270398336 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 2s 249us/step - loss: 1.9670 - acc: 0.1417 - val_loss: 1.9281 - val_acc: 0.1890\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9474 - acc: 0.1627 - val_loss: 1.9185 - val_acc: 0.2070\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.9441 - acc: 0.1672 - val_loss: 1.9093 - val_acc: 0.2100\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.9309 - acc: 0.1816 - val_loss: 1.8994 - val_acc: 0.2300\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.9205 - acc: 0.1939 - val_loss: 1.8902 - val_acc: 0.2340\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.9134 - acc: 0.1984 - val_loss: 1.8804 - val_acc: 0.2380\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9032 - acc: 0.2080 - val_loss: 1.8705 - val_acc: 0.2440\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8945 - acc: 0.2193 - val_loss: 1.8594 - val_acc: 0.2470\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8890 - acc: 0.2213 - val_loss: 1.8482 - val_acc: 0.2520\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8793 - acc: 0.2263 - val_loss: 1.8354 - val_acc: 0.2630\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.8729 - acc: 0.2289 - val_loss: 1.8226 - val_acc: 0.2670\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.8569 - acc: 0.2472 - val_loss: 1.8087 - val_acc: 0.2800\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8423 - acc: 0.2464 - val_loss: 1.7928 - val_acc: 0.2870\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8316 - acc: 0.2544 - val_loss: 1.7755 - val_acc: 0.2940\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.8187 - acc: 0.2687 - val_loss: 1.7556 - val_acc: 0.3030\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.8125 - acc: 0.2691 - val_loss: 1.7386 - val_acc: 0.3280\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.7935 - acc: 0.2848 - val_loss: 1.7175 - val_acc: 0.3600\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7815 - acc: 0.3015 - val_loss: 1.6936 - val_acc: 0.3780\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7624 - acc: 0.3092 - val_loss: 1.6691 - val_acc: 0.4130\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.7460 - acc: 0.3216 - val_loss: 1.6442 - val_acc: 0.4210\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7338 - acc: 0.3187 - val_loss: 1.6192 - val_acc: 0.4480\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.7099 - acc: 0.3465 - val_loss: 1.5940 - val_acc: 0.4630\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6971 - acc: 0.3481 - val_loss: 1.5668 - val_acc: 0.4740\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6647 - acc: 0.3633 - val_loss: 1.5373 - val_acc: 0.4940\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.6568 - acc: 0.3657 - val_loss: 1.5114 - val_acc: 0.5200\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.6399 - acc: 0.3735 - val_loss: 1.4838 - val_acc: 0.5460\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.6171 - acc: 0.3895 - val_loss: 1.4586 - val_acc: 0.5680\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.6003 - acc: 0.4051 - val_loss: 1.4297 - val_acc: 0.5650\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.5676 - acc: 0.4117 - val_loss: 1.4014 - val_acc: 0.5960\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5614 - acc: 0.4175 - val_loss: 1.3775 - val_acc: 0.6130\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 1.5453 - acc: 0.4208 - val_loss: 1.3481 - val_acc: 0.6140\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 1.5305 - acc: 0.4248 - val_loss: 1.3191 - val_acc: 0.6310\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5123 - acc: 0.4381 - val_loss: 1.2975 - val_acc: 0.6430\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 149us/step - loss: 1.4893 - acc: 0.4457 - val_loss: 1.2718 - val_acc: 0.6490\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.4698 - acc: 0.4501 - val_loss: 1.2481 - val_acc: 0.6640\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.4630 - acc: 0.4601 - val_loss: 1.2242 - val_acc: 0.6690\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.4383 - acc: 0.4669 - val_loss: 1.2043 - val_acc: 0.6790\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.4165 - acc: 0.4772 - val_loss: 1.1810 - val_acc: 0.6830\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.4048 - acc: 0.4776 - val_loss: 1.1574 - val_acc: 0.6840\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.4021 - acc: 0.4809 - val_loss: 1.1394 - val_acc: 0.6880\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.3584 - acc: 0.5024 - val_loss: 1.1141 - val_acc: 0.6870\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.3552 - acc: 0.5069 - val_loss: 1.1001 - val_acc: 0.6990\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 1.3250 - acc: 0.5193 - val_loss: 1.0793 - val_acc: 0.6980\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 129us/step - loss: 1.3323 - acc: 0.5115 - val_loss: 1.0652 - val_acc: 0.7070\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3112 - acc: 0.5195 - val_loss: 1.0468 - val_acc: 0.7040\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.2889 - acc: 0.5263 - val_loss: 1.0270 - val_acc: 0.7030\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2806 - acc: 0.5332 - val_loss: 1.0126 - val_acc: 0.7070\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.2831 - acc: 0.5355 - val_loss: 1.0019 - val_acc: 0.7100\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2612 - acc: 0.5352 - val_loss: 0.9881 - val_acc: 0.7180\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2446 - acc: 0.5436 - val_loss: 0.9747 - val_acc: 0.7210\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2465 - acc: 0.5405 - val_loss: 0.9613 - val_acc: 0.7260\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2200 - acc: 0.5469 - val_loss: 0.9451 - val_acc: 0.7320\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2080 - acc: 0.5591 - val_loss: 0.9328 - val_acc: 0.7340\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2068 - acc: 0.5508 - val_loss: 0.9195 - val_acc: 0.7390\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1895 - acc: 0.5677 - val_loss: 0.9112 - val_acc: 0.7380\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.1842 - acc: 0.5617 - val_loss: 0.9012 - val_acc: 0.7400\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1853 - acc: 0.5599 - val_loss: 0.8918 - val_acc: 0.7390\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1555 - acc: 0.5763 - val_loss: 0.8805 - val_acc: 0.7420\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.1591 - acc: 0.5767 - val_loss: 0.8709 - val_acc: 0.7380\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1447 - acc: 0.5795 - val_loss: 0.8611 - val_acc: 0.7340\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.1445 - acc: 0.5877 - val_loss: 0.8551 - val_acc: 0.7360\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1203 - acc: 0.5871 - val_loss: 0.8402 - val_acc: 0.7410\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.1237 - acc: 0.5948 - val_loss: 0.8343 - val_acc: 0.7410\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.1069 - acc: 0.5921 - val_loss: 0.8253 - val_acc: 0.7400\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1081 - acc: 0.5940 - val_loss: 0.8200 - val_acc: 0.7390\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.1021 - acc: 0.5939 - val_loss: 0.8149 - val_acc: 0.7380\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0973 - acc: 0.5956 - val_loss: 0.8074 - val_acc: 0.7390\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0805 - acc: 0.5965 - val_loss: 0.8002 - val_acc: 0.7480\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0644 - acc: 0.6061 - val_loss: 0.7874 - val_acc: 0.7450\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0659 - acc: 0.6045 - val_loss: 0.7814 - val_acc: 0.7460\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0534 - acc: 0.6103 - val_loss: 0.7750 - val_acc: 0.7480\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0618 - acc: 0.6077 - val_loss: 0.7732 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0362 - acc: 0.6195 - val_loss: 0.7650 - val_acc: 0.7450\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0383 - acc: 0.6177 - val_loss: 0.7590 - val_acc: 0.7550\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.0396 - acc: 0.6195 - val_loss: 0.7529 - val_acc: 0.7540\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0173 - acc: 0.6233 - val_loss: 0.7476 - val_acc: 0.7520\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0239 - acc: 0.6244 - val_loss: 0.7412 - val_acc: 0.7520\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0128 - acc: 0.6340 - val_loss: 0.7360 - val_acc: 0.7540\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0184 - acc: 0.6176 - val_loss: 0.7335 - val_acc: 0.7520\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0179 - acc: 0.6155 - val_loss: 0.7316 - val_acc: 0.7520\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0007 - acc: 0.6239 - val_loss: 0.7243 - val_acc: 0.7560\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9902 - acc: 0.6375 - val_loss: 0.7187 - val_acc: 0.7560\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 187us/step - loss: 0.9947 - acc: 0.6347 - val_loss: 0.7142 - val_acc: 0.7560\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9838 - acc: 0.6319 - val_loss: 0.7135 - val_acc: 0.7580\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9809 - acc: 0.6360 - val_loss: 0.7059 - val_acc: 0.7600\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.9594 - acc: 0.6439 - val_loss: 0.7019 - val_acc: 0.7560\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9733 - acc: 0.6412 - val_loss: 0.7010 - val_acc: 0.7620\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9754 - acc: 0.6356 - val_loss: 0.6990 - val_acc: 0.7630\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.9614 - acc: 0.6473 - val_loss: 0.6938 - val_acc: 0.7630\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9620 - acc: 0.6411 - val_loss: 0.6890 - val_acc: 0.7630\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9497 - acc: 0.6560 - val_loss: 0.6856 - val_acc: 0.7660\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9518 - acc: 0.6499 - val_loss: 0.6813 - val_acc: 0.7670\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.9495 - acc: 0.6491 - val_loss: 0.6796 - val_acc: 0.7660\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.9507 - acc: 0.6519 - val_loss: 0.6776 - val_acc: 0.7620\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.9128 - acc: 0.6568 - val_loss: 0.6705 - val_acc: 0.7660\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9277 - acc: 0.6607 - val_loss: 0.6696 - val_acc: 0.7660\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9285 - acc: 0.6549 - val_loss: 0.6649 - val_acc: 0.7650\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9285 - acc: 0.6557 - val_loss: 0.6654 - val_acc: 0.7660\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.9259 - acc: 0.6527 - val_loss: 0.6651 - val_acc: 0.7660\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9340 - acc: 0.6559 - val_loss: 0.6629 - val_acc: 0.7660\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.9190 - acc: 0.6531 - val_loss: 0.6570 - val_acc: 0.7690\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9161 - acc: 0.6612 - val_loss: 0.6552 - val_acc: 0.7650\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 0.9016 - acc: 0.6653 - val_loss: 0.6519 - val_acc: 0.7650\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9124 - acc: 0.6616 - val_loss: 0.6528 - val_acc: 0.7670\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.9048 - acc: 0.6623 - val_loss: 0.6490 - val_acc: 0.7680\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9027 - acc: 0.6640 - val_loss: 0.6478 - val_acc: 0.7640\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8835 - acc: 0.6769 - val_loss: 0.6443 - val_acc: 0.7640\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 0.8838 - acc: 0.6693 - val_loss: 0.6415 - val_acc: 0.7710\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8846 - acc: 0.6701 - val_loss: 0.6410 - val_acc: 0.7700\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8760 - acc: 0.6705 - val_loss: 0.6359 - val_acc: 0.7700\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8777 - acc: 0.6709 - val_loss: 0.6340 - val_acc: 0.7720\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8758 - acc: 0.6688 - val_loss: 0.6332 - val_acc: 0.7710\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8791 - acc: 0.6721 - val_loss: 0.6316 - val_acc: 0.7690\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8694 - acc: 0.6769 - val_loss: 0.6296 - val_acc: 0.7680\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8564 - acc: 0.6839 - val_loss: 0.6235 - val_acc: 0.7740\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8774 - acc: 0.6723 - val_loss: 0.6254 - val_acc: 0.7720\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8782 - acc: 0.6693 - val_loss: 0.6241 - val_acc: 0.7730\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8652 - acc: 0.6775 - val_loss: 0.6207 - val_acc: 0.7760\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8600 - acc: 0.6795 - val_loss: 0.6187 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8675 - acc: 0.6752 - val_loss: 0.6197 - val_acc: 0.7810\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8436 - acc: 0.6809 - val_loss: 0.6192 - val_acc: 0.7760\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8587 - acc: 0.6827 - val_loss: 0.6165 - val_acc: 0.7760\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8415 - acc: 0.6859 - val_loss: 0.6124 - val_acc: 0.7800\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8514 - acc: 0.6809 - val_loss: 0.6108 - val_acc: 0.7770\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8445 - acc: 0.6885 - val_loss: 0.6125 - val_acc: 0.7780\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8432 - acc: 0.6859 - val_loss: 0.6096 - val_acc: 0.7780\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8341 - acc: 0.6887 - val_loss: 0.6086 - val_acc: 0.7850\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8337 - acc: 0.6917 - val_loss: 0.6055 - val_acc: 0.7800\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8325 - acc: 0.6884 - val_loss: 0.6054 - val_acc: 0.7810\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8367 - acc: 0.6961 - val_loss: 0.6046 - val_acc: 0.7800\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8173 - acc: 0.6907 - val_loss: 0.6045 - val_acc: 0.7820\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8394 - acc: 0.6797 - val_loss: 0.6029 - val_acc: 0.7820\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8243 - acc: 0.6940 - val_loss: 0.6025 - val_acc: 0.7790\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8318 - acc: 0.6837 - val_loss: 0.5993 - val_acc: 0.7840\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8226 - acc: 0.6955 - val_loss: 0.5986 - val_acc: 0.7840\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8155 - acc: 0.6961 - val_loss: 0.5981 - val_acc: 0.7830\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8142 - acc: 0.6935 - val_loss: 0.5969 - val_acc: 0.7810\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8178 - acc: 0.6921 - val_loss: 0.5969 - val_acc: 0.7810\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8208 - acc: 0.6939 - val_loss: 0.5952 - val_acc: 0.7830\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8239 - acc: 0.6988 - val_loss: 0.5971 - val_acc: 0.7810\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8035 - acc: 0.7013 - val_loss: 0.5941 - val_acc: 0.7790\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8152 - acc: 0.7004 - val_loss: 0.5903 - val_acc: 0.7820\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.7962 - acc: 0.6983 - val_loss: 0.5906 - val_acc: 0.7770\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.7979 - acc: 0.7009 - val_loss: 0.5908 - val_acc: 0.7830\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8011 - acc: 0.7033 - val_loss: 0.5877 - val_acc: 0.7860\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7779 - acc: 0.7083 - val_loss: 0.5863 - val_acc: 0.7780\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7941 - acc: 0.6972 - val_loss: 0.5865 - val_acc: 0.7750\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7799 - acc: 0.7023 - val_loss: 0.5837 - val_acc: 0.7820\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.7955 - acc: 0.6987 - val_loss: 0.5829 - val_acc: 0.7840\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7727 - acc: 0.7104 - val_loss: 0.5819 - val_acc: 0.7800\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7867 - acc: 0.7071 - val_loss: 0.5827 - val_acc: 0.7810\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 0.7723 - acc: 0.7112 - val_loss: 0.5795 - val_acc: 0.7780\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7817 - acc: 0.7035 - val_loss: 0.5798 - val_acc: 0.7850\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7737 - acc: 0.7060 - val_loss: 0.5796 - val_acc: 0.7870\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7835 - acc: 0.7049 - val_loss: 0.5775 - val_acc: 0.7860\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7839 - acc: 0.7048 - val_loss: 0.5772 - val_acc: 0.7860\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 0.7762 - acc: 0.7109 - val_loss: 0.5766 - val_acc: 0.7820\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.7779 - acc: 0.7089 - val_loss: 0.5760 - val_acc: 0.7820\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 118us/step - loss: 0.7886 - acc: 0.7083 - val_loss: 0.5775 - val_acc: 0.7800\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7638 - acc: 0.7145 - val_loss: 0.5760 - val_acc: 0.7800\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7665 - acc: 0.7107 - val_loss: 0.5781 - val_acc: 0.7820\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7713 - acc: 0.7176 - val_loss: 0.5753 - val_acc: 0.7830\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7557 - acc: 0.7175 - val_loss: 0.5738 - val_acc: 0.7800\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.7608 - acc: 0.7112 - val_loss: 0.5725 - val_acc: 0.7840\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7508 - acc: 0.7196 - val_loss: 0.5708 - val_acc: 0.7870\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.7619 - acc: 0.7165 - val_loss: 0.5732 - val_acc: 0.7860\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7638 - acc: 0.7145 - val_loss: 0.5712 - val_acc: 0.7850\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7533 - acc: 0.7156 - val_loss: 0.5697 - val_acc: 0.7810\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7590 - acc: 0.7149 - val_loss: 0.5698 - val_acc: 0.7800\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.7506 - acc: 0.7124 - val_loss: 0.5688 - val_acc: 0.7770\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7505 - acc: 0.7145 - val_loss: 0.5686 - val_acc: 0.7760\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7409 - acc: 0.7161 - val_loss: 0.5668 - val_acc: 0.7800\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7454 - acc: 0.7169 - val_loss: 0.5660 - val_acc: 0.7820\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7488 - acc: 0.7124 - val_loss: 0.5688 - val_acc: 0.7890\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7483 - acc: 0.7200 - val_loss: 0.5655 - val_acc: 0.7870\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7262 - acc: 0.7243 - val_loss: 0.5633 - val_acc: 0.7840\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.7362 - acc: 0.7184 - val_loss: 0.5654 - val_acc: 0.7880\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7326 - acc: 0.7216 - val_loss: 0.5632 - val_acc: 0.7860\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7333 - acc: 0.7215 - val_loss: 0.5620 - val_acc: 0.7910\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7413 - acc: 0.7285 - val_loss: 0.5632 - val_acc: 0.7910\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.7434 - acc: 0.7260 - val_loss: 0.5651 - val_acc: 0.7960\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.7363 - acc: 0.7212 - val_loss: 0.5621 - val_acc: 0.7830\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7229 - acc: 0.7249 - val_loss: 0.5611 - val_acc: 0.7890\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.7349 - acc: 0.7233 - val_loss: 0.5605 - val_acc: 0.7910\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7318 - acc: 0.7257 - val_loss: 0.5618 - val_acc: 0.7910\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 0.7410 - acc: 0.7155 - val_loss: 0.5596 - val_acc: 0.7880\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7298 - acc: 0.7271 - val_loss: 0.5584 - val_acc: 0.7900\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7211 - acc: 0.7281 - val_loss: 0.5579 - val_acc: 0.7920\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7242 - acc: 0.7259 - val_loss: 0.5578 - val_acc: 0.7920\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.7248 - acc: 0.7293 - val_loss: 0.5570 - val_acc: 0.7910\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.7177 - acc: 0.7187 - val_loss: 0.5584 - val_acc: 0.7920\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.7178 - acc: 0.7271 - val_loss: 0.5571 - val_acc: 0.7920\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7095 - acc: 0.7317 - val_loss: 0.5564 - val_acc: 0.7870\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7069 - acc: 0.7372 - val_loss: 0.5544 - val_acc: 0.7890\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7255 - acc: 0.7253 - val_loss: 0.5550 - val_acc: 0.7900\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.7172 - acc: 0.7323 - val_loss: 0.5548 - val_acc: 0.7930\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.6985 - acc: 0.7304 - val_loss: 0.5550 - val_acc: 0.7890\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7201 - acc: 0.7255 - val_loss: 0.5541 - val_acc: 0.7920\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7105 - acc: 0.7283 - val_loss: 0.5535 - val_acc: 0.7940\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.7187 - acc: 0.7260 - val_loss: 0.5543 - val_acc: 0.7920\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 54us/step\n",
      "1500/1500 [==============================] - 0s 107us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4355910324573517, 0.8416]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5996321500937144, 0.7680000004768371]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 68us/step - loss: 1.9040 - acc: 0.2080 - val_loss: 1.8542 - val_acc: 0.2480\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.7750 - acc: 0.3207 - val_loss: 1.6784 - val_acc: 0.4043\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 1.5463 - acc: 0.5030 - val_loss: 1.4143 - val_acc: 0.5757\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 1.2915 - acc: 0.6181 - val_loss: 1.1865 - val_acc: 0.6457\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 1.0929 - acc: 0.6677 - val_loss: 1.0266 - val_acc: 0.6763\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 3s 88us/step - loss: 0.9553 - acc: 0.6956 - val_loss: 0.9157 - val_acc: 0.6980\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.8616 - acc: 0.7117 - val_loss: 0.8423 - val_acc: 0.7127\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.7965 - acc: 0.7247 - val_loss: 0.7903 - val_acc: 0.7180\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.7502 - acc: 0.7321 - val_loss: 0.7551 - val_acc: 0.7307\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.7152 - acc: 0.7401 - val_loss: 0.7293 - val_acc: 0.7377\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.6880 - acc: 0.7474 - val_loss: 0.7077 - val_acc: 0.7417\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.6661 - acc: 0.7547 - val_loss: 0.6932 - val_acc: 0.7517\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.6480 - acc: 0.7602 - val_loss: 0.6774 - val_acc: 0.7517\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.6325 - acc: 0.7658 - val_loss: 0.6661 - val_acc: 0.7560\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.6189 - acc: 0.7699 - val_loss: 0.6560 - val_acc: 0.7570\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.6069 - acc: 0.7737 - val_loss: 0.6492 - val_acc: 0.7590\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.5963 - acc: 0.7775 - val_loss: 0.6434 - val_acc: 0.7630\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5864 - acc: 0.7813 - val_loss: 0.6370 - val_acc: 0.7670\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5774 - acc: 0.7853 - val_loss: 0.6308 - val_acc: 0.7673\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.5690 - acc: 0.7868 - val_loss: 0.6235 - val_acc: 0.7713\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5615 - acc: 0.7908 - val_loss: 0.6200 - val_acc: 0.7747\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5541 - acc: 0.7931 - val_loss: 0.6157 - val_acc: 0.7743\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.5471 - acc: 0.7964 - val_loss: 0.6108 - val_acc: 0.7760\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5410 - acc: 0.7982 - val_loss: 0.6077 - val_acc: 0.7760\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.5346 - acc: 0.8018 - val_loss: 0.6053 - val_acc: 0.7807\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.5288 - acc: 0.8035 - val_loss: 0.6003 - val_acc: 0.7803\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.5230 - acc: 0.8057 - val_loss: 0.5984 - val_acc: 0.7803\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5178 - acc: 0.8081 - val_loss: 0.5945 - val_acc: 0.7807\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.5124 - acc: 0.8106 - val_loss: 0.5934 - val_acc: 0.7813\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5074 - acc: 0.8126 - val_loss: 0.5898 - val_acc: 0.7833\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.5024 - acc: 0.8156 - val_loss: 0.5888 - val_acc: 0.7833\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4980 - acc: 0.8161 - val_loss: 0.5876 - val_acc: 0.7833\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4933 - acc: 0.8188 - val_loss: 0.5850 - val_acc: 0.7803\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4892 - acc: 0.8202 - val_loss: 0.5829 - val_acc: 0.7883\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4850 - acc: 0.8229 - val_loss: 0.5806 - val_acc: 0.7903\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4807 - acc: 0.8243 - val_loss: 0.5809 - val_acc: 0.7840\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4765 - acc: 0.8261 - val_loss: 0.5778 - val_acc: 0.7870\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4729 - acc: 0.8274 - val_loss: 0.5766 - val_acc: 0.7907\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4690 - acc: 0.8305 - val_loss: 0.5755 - val_acc: 0.7893\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4653 - acc: 0.8313 - val_loss: 0.5745 - val_acc: 0.7880\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4613 - acc: 0.8331 - val_loss: 0.5779 - val_acc: 0.7893\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4581 - acc: 0.8348 - val_loss: 0.5717 - val_acc: 0.7897\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4549 - acc: 0.8344 - val_loss: 0.5710 - val_acc: 0.7877\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4514 - acc: 0.8368 - val_loss: 0.5704 - val_acc: 0.7927\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4480 - acc: 0.8373 - val_loss: 0.5701 - val_acc: 0.7927\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4451 - acc: 0.8385 - val_loss: 0.5681 - val_acc: 0.7913\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4419 - acc: 0.8401 - val_loss: 0.5687 - val_acc: 0.7923\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4388 - acc: 0.8414 - val_loss: 0.5676 - val_acc: 0.7923\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4359 - acc: 0.8432 - val_loss: 0.5666 - val_acc: 0.7950\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4328 - acc: 0.8427 - val_loss: 0.5660 - val_acc: 0.7943\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.4300 - acc: 0.8446 - val_loss: 0.5694 - val_acc: 0.7920\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4276 - acc: 0.8457 - val_loss: 0.5660 - val_acc: 0.7933\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4246 - acc: 0.8472 - val_loss: 0.5662 - val_acc: 0.7927\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4219 - acc: 0.8487 - val_loss: 0.5666 - val_acc: 0.7933\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4194 - acc: 0.8496 - val_loss: 0.5654 - val_acc: 0.7933\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4171 - acc: 0.8505 - val_loss: 0.5645 - val_acc: 0.7940\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.4142 - acc: 0.8515 - val_loss: 0.5638 - val_acc: 0.7960\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.4118 - acc: 0.8528 - val_loss: 0.5645 - val_acc: 0.7940\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4090 - acc: 0.8530 - val_loss: 0.5646 - val_acc: 0.7957\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4066 - acc: 0.8548 - val_loss: 0.5641 - val_acc: 0.7947\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4041 - acc: 0.8557 - val_loss: 0.5658 - val_acc: 0.7967\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4019 - acc: 0.8558 - val_loss: 0.5642 - val_acc: 0.7960\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4000 - acc: 0.8571 - val_loss: 0.5645 - val_acc: 0.7957\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3979 - acc: 0.8579 - val_loss: 0.5647 - val_acc: 0.7947\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3953 - acc: 0.8584 - val_loss: 0.5652 - val_acc: 0.7950\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3932 - acc: 0.8598 - val_loss: 0.5674 - val_acc: 0.7930\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3912 - acc: 0.8601 - val_loss: 0.5659 - val_acc: 0.7950\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3891 - acc: 0.8617 - val_loss: 0.5651 - val_acc: 0.7983\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3869 - acc: 0.8630 - val_loss: 0.5668 - val_acc: 0.7970\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3850 - acc: 0.8636 - val_loss: 0.5659 - val_acc: 0.7953\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3823 - acc: 0.8636 - val_loss: 0.5665 - val_acc: 0.7950\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3808 - acc: 0.8649 - val_loss: 0.5662 - val_acc: 0.7950\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3790 - acc: 0.8666 - val_loss: 0.5667 - val_acc: 0.7950\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3771 - acc: 0.8656 - val_loss: 0.5663 - val_acc: 0.7957\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3751 - acc: 0.8666 - val_loss: 0.5680 - val_acc: 0.7973\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3732 - acc: 0.8673 - val_loss: 0.5683 - val_acc: 0.7963\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3714 - acc: 0.8690 - val_loss: 0.5686 - val_acc: 0.7950\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3697 - acc: 0.8684 - val_loss: 0.5676 - val_acc: 0.7950\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3682 - acc: 0.8694 - val_loss: 0.5686 - val_acc: 0.7947\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3657 - acc: 0.8707 - val_loss: 0.5709 - val_acc: 0.7963\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3642 - acc: 0.8709 - val_loss: 0.5693 - val_acc: 0.7953\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3627 - acc: 0.8705 - val_loss: 0.5698 - val_acc: 0.7953\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3609 - acc: 0.8722 - val_loss: 0.5714 - val_acc: 0.7963\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3594 - acc: 0.8725 - val_loss: 0.5704 - val_acc: 0.7983\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3577 - acc: 0.8730 - val_loss: 0.5718 - val_acc: 0.7993\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3561 - acc: 0.8741 - val_loss: 0.5737 - val_acc: 0.7980\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3545 - acc: 0.8747 - val_loss: 0.5743 - val_acc: 0.7983\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3526 - acc: 0.8749 - val_loss: 0.5749 - val_acc: 0.7963\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3511 - acc: 0.8765 - val_loss: 0.5738 - val_acc: 0.7987\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3495 - acc: 0.8768 - val_loss: 0.5744 - val_acc: 0.7997\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3480 - acc: 0.8767 - val_loss: 0.5750 - val_acc: 0.7980\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3465 - acc: 0.8776 - val_loss: 0.5745 - val_acc: 0.8000\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3452 - acc: 0.8776 - val_loss: 0.5765 - val_acc: 0.7997\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3436 - acc: 0.8791 - val_loss: 0.5771 - val_acc: 0.7990\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3420 - acc: 0.8788 - val_loss: 0.5781 - val_acc: 0.7967\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3406 - acc: 0.8799 - val_loss: 0.5796 - val_acc: 0.7973\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3388 - acc: 0.8802 - val_loss: 0.5784 - val_acc: 0.7977\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3375 - acc: 0.8811 - val_loss: 0.5782 - val_acc: 0.7997\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3363 - acc: 0.8812 - val_loss: 0.5781 - val_acc: 0.7987\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3346 - acc: 0.8819 - val_loss: 0.5816 - val_acc: 0.7977\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3337 - acc: 0.8829 - val_loss: 0.5811 - val_acc: 0.7980\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3320 - acc: 0.8833 - val_loss: 0.5840 - val_acc: 0.7980\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3309 - acc: 0.8835 - val_loss: 0.5828 - val_acc: 0.7967\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3296 - acc: 0.8845 - val_loss: 0.5855 - val_acc: 0.7967\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3282 - acc: 0.8845 - val_loss: 0.5848 - val_acc: 0.7963\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3264 - acc: 0.8853 - val_loss: 0.5856 - val_acc: 0.7987\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3254 - acc: 0.8855 - val_loss: 0.5843 - val_acc: 0.7990\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3243 - acc: 0.8867 - val_loss: 0.5869 - val_acc: 0.7973\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3231 - acc: 0.8866 - val_loss: 0.5859 - val_acc: 0.7987\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3214 - acc: 0.8873 - val_loss: 0.5881 - val_acc: 0.7967\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3200 - acc: 0.8886 - val_loss: 0.5929 - val_acc: 0.7947\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3191 - acc: 0.8885 - val_loss: 0.5896 - val_acc: 0.7967\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3174 - acc: 0.8890 - val_loss: 0.5914 - val_acc: 0.7967\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3163 - acc: 0.8900 - val_loss: 0.5902 - val_acc: 0.7990\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3155 - acc: 0.8905 - val_loss: 0.5915 - val_acc: 0.7970\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3138 - acc: 0.8904 - val_loss: 0.5946 - val_acc: 0.7960\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3127 - acc: 0.8905 - val_loss: 0.5931 - val_acc: 0.7963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3113 - acc: 0.8913 - val_loss: 0.5941 - val_acc: 0.8000\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3102 - acc: 0.8923 - val_loss: 0.5942 - val_acc: 0.7983\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3086 - acc: 0.8931 - val_loss: 0.5995 - val_acc: 0.7990\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 38us/step\n",
      "4000/4000 [==============================] - 0s 51us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30719884325518754, 0.8936666666666667]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5460094529390335, 0.81025]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
